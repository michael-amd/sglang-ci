INFO 10-28 11:15:42 __init__.py:179] Automatically detected platform rocm.
WARNING 10-28 11:15:42 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-28 11:15:42] WARNING server_args.py:1129: Attention backend not explicitly specified. Use aiter backend by default.
[2025-10-28 11:15:42] WARNING server_args.py:1318: DP attention is enabled. The chunked prefill size is adjusted to 16384 to avoid MoE kernel issues. 
[2025-10-28 11:15:42] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-28 11:15:42] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.7200000000000001, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=0.3, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=989258327, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=8, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=True, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
[2025-10-28 11:15:43] Using default HuggingFace chat template with detected content format: string
INFO 10-28 11:15:51 __init__.py:179] Automatically detected platform rocm.
INFO 10-28 11:15:52 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-28 11:15:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-28 11:15:52] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 10-28 11:16:01 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-28 11:16:01] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-28 11:16:01 DP3 TP3] Process 815 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
INFO 10-28 11:16:02 __init__.py:179] Automatically detected platform rocm.
INFO 10-28 11:16:02 __init__.py:179] Automatically detected platform rocm.
INFO 10-28 11:16:02 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-28 11:16:02] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-28 11:16:02] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-28 11:16:02] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-28 11:16:02 DP7 TP7] Process 819 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[2025-10-28 11:16:02 DP1 TP1] Process 813 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
[2025-10-28 11:16:02 DP2 TP2] Process 814 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
[2025-10-28 11:16:02 DP3 TP3] Init torch distributed begin.
INFO 10-28 11:16:02 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
INFO 10-28 11:16:02 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
INFO 10-28 11:16:02 __init__.py:179] Automatically detected platform rocm.
[2025-10-28 11:16:02] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-28 11:16:02 DP1 TP1] Init torch distributed begin.
[2025-10-28 11:16:02 DP2 TP2] Init torch distributed begin.
[2025-10-28 11:16:02 DP7 TP7] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
INFO 10-28 11:16:02 __init__.py:179] Automatically detected platform rocm.
[2025-10-28 11:16:02] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-28 11:16:02] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-28 11:16:02 DP0 TP0] Process 812 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-10-28 11:16:02] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-10-28 11:16:02 DP5 TP5] Process 817 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-10-28 11:16:02 DP4 TP4] Process 816 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[2025-10-28 11:16:02 DP6 TP6] Process 818 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
[2025-10-28 11:16:02 DP0 TP0] Init torch distributed begin.
[2025-10-28 11:16:03 DP5 TP5] Init torch distributed begin.
[2025-10-28 11:16:03 DP4 TP4] Init torch distributed begin.
[2025-10-28 11:16:03 DP6 TP6] Init torch distributed begin.
[2025-10-28 11:16:03 DP0 TP0] sglang is using nccl==2.21.5
[2025-10-28 11:16:05 DP0 TP0] Init torch distributed ends. mem usage=3.65 GB
[2025-10-28 11:16:05 DP7 TP7] Init torch distributed ends. mem usage=3.94 GB
[2025-10-28 11:16:05 DP6 TP6] Init torch distributed ends. mem usage=3.95 GB
[2025-10-28 11:16:05 DP2 TP2] Init torch distributed ends. mem usage=4.07 GB
[2025-10-28 11:16:05 DP4 TP4] Init torch distributed ends. mem usage=4.01 GB
[2025-10-28 11:16:05 DP5 TP5] Init torch distributed ends. mem usage=3.93 GB
[2025-10-28 11:16:05 DP1 TP1] Init torch distributed ends. mem usage=4.07 GB
[2025-10-28 11:16:05 DP3 TP3] Init torch distributed ends. mem usage=4.06 GB
[2025-10-28 11:16:07 DP7 TP7] Load weight begin. avail mem=187.32 GB
[2025-10-28 11:16:07 DP0 TP0] Load weight begin. avail mem=187.61 GB
[2025-10-28 11:16:07 DP0 TP0] Detected fp8 checkpoint.
[2025-10-28 11:16:07 DP0 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-10-28 11:16:07 DP6 TP6] Load weight begin. avail mem=187.31 GB
[2025-10-28 11:16:07 DP4 TP4] Load weight begin. avail mem=187.25 GB
[2025-10-28 11:16:07 DP2 TP2] Load weight begin. avail mem=187.19 GB
[2025-10-28 11:16:07 DP3 TP3] Load weight begin. avail mem=187.20 GB
[2025-10-28 11:16:07 DP1 TP1] Load weight begin. avail mem=187.19 GB
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
[2025-10-28 11:16:07 DP5 TP5] Load weight begin. avail mem=187.33 GB
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:29,  5.47it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:32,  5.02it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:27,  5.91it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:33,  4.73it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:28,  5.45it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:01<00:23,  6.53it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:01<00:22,  6.86it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:01<00:21,  7.15it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:30,  4.96it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:01<00:27,  5.63it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:02<00:45,  3.35it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:02<00:40,  3.74it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:02<00:25,  5.90it/s]
Loading safetensors checkpoint shards:  11% Completed | 18/163 [00:02<00:15,  9.10it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:03<00:15,  9.45it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:03<00:11, 11.99it/s]
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:03<00:11, 11.69it/s]
Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:03<00:10, 13.17it/s]
Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:03<00:09, 14.58it/s]
Loading safetensors checkpoint shards:  20% Completed | 32/163 [00:03<00:08, 15.24it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:04<00:07, 16.88it/s]
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:04<00:07, 16.40it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:04<00:16,  7.32it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:04<00:13,  8.80it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:05<00:11, 10.32it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:05<00:10, 11.35it/s]
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:05<00:09, 11.60it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:05<00:07, 14.16it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:05<00:08, 13.47it/s]
Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:05<00:07, 14.93it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:05<00:06, 15.42it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:06<00:07, 13.74it/s]
Loading safetensors checkpoint shards:  39% Completed | 63/163 [00:06<00:06, 16.50it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:06<00:05, 16.96it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:06<00:05, 18.95it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:07<00:11,  8.27it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:07<00:09,  9.40it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:07<00:08, 10.77it/s]
Loading safetensors checkpoint shards:  47% Completed | 77/163 [00:07<00:07, 11.24it/s]
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:07<00:05, 14.01it/s]
Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:07<00:04, 16.01it/s]
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:08<00:04, 16.21it/s]
Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:08<00:04, 17.69it/s]
Loading safetensors checkpoint shards:  56% Completed | 91/163 [00:08<00:03, 19.56it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:08<00:03, 17.49it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:08<00:03, 17.86it/s]
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:08<00:04, 13.95it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:08<00:04, 14.76it/s]
Loading safetensors checkpoint shards:  63% Completed | 102/163 [00:09<00:05, 11.89it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:09<00:04, 14.50it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:09<00:04, 13.67it/s]
Loading safetensors checkpoint shards:  67% Completed | 110/163 [00:10<00:07,  7.31it/s]
Loading safetensors checkpoint shards:  69% Completed | 113/163 [00:10<00:05,  9.68it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:10<00:04, 11.71it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:10<00:03, 14.11it/s]
Loading safetensors checkpoint shards:  74% Completed | 121/163 [00:10<00:03, 13.26it/s]
Loading safetensors checkpoint shards:  75% Completed | 123/163 [00:10<00:02, 14.36it/s]
Loading safetensors checkpoint shards:  77% Completed | 126/163 [00:11<00:02, 16.96it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:11<00:01, 18.72it/s]
Loading safetensors checkpoint shards:  81% Completed | 132/163 [00:11<00:01, 20.54it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:11<00:01, 19.98it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:11<00:01, 20.92it/s]
Loading safetensors checkpoint shards:  87% Completed | 141/163 [00:11<00:01, 20.17it/s]
Loading safetensors checkpoint shards:  88% Completed | 144/163 [00:11<00:00, 21.11it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:12<00:00, 16.56it/s]
Loading safetensors checkpoint shards:  92% Completed | 150/163 [00:12<00:00, 18.07it/s]
Loading safetensors checkpoint shards:  93% Completed | 152/163 [00:12<00:00, 17.41it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:12<00:00, 18.60it/s]
Loading safetensors checkpoint shards:  97% Completed | 158/163 [00:12<00:00, 20.01it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:13<00:00,  6.61it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:13<00:00,  7.73it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:13<00:00, 11.70it/s]

[2025-10-28 11:17:00 DP5 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.86 GB, mem usage=90.47 GB.
[2025-10-28 11:17:00 DP4 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.78 GB, mem usage=90.47 GB.
[2025-10-28 11:17:00 DP6 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.85 GB, mem usage=90.47 GB.
[2025-10-28 11:17:01 DP7 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.86 GB, mem usage=90.47 GB.
[2025-10-28 11:17:02 DP2 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.72 GB, mem usage=90.47 GB.
[2025-10-28 11:17:02 DP1 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.72 GB, mem usage=90.47 GB.
[2025-10-28 11:17:02 DP0 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=97.14 GB, mem usage=90.47 GB.
[2025-10-28 11:17:03 DP3 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=96.73 GB, mem usage=90.47 GB.
[2025-10-28 11:17:03 DP0 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-28 11:17:03 DP5 TP5] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-28 11:17:03 DP5 TP5] Memory pool end. avail mem=51.25 GB
[2025-10-28 11:17:03 DP0 TP0] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-28 11:17:03 DP0 TP0] Memory pool end. avail mem=51.53 GB
[2025-10-28 11:17:03 DP4 TP4] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-28 11:17:03 DP6 TP6] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-28 11:17:03 DP7 TP7] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-28 11:17:03 DP4 TP4] Memory pool end. avail mem=51.17 GB
[2025-10-28 11:17:03 DP6 TP6] Memory pool end. avail mem=51.23 GB
[2025-10-28 11:17:03 DP7 TP7] Memory pool end. avail mem=51.24 GB
[2025-10-28 11:17:03 DP1 TP1] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-28 11:17:03 DP3 TP3] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-28 11:17:03 DP3 TP3] Memory pool end. avail mem=51.12 GB
[2025-10-28 11:17:03 DP1 TP1] Memory pool end. avail mem=51.11 GB
[2025-10-28 11:17:03 DP2 TP2] KV Cache is allocated. #tokens: 676049, KV size: 44.24 GB
[2025-10-28 11:17:03 DP2 TP2] Memory pool end. avail mem=51.11 GB
[2025-10-28 11:17:04 DP2 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=50.90 GB
[2025-10-28 11:17:04 DP0 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=51.32 GB
[2025-10-28 11:17:04 DP0 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-10-28 11:17:05 DP1 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=50.91 GB
[2025-10-28 11:17:05 DP4 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=50.96 GB
[2025-10-28 11:17:05 DP5 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=51.05 GB
[2025-10-28 11:17:05 DP3 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=50.91 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=50.68 GB):   0%|          | 0/52 [00:00<?, ?it/s][2025-10-28 11:17:06 DP6 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=51.03 GB
[2025-10-28 11:17:06 DP7 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=51.04 GB
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-28 11:17:07 DP7 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-28 11:17:07 DP6 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-28 11:17:07 DP3 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-28 11:17:07 DP0 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-28 11:17:07 DP4 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-28 11:17:07 DP5 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-28 11:17:07 DP1 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-28 11:17:07 DP2 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-28 11:17:08 DP0 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-28 11:17:08 DP4 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-28 11:17:08 DP3 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-28 11:17:08 DP5 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-28 11:17:08 DP1 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-28 11:17:08 DP2 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:08 DP0 TP0] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:08 DP0 TP0] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:09 DP4 TP4] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:09 DP4 TP4] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:09 DP3 TP3] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:09 DP3 TP3] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:09 DP1 TP1] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:09 DP1 TP1] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:09 DP2 TP2] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:09 DP2 TP2] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:09 DP5 TP5] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:09 DP5 TP5] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-28 11:17:10 DP7 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-28 11:17:10 DP6 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_dec_stage1_bf16_a16w16_subQ128_mqa128E Success
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:10 DP7 TP7] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:10 DP7 TP7] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:10 DP6 TP6] shape M:4096, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:10 DP6 TP6] shape M:4096, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP2 TP2] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP3 TP3] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP2 TP2] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP1 TP1] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP3 TP3] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP5 TP5] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP4 TP4] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP6 TP6] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP0 TP0] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP7 TP7] shape M:4096, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP1 TP1] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP5 TP5] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP4 TP4] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP6 TP6] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP0 TP0] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:11 DP7 TP7] shape M:4096, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:11 DP2 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:11 DP3 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:11 DP5 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:11 DP1 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:11 DP4 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:11 DP0 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:11 DP6 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:11 DP7 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[rank2]:[W1028 11:17:11.004267934 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank6]:[W1028 11:17:11.004341604 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank3]:[W1028 11:17:11.004354428 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank0]:[W1028 11:17:11.004362762 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank7]:[W1028 11:17:11.004336051 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank4]:[W1028 11:17:11.004339616 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank5]:[W1028 11:17:11.004402082 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
[rank1]:[W1028 11:17:11.005293935 HIPGraph.cpp:134] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())
Capturing batches (bs=512 avail_mem=50.68 GB):   2%|         | 1/52 [00:06<05:44,  6.75s/it]Capturing batches (bs=496 avail_mem=42.68 GB):   2%|         | 1/52 [00:06<05:44,  6.75s/it]Capturing batches (bs=496 avail_mem=42.68 GB):   4%|         | 2/52 [00:07<02:39,  3.18s/it]Capturing batches (bs=480 avail_mem=42.68 GB):   4%|         | 2/52 [00:07<02:39,  3.18s/it]Capturing batches (bs=480 avail_mem=42.68 GB):   6%|         | 3/52 [00:08<01:40,  2.05s/it]Capturing batches (bs=464 avail_mem=42.67 GB):   6%|         | 3/52 [00:08<01:40,  2.05s/it]Capturing batches (bs=464 avail_mem=42.67 GB):   8%|         | 4/52 [00:08<01:12,  1.52s/it]Capturing batches (bs=448 avail_mem=42.66 GB):   8%|         | 4/52 [00:08<01:12,  1.52s/it]Capturing batches (bs=448 avail_mem=42.66 GB):  10%|         | 5/52 [00:09<00:57,  1.22s/it]Capturing batches (bs=432 avail_mem=42.65 GB):  10%|         | 5/52 [00:09<00:57,  1.22s/it]Capturing batches (bs=432 avail_mem=42.65 GB):  12%|        | 6/52 [00:10<00:46,  1.01s/it]Capturing batches (bs=416 avail_mem=42.65 GB):  12%|        | 6/52 [00:10<00:46,  1.01s/it]Capturing batches (bs=416 avail_mem=42.65 GB):  13%|        | 7/52 [00:10<00:39,  1.14it/s]Capturing batches (bs=400 avail_mem=42.64 GB):  13%|        | 7/52 [00:10<00:39,  1.14it/s]Capturing batches (bs=400 avail_mem=42.64 GB):  15%|        | 8/52 [00:11<00:34,  1.27it/s]Capturing batches (bs=384 avail_mem=42.63 GB):  15%|        | 8/52 [00:11<00:34,  1.27it/s]Capturing batches (bs=384 avail_mem=42.63 GB):  17%|        | 9/52 [00:11<00:28,  1.50it/s]Capturing batches (bs=368 avail_mem=42.62 GB):  17%|        | 9/52 [00:11<00:28,  1.50it/s]Capturing batches (bs=368 avail_mem=42.62 GB):  19%|        | 10/52 [00:12<00:27,  1.54it/s]Capturing batches (bs=352 avail_mem=42.62 GB):  19%|        | 10/52 [00:12<00:27,  1.54it/s]Capturing batches (bs=352 avail_mem=42.62 GB):  21%|        | 11/52 [00:12<00:25,  1.58it/s]Capturing batches (bs=336 avail_mem=42.61 GB):  21%|        | 11/52 [00:12<00:25,  1.58it/s]Capturing batches (bs=336 avail_mem=42.61 GB):  23%|       | 12/52 [00:13<00:25,  1.60it/s]Capturing batches (bs=320 avail_mem=42.60 GB):  23%|       | 12/52 [00:13<00:25,  1.60it/s]Capturing batches (bs=320 avail_mem=42.60 GB):  25%|       | 13/52 [00:14<00:24,  1.62it/s]Capturing batches (bs=304 avail_mem=42.60 GB):  25%|       | 13/52 [00:14<00:24,  1.62it/s]Capturing batches (bs=304 avail_mem=42.60 GB):  27%|       | 14/52 [00:14<00:20,  1.83it/s]Capturing batches (bs=288 avail_mem=42.59 GB):  27%|       | 14/52 [00:14<00:20,  1.83it/s]Capturing batches (bs=288 avail_mem=42.59 GB):  29%|       | 15/52 [00:14<00:18,  1.99it/s]Capturing batches (bs=272 avail_mem=42.58 GB):  29%|       | 15/52 [00:14<00:18,  1.99it/s]Capturing batches (bs=272 avail_mem=42.58 GB):  31%|       | 16/52 [00:15<00:19,  1.86it/s]Capturing batches (bs=256 avail_mem=42.58 GB):  31%|       | 16/52 [00:15<00:19,  1.86it/s][aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP4 TP4] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP4 TP4] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP4 TP4] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP4 TP4] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP7 TP7] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP7 TP7] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP7 TP7] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP7 TP7] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP5 TP5] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP5 TP5] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP5 TP5] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP5 TP5] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP6 TP6] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP6 TP6] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP6 TP6] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP6 TP6] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP0 TP0] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP0 TP0] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP0 TP0] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP0 TP0] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP2 TP2] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP2 TP2] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP2 TP2] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP2 TP2] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP1 TP1] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP1 TP1] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP1 TP1] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP1 TP1] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP3 TP3] shape M:2048, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:20 DP3 TP3] shape M:2048, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP3 TP3] shape M:2048, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:20 DP3 TP3] shape M:2048, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=256 avail_mem=42.58 GB):  33%|      | 17/52 [00:16<00:19,  1.80it/s]Capturing batches (bs=248 avail_mem=42.57 GB):  33%|      | 17/52 [00:16<00:19,  1.80it/s]Capturing batches (bs=248 avail_mem=42.57 GB):  35%|      | 18/52 [00:16<00:19,  1.76it/s]Capturing batches (bs=240 avail_mem=42.56 GB):  35%|      | 18/52 [00:16<00:19,  1.76it/s]Capturing batches (bs=240 avail_mem=42.56 GB):  37%|      | 19/52 [00:17<00:18,  1.83it/s]Capturing batches (bs=232 avail_mem=42.55 GB):  37%|      | 19/52 [00:17<00:18,  1.83it/s]Capturing batches (bs=232 avail_mem=42.55 GB):  38%|      | 20/52 [00:17<00:17,  1.88it/s]Capturing batches (bs=224 avail_mem=42.55 GB):  38%|      | 20/52 [00:17<00:17,  1.88it/s]Capturing batches (bs=224 avail_mem=42.55 GB):  40%|      | 21/52 [00:18<00:17,  1.80it/s]Capturing batches (bs=216 avail_mem=42.54 GB):  40%|      | 21/52 [00:18<00:17,  1.80it/s]Capturing batches (bs=216 avail_mem=42.54 GB):  42%|     | 22/52 [00:18<00:16,  1.85it/s]Capturing batches (bs=208 avail_mem=42.53 GB):  42%|     | 22/52 [00:18<00:16,  1.85it/s]Capturing batches (bs=208 avail_mem=42.53 GB):  44%|     | 23/52 [00:19<00:14,  2.01it/s]Capturing batches (bs=200 avail_mem=42.53 GB):  44%|     | 23/52 [00:19<00:14,  2.01it/s]Capturing batches (bs=200 avail_mem=42.53 GB):  46%|     | 24/52 [00:19<00:14,  1.90it/s]Capturing batches (bs=192 avail_mem=42.52 GB):  46%|     | 24/52 [00:19<00:14,  1.90it/s][aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP5 TP5] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP7 TP7] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP4 TP4] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP6 TP6] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP0 TP0] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP3 TP3] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP2 TP2] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP1 TP1] shape M:1536, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:25 DP5 TP5] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:25 DP7 TP7] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:25 DP4 TP4] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:25 DP6 TP6] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:25 DP0 TP0] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:25 DP3 TP3] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:25 DP2 TP2] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:25 DP1 TP1] shape M:1536, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x128x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP5 TP5] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP4 TP4] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP0 TP0] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP7 TP7] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP2 TP2] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP3 TP3] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP5 TP5] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP6 TP6] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP4 TP4] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP1 TP1] shape M:1536, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP0 TP0] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP7 TP7] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP2 TP2] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP3 TP3] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP6 TP6] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:25 DP1 TP1] shape M:1536, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=192 avail_mem=42.52 GB):  48%|     | 25/52 [00:20<00:13,  2.04it/s]Capturing batches (bs=184 avail_mem=42.52 GB):  48%|     | 25/52 [00:20<00:13,  2.04it/s]Capturing batches (bs=184 avail_mem=42.52 GB):  50%|     | 26/52 [00:20<00:12,  2.16it/s]Capturing batches (bs=176 avail_mem=42.51 GB):  50%|     | 26/52 [00:20<00:12,  2.16it/s]Capturing batches (bs=176 avail_mem=42.51 GB):  52%|    | 27/52 [00:21<00:11,  2.24it/s]Capturing batches (bs=168 avail_mem=42.51 GB):  52%|    | 27/52 [00:21<00:11,  2.24it/s]Capturing batches (bs=168 avail_mem=42.51 GB):  54%|    | 28/52 [00:21<00:11,  2.04it/s]Capturing batches (bs=160 avail_mem=42.50 GB):  54%|    | 28/52 [00:21<00:11,  2.04it/s]Capturing batches (bs=160 avail_mem=42.50 GB):  56%|    | 29/52 [00:22<00:10,  2.15it/s]Capturing batches (bs=152 avail_mem=42.49 GB):  56%|    | 29/52 [00:22<00:10,  2.15it/s]Capturing batches (bs=152 avail_mem=42.49 GB):  58%|    | 30/52 [00:22<00:10,  2.11it/s]Capturing batches (bs=144 avail_mem=42.49 GB):  58%|    | 30/52 [00:22<00:10,  2.11it/s]Capturing batches (bs=144 avail_mem=42.49 GB):  60%|    | 31/52 [00:22<00:09,  2.21it/s]Capturing batches (bs=136 avail_mem=42.48 GB):  60%|    | 31/52 [00:22<00:09,  2.21it/s]Capturing batches (bs=136 avail_mem=42.48 GB):  62%|   | 32/52 [00:23<00:08,  2.27it/s]Capturing batches (bs=128 avail_mem=42.47 GB):  62%|   | 32/52 [00:23<00:08,  2.27it/s][aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP7 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP4 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP5 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP0 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP6 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP2 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP3 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP1 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP7 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP4 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP0 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP5 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP6 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP2 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP1 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-28 11:17:28 DP3 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP7 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP4 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP0 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP3 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP5 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP2 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP7 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP6 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP4 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP0 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP1 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP3 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP5 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP2 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP6 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:28 DP1 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
Capturing batches (bs=128 avail_mem=42.47 GB):  63%|   | 33/52 [00:23<00:08,  2.35it/s]Capturing batches (bs=120 avail_mem=42.47 GB):  63%|   | 33/52 [00:23<00:08,  2.35it/s][aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP7 TP7] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP4 TP4] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP6 TP6] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP3 TP3] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP2 TP2] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP5 TP5] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP1 TP1] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP0 TP0] [fused_moe] using default for (960, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.47 GB):  65%|   | 34/52 [00:24<00:08,  2.24it/s]Capturing batches (bs=112 avail_mem=42.46 GB):  65%|   | 34/52 [00:24<00:08,  2.24it/s][aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP4 TP4] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP0 TP0] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP5 TP5] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP6 TP6] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP3 TP3] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP2 TP2] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP7 TP7] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP1 TP1] [fused_moe] using default for (896, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.46 GB):  67%|   | 35/52 [00:24<00:07,  2.31it/s]Capturing batches (bs=104 avail_mem=42.46 GB):  67%|   | 35/52 [00:24<00:07,  2.31it/s][aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP2 TP2] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:29 DP0 TP0] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP3 TP3] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP4 TP4] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP1 TP1] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP6 TP6] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP5 TP5] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP7 TP7] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.46 GB):  69%|   | 36/52 [00:25<00:07,  2.20it/s]Capturing batches (bs=96 avail_mem=42.45 GB):  69%|   | 36/52 [00:25<00:07,  2.20it/s] [aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP4 TP4] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP2 TP2] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP3 TP3] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP1 TP1] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP5 TP5] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP7 TP7] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP0 TP0] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP6 TP6] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.45 GB):  71%|   | 37/52 [00:25<00:06,  2.29it/s]Capturing batches (bs=88 avail_mem=42.45 GB):  71%|   | 37/52 [00:25<00:06,  2.29it/s][aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP4 TP4] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP2 TP2] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP5 TP5] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP6 TP6] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP3 TP3] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP1 TP1] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP0 TP0] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:30 DP7 TP7] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.45 GB):  73%|  | 38/52 [00:26<00:06,  2.19it/s]Capturing batches (bs=80 avail_mem=42.44 GB):  73%|  | 38/52 [00:26<00:06,  2.19it/s][aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP2 TP2] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP4 TP4] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP5 TP5] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP0 TP0] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP7 TP7] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP3 TP3] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP6 TP6] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP1 TP1] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.44 GB):  75%|  | 39/52 [00:26<00:05,  2.27it/s]Capturing batches (bs=72 avail_mem=42.43 GB):  75%|  | 39/52 [00:26<00:05,  2.27it/s][aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP2 TP2] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP5 TP5] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP4 TP4] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP1 TP1] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP0 TP0] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP3 TP3] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP7 TP7] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:31 DP6 TP6] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.43 GB):  77%|  | 40/52 [00:26<00:05,  2.19it/s]Capturing batches (bs=64 avail_mem=42.43 GB):  77%|  | 40/52 [00:26<00:05,  2.19it/s][aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP7 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP4 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP5 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP0 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP2 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP6 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP1 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP3 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP7 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP4 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP5 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP0 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP2 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP6 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP3 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP1 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP4 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP5 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP7 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP2 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP0 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP6 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP3 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP1 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP4 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP7 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP5 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP2 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP0 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP6 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP3 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:17:32 DP1 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP4 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP7 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP5 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP0 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP6 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP2 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP3 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP1 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.43 GB):  79%|  | 41/52 [00:27<00:04,  2.26it/s]Capturing batches (bs=56 avail_mem=42.42 GB):  79%|  | 41/52 [00:27<00:04,  2.26it/s][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP7 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP4 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP5 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP6 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP3 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP1 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP0 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:32 DP2 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.42 GB):  81%|  | 42/52 [00:27<00:04,  2.19it/s]Capturing batches (bs=48 avail_mem=42.41 GB):  81%|  | 42/52 [00:27<00:04,  2.19it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP4 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP3 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP1 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP0 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP7 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP5 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP2 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP6 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.41 GB):  83%| | 43/52 [00:28<00:03,  2.26it/s]Capturing batches (bs=40 avail_mem=42.41 GB):  83%| | 43/52 [00:28<00:03,  2.26it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP6 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP1 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP3 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP2 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP4 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP0 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP7 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:17:33 DP5 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.41 GB):  85%| | 44/52 [00:28<00:03,  2.18it/s]Capturing batches (bs=32 avail_mem=42.40 GB):  85%| | 44/52 [00:28<00:03,  2.18it/s][rank2]:W1028 11:17:36.310000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:17:36.326000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1028 11:17:36.336000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:17:36.349000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1028 11:17:36.362000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1028 11:17:36.386000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1028 11:17:36.402000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:17:36.412000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:17:36.424000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:17:36.437000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:17:36.478000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:17:36.484000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:17:36.499000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1028 11:17:36.510000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:17:36.521000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:17:36.534000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:17:36.554000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1028 11:17:36.652000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1028 11:17:36.811000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:17:36.830000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1028 11:17:36.888000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:17:36.906000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:17:36.985000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:17:37.014000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:17:37.046000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:17:37.061000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:17:37.069000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1028 11:17:37.081000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:17:37.093000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1028 11:17:37.162000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:17:37.174000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:17:37.185000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:17:37.208000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:17:37.210000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:17:37.227000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:17:37.246000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:17:37.258000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:17:37.270000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:17:37.277000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:17:37.293000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:17:37.313000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:17:37.325000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:17:37.338000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:17:37.354000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:17:37.360000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:17:37.368000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:17:37.435000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:17:37.501000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:17:37.549000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:17:37.581000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:17:37.680000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:17:37.713000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:17:37.747000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:17:37.781000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:17:37.813000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:17:37.847000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:17:37.847000 819 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank2]:W1028 11:17:37.850000 814 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank5]:W1028 11:17:37.864000 817 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank0]:W1028 11:17:37.878000 812 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank4]:W1028 11:17:37.885000 816 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank3]:W1028 11:17:38.042000 815 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank6]:W1028 11:17:38.405000 818 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
[rank1]:W1028 11:17:38.457000 813 torch/_inductor/utils.py:1349] [27/0] Please pip install Composable Kernel package
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0105 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0107 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_19 0.0107 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0107 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0109 ms 93.8% 
  triton_bmm_15 0.0111 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0113 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0113 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0115 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3098 seconds and 0.5211 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0106 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0107 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0108 ms 97.0% 
  triton_bmm_25 0.0109 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_19 0.0109 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0112 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0114 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_16 0.0115 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_20 0.0116 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3717 seconds and 0.5196 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0105 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0107 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0107 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0109 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0109 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_9 0.0113 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0113 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0114 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0115 ms 89.5% 
SingleProcess AUTOTUNE benchmarking takes 5.3538 seconds and 0.6037 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0105 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0106 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0107 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0107 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0109 ms 96.3% 
  triton_bmm_15 0.0111 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0113 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0114 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0114 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3629 seconds and 0.6508 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0106 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0107 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0108 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0108 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_19 0.0109 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0111 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0112 ms 95.0% 
  triton_bmm_16 0.0115 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0115 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0117 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3492 seconds and 0.6515 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0106 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0107 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0108 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0109 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0109 ms 97.8% 
  triton_bmm_25 0.0110 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0112 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0116 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0117 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0118 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3622 seconds and 0.4478 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0107 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0108 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0108 ms 98.9% 
  triton_bmm_19 0.0110 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_25 0.0110 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_21 0.0110 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0116 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_15 0.0116 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_22 0.0116 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_20 0.0119 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3341 seconds and 0.4694 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x128, 128x128x512)
  triton_bmm_17 0.0106 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_25 0.0107 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_23 0.0107 ms 98.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_21 0.0109 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0110 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0113 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0114 ms 93.0% 
  triton_bmm_9 0.0115 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_16 0.0117 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_22 0.0117 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3479 seconds and 0.4668 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1028 11:17:49.533000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:17:49.712000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:17:49.856000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:17:50.064000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:17:50.066000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:17:50.231000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:17:50.348000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:17:50.368000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:17:50.530000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:17:50.597000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:17:50.642000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:17:50.857000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1028 11:17:50.941000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:17:51.043000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:17:51.155000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1028 11:17:51.444000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1028 11:17:51.573000 819 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1028 11:17:51.922000 812 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank4]:W1028 11:17:52.073000 816 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1028 11:17:52.100000 813 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank6]:W1028 11:17:52.226000 818 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1028 11:17:52.585000 814 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank3]:W1028 11:17:52.828000 815 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
[rank5]:W1028 11:17:53.014000 817 torch/_inductor/utils.py:1349] [39/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_48 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_41 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_46 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0105 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0106 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0107 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0107 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0111 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3630 seconds and 0.6180 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_39 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_46 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_41 0.0103 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0105 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0107 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0107 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0109 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0113 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0113 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2829 seconds and 0.6486 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_39 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_41 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0105 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0106 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0106 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0110 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0110 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0111 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0112 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0114 ms 90.5% 
SingleProcess AUTOTUNE benchmarking takes 5.2775 seconds and 0.6497 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_41 0.0104 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0104 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0104 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0104 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0105 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0109 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0110 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0111 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_45 0.0113 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0115 ms 90.6% 
SingleProcess AUTOTUNE benchmarking takes 5.2833 seconds and 0.6467 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_39 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_41 0.0104 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0104 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0104 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0107 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0109 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0110 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_47 0.0111 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0113 ms 91.1% 
  triton_bmm_45 0.0113 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3467 seconds and 0.6141 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_39 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_48 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_41 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0102 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0103 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_49 0.0107 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0107 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0108 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0111 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_45 0.0111 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2729 seconds and 0.6307 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_48 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0103 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_41 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0106 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0107 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0110 ms 93.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0111 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_31 0.0112 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0115 ms 88.9% 
SingleProcess AUTOTUNE benchmarking takes 5.3020 seconds and 0.6297 seconds precompiling for 27 choices
AUTOTUNE bmm(128x32x512, 128x512x128)
  triton_bmm_46 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0103 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_41 0.0103 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_39 0.0104 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_47 0.0107 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0108 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0108 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0109 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_31 0.0112 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0114 ms 90.5% 
SingleProcess AUTOTUNE benchmarking takes 5.2823 seconds and 0.6068 seconds precompiling for 27 choices
[rank0]:W1028 11:17:59.215000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:17:59.242000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:17:59.290000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:17:59.316000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:17:59.362000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:17:59.387000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:17:59.414000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:17:59.437000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:17:59.535000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:17:59.539000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:17:59.615000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:17:59.712000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:17:59.795000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:17:59.821000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:17:59.872000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:17:59.897000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:17:59.972000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:17:59.996000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:00.149000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:00.225000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:00.242000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:00.319000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:00.323000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:00.417000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:03.391000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:03.467000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:03.566000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:03 DP7 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1028 11:18:04.020000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:04.095000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:04.148000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:04.192000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:04.215000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:04.222000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:04 DP2 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank4]:W1028 11:18:04.290000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:04.319000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:04 DP0 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank4]:W1028 11:18:04.389000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:04 DP4 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1028 11:18:04.479000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:04.554000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:04.566000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:04.641000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:04.651000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:04 DP6 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1028 11:18:04.739000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:04.746000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:04 DP5 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1028 11:18:04.820000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:04.918000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:04 DP1 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1028 11:18:05.104000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:05.178000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:05.286000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:05 DP3 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1028 11:18:05.855000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:05.883000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:05.906000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:05.929000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:05.936000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:05.960000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:05.960000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:05.980000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:06.011000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:06.027000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:06.034000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:06.061000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:06.079000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:06.110000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:06 DP2 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:06 DP6 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1028 11:18:06.117000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:06 DP7 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1028 11:18:06.133000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:06 DP4 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:06 DP5 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1028 11:18:06.193000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:06.292000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:06.318000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:06 DP1 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1028 11:18:06.393000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:06.491000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:06.511000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:06 DP0 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1028 11:18:06.586000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:06.684000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:18:06 DP3 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1028 11:18:07.251000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:07.261000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:07.270000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:07.285000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:07.296000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:07.304000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:07.317000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:07.346000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:07.491000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:07.501000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:07.510000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:07.524000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:07.535000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:07.545000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:07.557000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:07.586000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:07.731000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:07.740000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:07.750000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:07.761000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:07.775000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:07.785000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:07.797000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:07.827000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:18:08 DP6 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:18:08 DP7 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:18:08 DP4 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:18:08 DP0 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:18:08 DP5 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:18:08 DP1 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:18:08 DP2 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:18:08 DP3 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1028 11:18:09.401000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:09.421000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:09.433000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:09.449000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:09.455000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:09.476000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:09.490000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:09.496000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:09.498000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:09.507000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:09.524000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:09.531000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:09.552000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:09.566000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:09.572000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:09.574000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:09.582000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:09.601000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:18:09 DP7 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[rank2]:W1028 11:18:09.606000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:18:09 DP6 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:18:09 DP0 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[rank1]:W1028 11:18:09.652000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:18:09 DP5 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[rank3]:W1028 11:18:09.661000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:18:09 DP2 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:18:09 DP1 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:18:09 DP3 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1028 11:18:09.881000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:09.956000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:10.038000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:18:10 DP4 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0142 ms 100.0% 
  triton_mm_55 0.0275 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 33.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0456 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0534 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0611 ms 23.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0756 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2114 seconds and 0.6563 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0145 ms 100.0% 
  triton_mm_55 0.0287 ms 50.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0423 ms 34.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0456 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0506 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0536 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0585 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0610 ms 23.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0756 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2077 seconds and 0.5325 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0149 ms 100.0% 
  triton_mm_55 0.0317 ms 47.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0423 ms 35.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0457 ms 32.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0507 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0535 ms 27.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0583 ms 25.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0584 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0611 ms 24.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_59 0.0837 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.1868 seconds and 0.4953 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0146 ms 100.0% 
  triton_mm_55 0.0314 ms 46.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0419 ms 34.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0455 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0504 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0533 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0580 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0609 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0755 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2914 seconds and 0.6600 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0143 ms 100.0% 
  triton_mm_55 0.0277 ms 51.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 34.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0456 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0505 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0533 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 24.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_69 0.0583 ms 24.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_66 0.0610 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0759 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2174 seconds and 0.5258 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0148 ms 100.0% 
  triton_mm_55 0.0321 ms 45.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0423 ms 34.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0458 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0507 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0536 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 25.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_69 0.0584 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_66 0.0610 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0830 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2455 seconds and 0.6537 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0145 ms 100.0% 
  triton_mm_55 0.0273 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0421 ms 34.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0455 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0504 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0533 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0582 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0582 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0610 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0751 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2592 seconds and 0.5398 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x256)
  mm 0.0153 ms 100.0% 
  triton_mm_55 0.0273 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_61 0.0420 ms 36.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0456 ms 33.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_68 0.0505 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_60 0.0532 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_69 0.0580 ms 26.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0583 ms 26.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0609 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_53 0.0753 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2395 seconds and 0.4373 seconds precompiling for 39 choices
[rank6]:W1028 11:18:20.501000 818 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5586b70420>
[rank7]:W1028 11:18:20.512000 819 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8de6f91830>
[rank0]:W1028 11:18:20.523000 812 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29c930>
[rank6]:W1028 11:18:20.533000 818 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5585401740>
[rank7]:W1028 11:18:20.544000 819 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fcf975d0ae0>
[rank1]:W1028 11:18:20.554000 813 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb2c27d61c0>
[rank2]:W1028 11:18:20.566000 814 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1d170>
[rank1]:W1028 11:18:20.585000 813 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ff46b548ae0>
[rank0]:W1028 11:18:20.600000 812 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29cbd0>
[rank3]:W1028 11:18:20.613000 815 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cff0>
[rank3]:W1028 11:18:20.645000 815 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cae0>
[rank2]:W1028 11:18:20.645000 814 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1c750>
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:18:20 DP6 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:18:20 DP1 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:18:20 DP7 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:18:20 DP0 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:18:20 DP2 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1028 11:18:20.794000 817 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72bd280420>
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:18:20 DP3 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1028 11:18:20.873000 817 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72ba9db9c0>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:18:20 DP5 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1028 11:18:21.205000 816 torch/_dynamo/variables/builtin.py:1091] [68/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28daa9f9f0>
[rank4]:W1028 11:18:21.237000 816 torch/_dynamo/variables/builtin.py:1091] [69/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28d957f9f0>
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:18:21 DP4 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1028 11:18:23.074000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:23.081000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:23.090000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:23.099000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:23.109000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:23.119000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:23.130000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:23.186000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:23.318000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:23.325000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:23.333000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:23.351000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:23.362000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:23.373000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:23.385000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:23.426000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:23.557000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:23.565000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:23.577000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:23.591000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:23.602000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:23.613000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:23.625000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:23.666000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:23.797000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:23.809000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:23.822000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:23.831000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:23.841000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:23.853000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:23.864000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:23.906000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:24.037000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:24.049000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:24.065000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:24.075000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:24.085000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:24.096000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:24.107000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:24.146000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:24.278000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:24.289000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:24.309000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:24.319000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:24.329000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:24.340000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:24.351000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:24.386000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:24.517000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:24.529000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:24.553000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:24.563000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:24.573000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:24.585000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:24.596000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:24.626000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:24.758000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:24.797000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:24.808000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:24.825000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:24.839000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:24.869000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:24.878000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:24.891000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:25.022000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:25.042000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:25.052000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:25.070000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:25.080000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:25.118000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:25.132000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:25.167000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:25.302000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:25.317000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:25.326000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:25.335000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:25.347000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:25.358000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:25.375000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:25.410000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:25.546000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:25.562000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:25.570000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:25.580000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:25.591000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:25.601000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:25.615000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:25.654000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:25.789000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:25.805000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:25.814000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:25.836000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:25.845000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:25.857000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:25.875000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:25.898000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:26.050000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:26.058000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:26.079000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:26.090000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:26.100000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:26.109000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:26.120000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:26.142000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:26.293000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:26.301000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:26.323000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:26.333000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:26.347000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:26.356000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:26.366000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:26.382000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:26.537000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:26.545000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:26.567000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:26.576000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:26.587000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:26.602000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:26.611000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:26.622000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:26.781000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:26.789000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:26.807000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:26.817000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:26.828000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:26.846000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:26.855000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:26.866000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:27.025000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:27.033000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:27.051000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:27.061000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:27.072000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:27.090000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:27.099000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:27.110000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:27.269000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:27.277000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:27.295000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:27.304000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:27.315000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:27.334000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:27.343000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:27.354000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:27.513000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:27.521000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:27.539000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:27.548000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:27.559000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:27.578000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:27.587000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:27.598000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:27.757000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:27.765000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:27.784000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:27.793000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:27.803000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:27.822000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:27.831000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:27.842000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:28.001000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:28.009000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:28.028000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:28.037000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:28.048000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:28.066000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:28.076000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:28.088000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:28.254000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:28.276000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:28.306000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:28.314000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:28.325000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:28.335000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:28.347000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:28.358000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:28.498000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:28.532000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:28.554000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:28.563000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:28.579000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:28.596000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:28.607000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:28.638000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:28.802000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:28.810000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:28.818000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:28.828000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:28.840000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:28.850000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:28.862000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:28.886000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:29.046000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:29.053000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:29.062000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:29.073000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:29.085000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:29.094000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:29.106000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:29.130000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:29.290000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:29.298000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:29.306000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:29.331000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:29.341000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:29.353000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:29.375000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:29.385000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:29.534000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:29.545000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:29.580000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:29.589000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:29.600000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:29.612000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:29.621000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:29.632000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:29.778000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:29.789000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:29.823000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:29.833000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:29.844000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:29.858000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:29.867000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:29.878000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:30.021000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:30.033000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:30.067000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:30.077000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:30.088000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:30.102000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:30.111000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:30.122000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:30.265000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:30.277000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:30.312000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:30.321000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:30.332000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:30.346000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:30.355000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:30.366000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:30.509000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:30.521000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:30.557000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:30.565000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:30.576000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:30.590000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:30.599000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:30.610000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:30.753000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:30.765000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:30.803000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:30.812000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:30.824000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:30.835000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:30.844000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:30.855000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:30.998000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:31.009000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:31.047000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:31.057000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:31.068000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:31.079000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:31.088000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:31.099000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:31.241000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:31.253000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:31.291000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:31.300000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:31.311000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:31.322000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:31.331000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:31.342000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:31.485000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:31.497000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:31.535000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:31.545000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:31.557000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:31.568000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:31.577000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:31.588000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:31.731000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:31.743000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:31.778000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:31.786000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:31.798000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:31.816000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:31.824000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:31.836000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:31.980000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:32.030000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:32.056000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:32.066000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:32.075000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:32.085000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:32.094000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:32.114000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:32.251000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:32.277000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:32.303000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:32.313000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:32.322000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:32.337000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:32.358000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:32.396000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:32.555000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:32.565000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:32.574000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:32.582000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:32.591000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:32.601000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:32.610000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:32.643000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:32.802000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:32.810000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:32.818000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:32.831000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:32.840000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:32.852000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:32.863000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:32.886000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:33.049000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:33.058000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:33.065000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:33.076000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:33.086000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:33.098000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:33.109000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:33.134000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:33.297000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:33.306000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:33.314000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:33.330000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:33.343000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:33.355000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:33.382000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:33.392000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:33.546000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:33.554000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:33.575000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:33.591000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:33.602000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:33.626000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:33.635000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:33.647000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:33.797000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:33.805000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:33.830000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:33.847000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:33.859000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:33.875000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:33.883000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:33.899000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:34.045000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:34.053000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:34.074000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:34.091000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:34.103000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:34.122000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:34.131000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:34.143000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:34.293000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:34.302000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:34.314000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:34.335000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:34.347000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:34.371000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:34.380000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:34.390000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:34.545000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:34.553000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:34.562000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:34.579000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:34.591000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:34.618000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:34.629000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:34.639000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:34.790000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:34.802000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:34.811000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:34.823000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:34.835000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:34.866000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:34.877000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:34.887000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:35.034000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:35.046000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:35.058000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:35.068000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:35.079000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:35.114000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:35.125000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:35.134000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:35.278000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:35.290000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:35.306000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:35.316000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:35.327000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:35.362000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:35.371000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:35.383000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:35.522000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:35.534000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:35.553000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:35.564000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:35.579000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:35.611000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:35.619000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:35.635000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:35.770000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:35.782000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:35.802000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:35.811000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:35.823000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:35.859000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:35.868000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:35.888000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:36.026000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:36.064000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:36.098000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:36.110000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:36.119000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:36.129000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:36.141000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:36.152000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:36.289000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:36.315000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:36.346000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:36.358000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:36.367000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:36.378000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:36.389000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:36.401000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:36.536000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:36.562000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:36.589000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:36.608000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:36.628000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:36.637000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:36.648000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:36.692000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:36.837000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:36.855000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:36.865000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:36.879000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:36.889000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:36.897000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:36.907000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:36.943000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:37.085000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:37.103000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:37.113000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:37.127000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:37.141000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:37.150000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:37.160000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:37.187000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:37.933000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:37.943000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:37.954000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:37.984000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:37.991000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:38.002000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:38.025000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:38.033000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1802 ms 100.0% 
  triton_mm_127 0.2454 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2523 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2589 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2625 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_107 0.2632 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_112 0.2640 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2657 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2730 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2936 ms 61.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7948 seconds and 0.7758 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1838 ms 100.0% 
  triton_mm_127 0.2491 ms 73.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2579 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2616 ms 70.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2667 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2669 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2700 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2738 ms 67.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2842 ms 64.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.2988 ms 61.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9513 seconds and 0.6435 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1836 ms 100.0% 
  triton_mm_127 0.2493 ms 73.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2563 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2623 ms 70.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2648 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2681 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_107 0.2698 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_125 0.2707 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2769 ms 66.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2990 ms 61.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.8634 seconds and 0.7134 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1832 ms 100.0% 
  triton_mm_127 0.2481 ms 73.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2549 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2608 ms 70.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2642 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2644 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2660 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2689 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2701 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_105 0.2957 ms 62.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.8804 seconds and 0.4864 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1854 ms 100.0% 
  triton_mm_127 0.2476 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2559 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2632 ms 70.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2681 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2683 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2686 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2757 ms 67.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2849 ms 65.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.2982 ms 62.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9281 seconds and 0.6589 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1801 ms 100.0% 
  triton_mm_127 0.2439 ms 73.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2517 ms 71.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2607 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2632 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2647 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2652 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2706 ms 66.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2800 ms 64.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.2923 ms 61.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.8509 seconds and 0.5643 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1834 ms 100.0% 
  triton_mm_127 0.2489 ms 73.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2562 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2615 ms 70.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2661 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2664 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2715 ms 67.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2777 ms 66.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2853 ms 64.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_120 0.2981 ms 61.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9309 seconds and 0.7077 seconds precompiling for 39 choices
AUTOTUNE mm(256x7168, 7168x16160)
  mm 0.1824 ms 100.0% 
  triton_mm_127 0.2489 ms 73.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_126 0.2561 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_113 0.2627 ms 69.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_124 0.2657 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_112 0.2676 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_125 0.2721 ms 67.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_107 0.2759 ms 66.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_106 0.2824 ms 64.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_121 0.3004 ms 60.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.9543 seconds and 0.4165 seconds precompiling for 39 choices
Capturing batches (bs=32 avail_mem=42.40 GB):  87%| | 45/52 [01:47<02:48, 24.01s/it]Capturing batches (bs=24 avail_mem=41.76 GB):  87%| | 45/52 [01:47<02:48, 24.01s/it][rank2]:W1028 11:18:54.347000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:54.351000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:54.361000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:54.371000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:54.381000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:54.394000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:54.406000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:54.415000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:54.419000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:54.430000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:54.439000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:54.448000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:54.462000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:54.474000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:54.487000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:54.491000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:54.502000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:54.511000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:54.520000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:54.534000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:54.545000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:54.557000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:54.624000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:54.696000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:54.980000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:54.989000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:54.997000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:55.006000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:55.015000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:55.024000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:55.037000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:55.062000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:55.073000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:55.078000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:55.087000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:55.094000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:55.103000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:55.121000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:55.130000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:55.142000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:55.146000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:55.155000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:55.162000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:55.171000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:55.185000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:55.189000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:55.197000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:18:55.210000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:18:55.213000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:18:55.223000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:18:55.229000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:18:55.238000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:18:55.255000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:55.267000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:55.335000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:18:55.402000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:18:55.661000 812 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank2]:W1028 11:18:55.675000 814 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank5]:W1028 11:18:55.678000 817 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank6]:W1028 11:18:55.687000 818 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank7]:W1028 11:18:55.692000 819 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank4]:W1028 11:18:55.704000 816 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank1]:W1028 11:18:55.715000 813 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
[rank3]:W1028 11:18:56.011000 815 torch/_inductor/utils.py:1349] [27/1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0100 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_144 0.0101 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0101 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0101 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0101 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0103 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0104 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0105 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3295 seconds and 0.3244 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0101 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0102 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_143 0.0102 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_151 0.0102 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0103 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0104 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3219 seconds and 0.3530 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0100 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0101 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0101 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3104 seconds and 0.5013 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_144 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_145 0.0101 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_148 0.0101 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0102 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0102 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_153 0.0103 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0104 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0105 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0108 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2728 seconds and 0.5298 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_151 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_145 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_153 0.0103 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_148 0.0103 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0104 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0105 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0105 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3414 seconds and 0.3889 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_148 0.0102 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_151 0.0102 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_153 0.0103 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0103 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0103 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0105 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0107 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2692 seconds and 0.3565 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_144 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_145 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_148 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0104 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_147 0.0104 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_153 0.0104 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0105 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_149 0.0107 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_143 0.0107 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0111 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2689 seconds and 0.4000 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x128, 128x128x512)
  triton_bmm_145 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_144 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_148 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_147 0.0102 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_151 0.0102 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0103 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_153 0.0103 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_143 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_149 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_137 0.0105 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2942 seconds and 0.3836 seconds precompiling for 27 choices
[rank1]:W1028 11:19:06.627000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:06.697000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:06.892000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:07.127000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:07.205000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:07.404000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:07.573000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:07.831000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:08.069000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:08.162000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:08.242000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:08.333000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:08.663000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:08.672000 817 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank6]:W1028 11:19:08.695000 818 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank3]:W1028 11:19:08.741000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:08.843000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:09.132000 813 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank4]:W1028 11:19:09.353000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:09.373000 819 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank2]:W1028 11:19:09.658000 814 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank0]:W1028 11:19:09.977000 812 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank3]:W1028 11:19:10.087000 815 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
[rank4]:W1028 11:19:10.831000 816 torch/_inductor/utils.py:1349] [39/1_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0101 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_174 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_169 0.0101 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_165 0.0103 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0104 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_179 0.0106 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_175 0.0107 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0108 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0109 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0110 ms 91.6% 
SingleProcess AUTOTUNE benchmarking takes 5.2614 seconds and 0.5221 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0100 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0104 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0104 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0104 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0109 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0109 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_175 0.0109 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0111 ms 89.5% 
  triton_bmm_159 0.0111 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3009 seconds and 0.5267 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0100 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0100 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_174 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0101 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0103 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0106 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0106 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0108 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0110 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0111 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3771 seconds and 0.5292 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0099 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_174 0.0099 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0099 ms 98.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0101 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_175 0.0102 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0103 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_177 0.0105 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0107 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0107 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3225 seconds and 0.5219 seconds precompiling for 27 choices
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0099 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0099 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0100 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_176 0.0100 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_179 0.0101 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_175 0.0102 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0104 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_173 0.0107 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_170 0.0111 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2706 seconds and 0.5208 seconds precompiling for 27 choices
[rank5]:W1028 11:19:15.739000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0099 ms 99.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0101 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0101 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_179 0.0104 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_165 0.0104 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_175 0.0104 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0107 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0111 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_173 0.0111 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2365 seconds and 0.5254 seconds precompiling for 27 choices
[rank5]:W1028 11:19:15.815000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:15.914000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_169 0.0099 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_167 0.0100 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_176 0.0103 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_165 0.0104 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0104 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_179 0.0106 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_175 0.0107 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0109 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0109 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0113 ms 87.5% 
SingleProcess AUTOTUNE benchmarking takes 5.3154 seconds and 0.5156 seconds precompiling for 27 choices
[rank6]:W1028 11:19:16.032000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:16.109000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:16.208000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:16.263000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:16.338000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:16.435000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:16.448000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:16.524000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:16.623000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x24x512, 128x512x128)
  triton_bmm_167 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_169 0.0098 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_174 0.0102 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_176 0.0102 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_175 0.0105 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_179 0.0105 ms 92.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_165 0.0106 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_177 0.0107 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_159 0.0114 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0115 ms 85.3% 
SingleProcess AUTOTUNE benchmarking takes 5.3106 seconds and 0.5247 seconds precompiling for 27 choices
[rank0]:W1028 11:19:16.966000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:17.014000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:17.042000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:17.091000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:17.140000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:17.167000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:17.190000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:17.244000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:17.344000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:17.913000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:17.989000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:18.087000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:20.059000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:20.136000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:20.237000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:20.285000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:20.360000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:20.452000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:20.469000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:20.529000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:20.630000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:21.166000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:21.240000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:21.337000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:21.682000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:21.759000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:21.837000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:21.859000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:21.897000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:21.938000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:21.959000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:21.972000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:22.071000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:22.648000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:22.725000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:22.826000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:23.402000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:23.420000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:23.437000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:23.446000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:23.479000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:23.499000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:23.510000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:23.518000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:23.525000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:23.568000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:23.583000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:23.587000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:23.623000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:23.645000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:23.681000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:23.693000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:23.732000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:23.747000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:23.754000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:23.758000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:23.858000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:24.133000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:24.209000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:24.309000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:24.870000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:24.880000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:24.888000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:24.897000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:24.905000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:24.917000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:24.929000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:24.955000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:25.122000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:25.132000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:25.140000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:25.149000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:25.158000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:25.169000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:25.181000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:25.199000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:25.374000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:25.384000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:25.392000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:25.401000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:25.410000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:25.422000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:25.433000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:25.446000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:27.000000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:27.018000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:27.039000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:27.050000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:27.057000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:27.068000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:27.075000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:27.076000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:27.086000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:27.095000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:27.115000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:27.123000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:27.125000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:27.133000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:27.143000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:27.144000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:27.152000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:27.162000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:27.164000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:27.173000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:27.181000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:27.191000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:27.201000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:27.210000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0132 ms 100.0% 
  triton_mm_183 0.0275 ms 48.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0409 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0456 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0595 ms 22.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0612 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0752 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1896 seconds and 0.7696 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0132 ms 100.0% 
  triton_mm_183 0.0275 ms 48.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0409 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0454 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0532 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 22.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0612 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0752 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1854 seconds and 0.8300 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0127 ms 100.0% 
  triton_mm_183 0.0275 ms 46.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0495 ms 25.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0532 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0581 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0595 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0611 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0753 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2840 seconds and 0.8021 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0134 ms 100.0% 
  triton_mm_183 0.0275 ms 48.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0410 ms 32.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0456 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0499 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0583 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0752 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2301 seconds and 0.5995 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0134 ms 100.0% 
  triton_mm_183 0.0275 ms 48.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 32.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0533 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 23.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0600 ms 22.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0753 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1938 seconds and 0.6277 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0132 ms 100.0% 
  triton_mm_183 0.0275 ms 48.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0408 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0454 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0533 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0582 ms 22.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0596 ms 22.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0758 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2700 seconds and 0.7354 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0134 ms 100.0% 
  triton_mm_183 0.0275 ms 48.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0411 ms 32.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0455 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0499 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0535 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0584 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0598 ms 22.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0756 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2780 seconds and 0.8040 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x256)
  mm 0.0134 ms 100.0% 
  triton_mm_183 0.0277 ms 48.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_189 0.0409 ms 32.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_182 0.0456 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.0497 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_188 0.0534 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_197 0.0583 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_195 0.0599 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_194 0.0613 ms 21.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_181 0.0754 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2033 seconds and 0.6477 seconds precompiling for 39 choices
[rank2]:W1028 11:19:37.931000 814 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1d170>
[rank2]:W1028 11:19:37.955000 814 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1c750>
[rank7]:W1028 11:19:38.007000 819 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8de6f91830>
[rank3]:W1028 11:19:38.017000 815 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cff0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:19:38 DP2 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1028 11:19:38.030000 819 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fcf975d0ae0>
[rank4]:W1028 11:19:38.038000 816 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28daa9f9f0>
[rank3]:W1028 11:19:38.039000 815 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cae0>
[rank6]:W1028 11:19:38.049000 818 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5586b70420>
[rank5]:W1028 11:19:38.059000 817 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72bd280420>
[rank4]:W1028 11:19:38.061000 816 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28d957f9f0>
[rank1]:W1028 11:19:38.066000 813 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb2c27d61c0>
[rank6]:W1028 11:19:38.072000 818 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5585401740>
[rank5]:W1028 11:19:38.082000 817 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72ba9db9c0>
[rank1]:W1028 11:19:38.089000 813 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ff46b548ae0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:19:38 DP7 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:19:38 DP3 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:19:38 DP4 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:19:38 DP6 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1028 11:19:38.139000 812 torch/_dynamo/variables/builtin.py:1091] [68/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29c930>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:19:38 DP5 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:19:38 DP1 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1028 11:19:38.162000 812 torch/_dynamo/variables/builtin.py:1091] [69/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29cbd0>
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:19:38 DP0 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1028 11:19:39.439000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:39.449000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:39.458000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:39.475000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:39.484000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:39.496000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:39.530000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:39.556000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:39.698000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:39.714000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:39.723000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:39.735000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:39.752000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:39.783000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:39.813000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:39.829000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:39.977000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:39.985000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:40.014000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:40.040000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:40.070000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:40.086000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:40.101000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:40.112000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:40.251000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:40.259000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:40.268000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:40.292000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:40.322000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:40.334000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:40.353000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:40.364000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:40.503000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:40.511000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:40.520000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:40.540000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:40.574000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:40.583000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:40.600000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:40.612000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:40.751000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:40.759000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:40.770000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:40.787000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:40.822000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:40.834000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:40.846000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:40.858000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:40.999000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:41.007000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:41.022000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:41.035000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:41.075000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:41.085000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:41.095000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:41.107000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:41.247000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:41.255000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:41.274000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:41.285000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:41.326000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:41.336000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:41.347000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:41.359000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:41.496000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:41.503000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:41.526000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:41.536000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:41.578000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:41.589000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:41.600000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:41.612000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:41.751000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:41.759000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:41.778000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:41.788000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:41.830000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:41.841000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:41.851000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:41.863000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:42.002000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:42.012000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:42.032000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:42.041000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:42.088000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:42.101000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:42.110000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:42.121000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:42.258000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:42.267000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:42.284000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:42.294000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:42.336000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:42.348000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:42.358000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:42.370000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:42.510000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:42.519000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:42.532000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:42.546000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:42.584000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:42.595000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:42.606000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:42.618000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:42.758000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:42.767000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:42.780000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:42.798000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:42.832000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:42.843000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:42.854000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:42.866000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:43.006000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:43.015000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:43.027000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:43.050000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:43.080000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:43.092000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:43.106000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:43.116000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:43.254000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:43.263000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:43.276000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:43.302000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:43.328000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:43.339000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:43.358000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:43.369000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:43.510000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:43.520000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:43.529000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:43.556000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:43.574000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:43.586000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:43.613000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:43.624000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:43.761000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:43.776000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:43.786000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:43.808000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:43.825000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:43.838000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:43.860000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:43.872000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:44.019000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:44.028000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:44.040000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:44.054000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:44.080000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:44.091000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:44.107000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:44.117000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:44.267000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:44.278000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:44.292000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:44.306000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:44.328000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:44.340000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:44.358000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:44.370000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:44.514000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:44.528000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:44.538000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:44.560000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:44.574000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:44.586000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:44.612000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:44.623000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:44.761000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:44.776000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:44.790000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:44.812000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:44.826000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:44.839000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:44.860000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:44.873000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:45.042000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:45.053000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:45.062000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:45.084000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:45.096000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:45.106000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:45.118000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:45.150000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:45.294000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:45.308000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:45.318000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:45.335000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:45.347000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:45.358000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:45.370000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:45.406000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:45.548000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:45.558000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:45.572000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:45.586000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:45.595000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:45.612000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:45.623000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:45.649000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:45.868000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:45.877000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:45.887000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:45.898000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:45.907000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:45.917000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:45.926000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:45.982000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:46.149000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:46.158000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:46.171000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:46.191000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:46.240000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:46.286000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:46.295000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:46.308000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:46.450000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:46.458000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:46.467000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:46.500000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:46.537000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:46.547000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:46.557000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:46.570000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:46.773000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:46.782000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:46.794000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:46.804000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:46.814000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:46.826000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:46.875000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:46.884000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:47.032000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:47.042000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:47.053000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:47.063000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:47.073000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:47.084000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:47.131000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:47.141000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:47.288000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:47.298000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:47.309000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:47.319000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:47.329000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:47.339000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:47.386000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:47.397000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:47.548000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:47.557000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:47.569000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:47.578000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:47.588000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:47.600000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:47.642000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:47.653000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:47.804000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:47.813000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:47.824000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:47.834000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:47.844000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:47.856000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:47.900000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:47.908000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:48.068000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:48.077000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:48.089000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:48.098000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:48.107000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:48.119000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:48.154000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:48.164000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:48.324000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:48.333000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:48.344000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:48.354000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:48.364000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:48.380000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:48.410000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:48.421000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:48.574000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:48.582000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:48.594000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:48.608000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:48.620000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:48.630000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:48.669000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:48.680000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:48.830000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:48.837000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:48.850000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:48.861000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:48.872000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:48.886000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:48.920000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:48.932000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:49.086000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:49.093000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:49.106000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:49.117000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:49.128000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:49.142000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:49.172000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:49.184000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:49.342000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:49.349000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:49.363000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:49.372000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:49.383000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:49.398000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:49.425000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:49.435000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:49.599000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:49.611000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:49.619000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:49.631000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:49.640000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:49.656000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:49.676000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:49.685000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:49.851000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:49.864000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:49.873000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:49.884000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:49.894000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:49.908000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:49.930000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:49.941000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:50.115000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:50.126000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:50.136000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:50.149000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:50.158000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:50.172000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:50.188000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:50.197000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:50.367000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:50.378000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:50.389000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:50.400000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:50.410000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:50.424000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:50.444000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:50.453000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:50.618000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:50.632000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:50.641000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:50.650000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:50.668000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:50.682000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:50.701000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:50.712000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:50.873000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:50.884000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:50.898000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:50.908000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:50.920000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:50.938000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:50.952000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:50.964000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:51.131000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:51.139000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:51.154000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:51.164000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:51.174000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:51.194000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:51.206000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:51.218000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:51.379000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:51.393000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:51.410000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:51.419000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:51.430000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:51.450000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:51.461000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:51.473000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:51.627000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:51.645000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:51.666000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:51.675000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:51.686000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:51.706000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:51.717000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:51.730000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:51.943000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:51.951000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:51.962000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:51.972000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:51.983000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:51.994000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:52.005000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:52.049000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:52.202000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:52.210000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:52.221000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:52.232000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:52.242000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:52.257000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:52.269000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:52.301000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:52.470000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:52.480000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:52.489000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:52.499000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:52.512000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:52.520000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:52.532000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:52.558000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:52.737000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:52.746000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:52.756000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:52.768000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:52.785000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:52.794000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:52.817000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:52.906000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:53.051000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:53.061000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:53.072000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:53.081000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:53.090000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:53.098000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:53.108000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:53.162000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:53.391000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:53.401000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:53.408000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:53.420000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:53.429000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:53.499000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:53.510000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:53.520000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:53.692000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:53.702000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:53.710000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:53.719000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:53.752000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:53.764000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:53.778000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:53.802000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:54.021000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:54.031000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:54.040000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:54.050000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:54.060000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:54.069000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:54.132000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:54.143000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:54.285000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:54.296000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:54.304000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:54.315000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:54.325000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:54.334000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:54.388000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:54.400000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:19:55.093000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:19:55.103000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:19:55.112000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:19:55.120000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:19:55.129000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:19:55.138000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:19:55.147000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:19:55.156000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1484 ms 100.0% 
  triton_mm_235 0.2219 ms 66.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2226 ms 66.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2436 ms 60.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2471 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2504 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_227 0.2509 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2535 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2558 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2667 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7725 seconds and 0.7351 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1483 ms 100.0% 
  triton_mm_235 0.2237 ms 66.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2246 ms 66.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2428 ms 61.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2482 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2501 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2527 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2547 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2557 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2657 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8083 seconds and 0.6125 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1495 ms 100.0% 
  triton_mm_235 0.2257 ms 66.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2260 ms 66.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2421 ms 61.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2483 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2501 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2532 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_227 0.2545 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_254 0.2576 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2663 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8790 seconds and 0.4367 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1480 ms 100.0% 
  triton_mm_235 0.2254 ms 65.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2261 ms 65.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2429 ms 61.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2475 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2503 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2531 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2552 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_227 0.2556 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_252 0.2650 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8599 seconds and 0.7268 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1466 ms 100.0% 
  triton_mm_235 0.2206 ms 66.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2234 ms 65.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2414 ms 60.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2433 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2475 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2493 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2511 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_227 0.2519 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_252 0.2622 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8307 seconds and 0.6280 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1479 ms 100.0% 
  triton_mm_234 0.2172 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_235 0.2184 ms 67.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_233 0.2414 ms 61.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_227 0.2440 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_255 0.2465 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2498 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2521 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2541 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2639 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8512 seconds and 0.8538 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1471 ms 100.0% 
  triton_mm_235 0.2165 ms 67.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_234 0.2168 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_233 0.2395 ms 61.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2438 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_227 0.2460 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_241 0.2483 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2504 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2516 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_252 0.2612 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8537 seconds and 0.6356 seconds precompiling for 39 choices
AUTOTUNE mm(192x7168, 7168x16160)
  mm 0.1491 ms 100.0% 
  triton_mm_234 0.2234 ms 66.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_235 0.2237 ms 66.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_233 0.2437 ms 61.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_255 0.2466 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_241 0.2506 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_240 0.2533 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_254 0.2545 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_227 0.2554 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_252 0.2660 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.8464 seconds and 0.6141 seconds precompiling for 39 choices
Capturing batches (bs=24 avail_mem=41.76 GB):  88%| | 46/52 [03:04<03:59, 39.95s/it]Capturing batches (bs=16 avail_mem=41.15 GB):  88%| | 46/52 [03:04<03:59, 39.95s/it][rank0]:W1028 11:20:11.471000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:11.491000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:11.508000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:11.515000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:11.528000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:11.539000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:11.539000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:11.550000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:11.559000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:11.567000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:11.577000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:11.583000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:11.607000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:11.608000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:11.624000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:11.626000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:11.631000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:11.644000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:11.650000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:11.655000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:11.679000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:11.680000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:11.697000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:11.703000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:12.117000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:12.134000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:12.142000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:12.149000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:12.169000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:12.178000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:12.187000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:12.196000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:12.197000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:12.215000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:12.227000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:12.231000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:12.253000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:12.258000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:12.266000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:12.271000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:12.277000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:12.284000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:12.297000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:12.300000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:12.323000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:12.327000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:12.333000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:12.339000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:12.345000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:12.352000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:12.365000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:12.367000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:12.390000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:12.394000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:12.406000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:12.413000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:12.784000 812 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank2]:W1028 11:20:12.807000 814 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank7]:W1028 11:20:12.821000 819 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank4]:W1028 11:20:12.822000 816 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank1]:W1028 11:20:12.845000 813 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank5]:W1028 11:20:12.846000 817 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank6]:W1028 11:20:12.859000 818 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
[rank3]:W1028 11:20:12.868000 815 torch/_inductor/utils.py:1349] [27/2] Please pip install Composable Kernel package
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_273 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_272 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_279 0.0092 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_277 0.0093 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8451 seconds and 0.2198 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_271 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_270 0.0091 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0092 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_274 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_279 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_275 0.0093 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0094 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8308 seconds and 0.1380 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0090 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_279 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_276 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0092 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0092 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_277 0.0092 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_272 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8530 seconds and 0.2080 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_271 0.0091 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_270 0.0091 ms 99.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_276 0.0093 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_272 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_274 0.0094 ms 96.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_279 0.0094 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7928 seconds and 0.1360 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0090 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0091 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0092 ms 98.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_272 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_273 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_278 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_279 0.0093 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_275 0.0093 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_274 0.0094 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0094 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8293 seconds and 0.2288 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_271 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_270 0.0087 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_276 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_274 0.0090 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_273 0.0090 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_275 0.0090 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_279 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_272 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8706 seconds and 0.2974 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_270 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_271 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_274 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_273 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_279 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_272 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_276 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_275 0.0092 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_278 0.0092 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_277 0.0093 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8926 seconds and 0.2505 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x128, 128x128x512)
  triton_bmm_271 0.0092 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_270 0.0093 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_277 0.0093 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_275 0.0094 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_276 0.0094 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_278 0.0094 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_279 0.0094 ms 97.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_273 0.0095 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_272 0.0095 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_274 0.0095 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8599 seconds and 0.1670 seconds precompiling for 25 choices
[rank5]:W1028 11:20:22.929000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:23.028000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:23.165000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:23.309000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:23.347000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:23.397000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:23.431000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:23.529000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:23.663000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:23.808000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:23.850000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:23.899000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:23.984000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:24.484000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:24.588000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:24.910000 817 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank2]:W1028 11:20:25.091000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:25.116000 812 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank7]:W1028 11:20:25.152000 819 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank3]:W1028 11:20:25.172000 815 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank6]:W1028 11:20:25.216000 818 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank1]:W1028 11:20:25.283000 813 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank4]:W1028 11:20:25.816000 816 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
[rank2]:W1028 11:20:26.601000 814 torch/_inductor/utils.py:1349] [39/2_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0088 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_292 0.0094 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0094 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0096 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0097 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_298 0.0097 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0098 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0099 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0100 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8417 seconds and 0.4019 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_293 0.0092 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0094 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0094 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_299 0.0095 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0096 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0097 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_298 0.0097 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9729 seconds and 0.3999 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0088 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0095 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_298 0.0096 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0097 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0097 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_285 0.0100 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0101 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9082 seconds and 0.4063 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0095 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0096 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0097 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_299 0.0097 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_285 0.0098 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_298 0.0099 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8545 seconds and 0.4020 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0092 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0095 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0095 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_298 0.0097 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0098 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0099 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0100 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0444 seconds and 0.4096 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0095 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0095 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_299 0.0096 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0098 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_298 0.0099 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_285 0.0100 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0939 seconds and 0.4132 seconds precompiling for 25 choices
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_290 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_291 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0093 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0093 ms 92.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0097 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_298 0.0097 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0097 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_303 0.0097 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_302 0.0097 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_285 0.0100 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0537 seconds and 0.4069 seconds precompiling for 25 choices
[rank5]:W1028 11:20:31.576000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:31.654000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:31.755000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:31.781000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:31.807000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:31.857000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:31.883000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:31.926000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:31.940000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:31.985000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:32.007000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:32.017000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:32.052000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:32.111000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:32.117000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:32.128000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:32.142000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x16x512, 128x512x128)
  triton_bmm_291 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_290 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_293 0.0090 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_292 0.0091 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_302 0.0094 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_303 0.0094 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_298 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_299 0.0096 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_285 0.0097 ms 88.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_284 0.0097 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.0986 seconds and 0.4075 seconds precompiling for 25 choices
[rank7]:W1028 11:20:32.227000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:32.515000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:32.594000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:32.695000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:33.634000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:33.710000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:33.810000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:35.950000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:36.010000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:36.027000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:36.086000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:36.128000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:36 DP3 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1028 11:20:36.186000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:36 DP1 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank5]:W1028 11:20:36.348000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:36.426000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:36.528000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:36.568000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:36 DP5 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1028 11:20:36.643000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:36.646000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:36.723000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:36.742000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:36.787000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:36 DP0 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1028 11:20:36.824000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:36.864000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:36 DP6 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank7]:W1028 11:20:36.963000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:37 DP7 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank4]:W1028 11:20:37.246000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:37.321000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:37.420000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:37 DP4 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1028 11:20:38.238000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:38.314000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:38.414000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:38 DP2 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank4]:W1028 11:20:38.992000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:39.007000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:39.016000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:39.026000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:39.030000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:39.046000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:39.053000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:39.069000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:39.083000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:39.092000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:39.102000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:39.105000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:39.123000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:39.128000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:39.169000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:39.183000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:39.192000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:39.203000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:39.205000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:39.223000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:39 DP4 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank0]:W1028 11:20:39.228000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:39 DP1 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:39 DP6 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:39 DP5 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:39 DP7 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:39 DP3 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:39 DP0 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1028 11:20:39.602000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:39.678000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:39.778000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:20:39 DP2 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1028 11:20:40.340000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:40.349000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:40.358000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:40.367000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:40.378000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:40.390000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:40.400000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:40.420000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:40.596000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:40.606000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:40.615000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:40.626000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:40.638000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:40.650000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:40.660000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:40.675000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:40.864000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:40.873000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:40.882000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:40.891000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:40.901000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:40.914000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:40.924000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:40.937000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:20:41 DP3 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:20:41 DP7 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:20:41 DP6 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:20:41 DP4 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:20:41 DP1 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:20:41 DP5 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:20:41 DP0 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:20:41 DP2 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1028 11:20:42.499000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:42.504000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:42.514000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:42.524000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:42.542000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:42.566000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:42.579000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:42.585000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:42.588000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:42.590000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:42.600000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:42.629000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:42.643000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:42.655000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:42.663000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:42.665000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:42.667000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:42.677000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:42.705000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:42.706000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:20:42 DP7 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:20:42 DP3 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank5]:W1028 11:20:42.721000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:20:42 DP6 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:20:42 DP4 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank2]:W1028 11:20:42.742000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:20:42 DP1 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:20:42 DP5 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank0]:W1028 11:20:42.780000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:20:42 DP2 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank0]:W1028 11:20:42.857000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:20:42 DP0 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0125 ms 100.0% 
  triton_mm_307 0.0270 ms 46.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0419 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0494 ms 25.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0531 ms 23.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0582 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0611 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2562 seconds and 0.6534 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0121 ms 100.0% 
  triton_mm_307 0.0270 ms 45.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0417 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0531 ms 22.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0581 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2461 seconds and 0.6514 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0123 ms 100.0% 
  triton_mm_307 0.0271 ms 45.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0453 ms 27.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0496 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0584 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2999 seconds and 0.5091 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0125 ms 100.0% 
  triton_mm_307 0.0269 ms 46.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0418 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0451 ms 27.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0494 ms 25.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0531 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0581 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0599 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0741 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2734 seconds and 0.8283 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0124 ms 100.0% 
  triton_mm_307 0.0271 ms 45.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0455 ms 27.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0495 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0534 ms 23.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0583 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0599 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0611 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2612 seconds and 0.3713 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0123 ms 100.0% 
  triton_mm_307 0.0271 ms 45.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0420 ms 29.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0454 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0497 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0583 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0602 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 16.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.3034 seconds and 0.7925 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0127 ms 100.0% 
  triton_mm_307 0.0271 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0421 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0454 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0498 ms 25.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0584 ms 21.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0601 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0613 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2749 seconds and 0.5237 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x256)
  mm 0.0125 ms 100.0% 
  triton_mm_307 0.0272 ms 45.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_313 0.0419 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_306 0.0456 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_320 0.0496 ms 25.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_312 0.0532 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_321 0.0583 ms 21.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_319 0.0600 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_318 0.0612 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_305 0.0743 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2438 seconds and 0.3737 seconds precompiling for 39 choices
[rank4]:W1028 11:20:53.386000 816 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28daa9f9f0>
[rank5]:W1028 11:20:53.399000 817 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72bd280420>
[rank7]:W1028 11:20:53.406000 819 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8de6f91830>
[rank4]:W1028 11:20:53.409000 816 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28d957f9f0>
[rank5]:W1028 11:20:53.422000 817 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72ba9db9c0>
[rank7]:W1028 11:20:53.429000 819 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fcf975d0ae0>
[rank6]:W1028 11:20:53.448000 818 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5586b70420>
[rank6]:W1028 11:20:53.471000 818 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5585401740>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:20:53 DP4 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:20:53 DP5 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:20:53 DP7 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1028 11:20:53.497000 815 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cff0>
[rank3]:W1028 11:20:53.520000 815 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cae0>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:20:53 DP6 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1028 11:20:53.581000 814 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1d170>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:20:53 DP3 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1028 11:20:53.597000 812 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29c930>
[rank2]:W1028 11:20:53.604000 814 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1c750>
[rank0]:W1028 11:20:53.620000 812 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29cbd0>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:20:53 DP2 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:20:53 DP0 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1028 11:20:53.731000 813 torch/_dynamo/variables/builtin.py:1091] [68/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb2c27d61c0>
[rank1]:W1028 11:20:53.754000 813 torch/_dynamo/variables/builtin.py:1091] [69/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ff46b548ae0>
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:20:53 DP1 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1028 11:20:54.973000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:54.984000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:54.995000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:55.003000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:55.012000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:55.024000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:55.087000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:55.144000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:55.293000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:55.304000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:55.315000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:55.326000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:55.350000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:55.411000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:55.506000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:55.517000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:55.713000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:55.722000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:55.765000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:55.779000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:55.812000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:55.826000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:55.838000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:55.846000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:56.004000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:56.013000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:56.033000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:56.051000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:56.069000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:56.084000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:56.097000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:56.118000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:56.265000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:56.274000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:56.289000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:56.311000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:56.324000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:56.340000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:56.353000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:56.378000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:56.525000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:56.534000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:56.545000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:56.571000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:56.583000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:56.596000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:56.608000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:56.638000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:56.787000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:56.804000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:56.815000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:56.839000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:56.849000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:56.860000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:56.870000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:56.908000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:57.051000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:57.064000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:57.074000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:57.095000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:57.108000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:57.118000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:57.129000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:57.164000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:57.310000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:57.321000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:57.331000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:57.350000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:57.364000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:57.380000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:57.389000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:57.420000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:57.571000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:57.581000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:57.591000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:57.606000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:57.620000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:57.640000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:57.649000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:57.672000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:57.833000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:57.842000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:57.850000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:57.864000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:57.875000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:57.901000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:57.912000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:57.923000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:58.092000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:58.102000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:58.110000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:58.122000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:58.134000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:58.156000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:58.169000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:58.182000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:58.352000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:58.362000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:58.370000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:58.382000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:58.395000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:58.412000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:58.425000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:58.438000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:58.612000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:58.622000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:58.630000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:58.642000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:58.654000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:58.668000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:58.680000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:58.694000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:58.868000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:58.886000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:58.894000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:58.905000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:58.924000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:58.934000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:58.945000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:58.962000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:59.128000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:59.146000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:59.154000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:59.166000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:59.180000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:59.194000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:59.205000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:59.218000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:59.390000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:59.406000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:59.415000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:59.426000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:59.437000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:59.454000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:59.465000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:59.475000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:59.648000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:59.666000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:59.674000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:59.686000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:59.697000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:59.714000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:59.725000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:59.735000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:20:59.907000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:20:59.928000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:20:59.939000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:20:59.949000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:20:59.959000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:20:59.976000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:20:59.986000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:20:59.998000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:00.166000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:00.188000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:00.198000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:00.208000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:00.218000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:00.232000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:00.245000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:00.255000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:00.426000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:00.448000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:00.458000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:00.468000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:00.478000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:00.490000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:00.503000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:00.514000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:00.690000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:00.708000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:00.718000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:00.729000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:00.739000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:00.750000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:00.763000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:00.773000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:00.954000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:00.968000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:00.978000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:00.988000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:00.999000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:01.010000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:01.023000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:01.033000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:01.221000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:01.230000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:01.238000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:01.250000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:01.263000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:01.275000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:01.289000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:01.300000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:01.480000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:01.504000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:01.513000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:01.521000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:01.533000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:01.544000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:01.555000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:01.566000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:01.740000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:01.760000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:01.774000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:01.783000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:01.794000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:01.805000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:01.817000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:01.827000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:02.000000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:02.016000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:02.034000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:02.042000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:02.054000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:02.066000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:02.078000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:02.088000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:02.260000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:02.272000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:02.294000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:02.302000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:02.314000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:02.328000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:02.339000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:02.349000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:02.520000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:02.531000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:02.554000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:02.562000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:02.574000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:02.587000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:02.599000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:02.610000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:02.780000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:02.791000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:02.814000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:02.822000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:02.834000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:02.846000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:02.859000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:02.870000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:03.040000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:03.051000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:03.079000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:03.089000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:03.100000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:03.111000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:03.122000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:03.132000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:03.299000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:03.308000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:03.349000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:03.359000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:03.369000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:03.381000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:03.393000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:03.405000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:03.562000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:03.571000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:03.621000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:03.631000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:03.641000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:03.653000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:03.664000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:03.676000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:03.826000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:03.835000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:03.881000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:03.891000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:03.902000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:03.913000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:03.925000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:03.935000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:04.090000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:04.099000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:04.140000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:04.151000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:04.162000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:04.173000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:04.187000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:04.197000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:04.346000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:04.362000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:04.400000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:04.411000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:04.422000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:04.433000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:04.447000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:04.457000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:04.606000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:04.622000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:04.660000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:04.671000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:04.683000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:04.693000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:04.707000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:04.718000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:05.113000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:05.124000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:05.135000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:05.144000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:05.155000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:05.221000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:05.232000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:05.243000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:05.568000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:05.579000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:05.589000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:05.599000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:05.660000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:05.671000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:05.683000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:05.692000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:05.844000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:05.855000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:05.865000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:05.876000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:05.920000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:05.931000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:05.947000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:05.958000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:06.108000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:06.120000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:06.131000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:06.141000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:06.180000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:06.191000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:06.212000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:06.222000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:06.370000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:06.380000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:06.401000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:06.410000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:06.438000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:06.453000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:06.476000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:06.489000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:06.637000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:06.647000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:06.664000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:06.674000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:06.702000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:06.715000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:06.736000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:06.749000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:06.897000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:06.911000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:06.928000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:06.938000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:06.966000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:06.978000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:06.996000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:07.009000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:07.158000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:07.175000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:07.195000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:07.205000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:07.230000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:07.242000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:07.256000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:07.269000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:07.430000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:07.447000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:07.461000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:07.470000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:07.494000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:07.506000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:07.518000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:07.531000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:07.696000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:07.711000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:07.724000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:07.734000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:07.758000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:07.770000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:07.782000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:07.795000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:07.962000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:07.975000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:07.989000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:07.998000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:08.022000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:08.034000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:08.046000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:08.059000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:08.226000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:08.239000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:08.252000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:08.262000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:08.286000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:08.298000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:08.310000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:08.323000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:08.492000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:08.505000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:08.515000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:08.528000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:08.552000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:08.562000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:08.573000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:08.584000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:08.752000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:08.769000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:08.779000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:08.792000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:08.812000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:08.823000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:08.835000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:08.847000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:09.010000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:09.027000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:09.045000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:09.054000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:09.070000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:09.084000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:09.100000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:09.113000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:09.269000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:09.291000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:09.308000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:09.318000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:09.334000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:09.346000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:09.360000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:09.373000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:09.529000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:09.555000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:09.572000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:09.582000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:09.604000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:09.613000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:09.626000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:09.638000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:09.789000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:09.819000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:09.836000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:09.846000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:09.860000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:09.878000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:09.891000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:09.904000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:10.050000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:10.083000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:10.100000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:10.110000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:10.122000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:10.142000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:10.155000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:10.167000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:10.313000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:10.347000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:10.364000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:10.375000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:10.386000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:10.407000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:10.419000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:10.431000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:11.129000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:11.143000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:11.151000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:11.160000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:11.171000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:11.180000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:11.190000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:11.200000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1035 ms 100.0% 
  triton_mm_359 0.1442 ms 71.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1469 ms 70.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1636 ms 63.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1649 ms 62.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1756 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1762 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1798 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1836 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2028 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6167 seconds and 0.6397 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1057 ms 100.0% 
  triton_mm_359 0.1496 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1549 ms 68.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1679 ms 62.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1706 ms 62.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1785 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1787 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1859 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1859 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2077 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6763 seconds and 0.6111 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1055 ms 100.0% 
  triton_mm_359 0.1467 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1514 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1630 ms 64.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1660 ms 63.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1763 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1766 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1818 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1834 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2064 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6782 seconds and 0.6346 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1038 ms 100.0% 
  triton_mm_359 0.1451 ms 71.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1498 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1633 ms 63.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1669 ms 62.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1776 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1777 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1802 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1852 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2033 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7201 seconds and 0.6912 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1065 ms 100.0% 
  triton_mm_359 0.1481 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1538 ms 69.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1669 ms 63.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1698 ms 62.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1778 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1784 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1835 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1855 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2075 ms 51.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6694 seconds and 0.5375 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1080 ms 100.0% 
  triton_mm_359 0.1490 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1541 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1679 ms 64.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1691 ms 63.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1780 ms 60.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1782 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1838 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1856 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2077 ms 52.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6409 seconds and 0.4142 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1054 ms 100.0% 
  triton_mm_359 0.1480 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1522 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_366 0.1666 ms 63.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1676 ms 62.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_365 0.1782 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.1783 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_351 0.1822 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1863 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2069 ms 51.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6966 seconds and 0.7419 seconds precompiling for 39 choices
AUTOTUNE mm(128x7168, 7168x16160)
  mm 0.1061 ms 100.0% 
  triton_mm_359 0.1490 ms 71.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_358 0.1536 ms 69.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_367 0.1696 ms 62.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_366 0.1696 ms 62.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_364 0.1783 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.1788 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_351 0.1847 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_357 0.1859 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_345 0.2077 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6507 seconds and 0.4476 seconds precompiling for 39 choices
Capturing batches (bs=16 avail_mem=41.15 GB):  90%| | 47/52 [04:20<04:13, 50.75s/it]Capturing batches (bs=12 avail_mem=40.55 GB):  90%| | 47/52 [04:20<04:13, 50.75s/it][rank4]:W1028 11:21:27.446000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:27.457000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:27.463000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:27.484000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:27.495000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:27.506000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:27.515000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:27.518000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:27.527000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:27.528000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:27.532000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:27.554000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:27.564000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:27.575000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:27.587000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:27.589000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:27.597000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:27.601000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:27.605000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:27.628000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:27.638000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:27.648000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:27.660000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:27.670000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:28.086000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:28.098000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:28.107000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:28.126000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:28.134000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:28.149000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:28.158000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:28.171000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:28.174000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:28.180000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:28.191000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:28.209000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:28.218000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:28.237000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:28.241000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:28.245000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:28.249000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:28.260000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:28.262000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:28.280000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:28.289000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:28.308000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:28.311000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:28.315000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:28.317000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:28.330000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:28.331000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:28.349000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:28.358000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:28.378000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:28.384000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:28.399000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:28.929000 813 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank4]:W1028 11:21:28.977000 816 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank7]:W1028 11:21:28.980000 819 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank5]:W1028 11:21:29.002000 817 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank6]:W1028 11:21:29.016000 818 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank0]:W1028 11:21:29.031000 812 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank2]:W1028 11:21:29.052000 814 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
[rank3]:W1028 11:21:29.061000 815 torch/_inductor/utils.py:1349] [27/3] Please pip install Composable Kernel package
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_395 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0090 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0090 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_398 0.0091 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_403 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_401 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8408 seconds and 0.1885 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0089 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_393 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0091 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_392 0.0091 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0092 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_400 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_403 0.0092 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_399 0.0093 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8835 seconds and 0.2207 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_401 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0088 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0088 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_400 0.0088 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_403 0.0088 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_399 0.0089 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0089 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_398 0.0089 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8524 seconds and 0.2023 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0089 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0089 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0089 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_392 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_403 0.0091 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_396 0.0091 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0091 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_393 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8382 seconds and 0.1487 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_400 0.0089 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0089 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_395 0.0089 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_393 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_402 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_392 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_398 0.0091 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8508 seconds and 0.2057 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_395 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_400 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_392 0.0089 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_393 0.0089 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_396 0.0090 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_397 0.0090 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_398 0.0090 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0090 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8821 seconds and 0.1805 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_394 0.0088 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_395 0.0089 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_401 0.0091 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_397 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_403 0.0092 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_398 0.0092 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_399 0.0092 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0092 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8770 seconds and 0.1344 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x128, 128x128x512)
  triton_bmm_395 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_394 0.0087 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_397 0.0088 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_401 0.0088 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0088 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_396 0.0088 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_402 0.0088 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_399 0.0089 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_398 0.0089 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_403 0.0090 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9072 seconds and 0.1531 seconds precompiling for 25 choices
[rank5]:W1028 11:21:39.011000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:39.202000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:39.300000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:39.326000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:39.371000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:39.414000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:39.525000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:39.719000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:39.795000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:39.815000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:39.824000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:39.880000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:39.937000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:40.210000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:40.314000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:40.717000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:41.211000 814 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank1]:W1028 11:21:41.258000 813 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank5]:W1028 11:21:41.297000 817 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank4]:W1028 11:21:41.317000 816 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank7]:W1028 11:21:41.326000 819 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank0]:W1028 11:21:41.350000 812 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank6]:W1028 11:21:42.012000 818 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
[rank3]:W1028 11:21:42.082000 815 torch/_inductor/utils.py:1349] [39/3_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0083 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0088 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0089 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_426 0.0093 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0093 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_408 0.0095 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_422 0.0095 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0096 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_423 0.0096 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8677 seconds and 0.4306 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0088 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0092 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0093 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_427 0.0095 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0097 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_408 0.0098 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0098 ms 86.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0099 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0099 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9018 seconds and 0.4121 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_415 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0092 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0092 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_426 0.0096 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0096 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_408 0.0098 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_422 0.0098 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0098 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_409 0.0099 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8735 seconds and 0.4484 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_415 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_414 0.0086 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0089 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0089 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_426 0.0092 ms 92.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_422 0.0093 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_427 0.0093 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_423 0.0093 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_425 0.0097 ms 87.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_424 0.0098 ms 86.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8991 seconds and 0.4326 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0087 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0091 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_427 0.0093 ms 92.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0095 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_422 0.0097 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0097 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_425 0.0099 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0100 ms 85.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9448 seconds and 0.4601 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0089 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0090 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_427 0.0092 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_426 0.0093 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_408 0.0094 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0094 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0094 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0095 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8853 seconds and 0.4108 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0087 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0092 ms 92.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_426 0.0093 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_427 0.0093 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_422 0.0095 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_423 0.0096 ms 88.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0097 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_409 0.0098 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8714 seconds and 0.4192 seconds precompiling for 25 choices
AUTOTUNE bmm(128x12x512, 128x512x128)
  triton_bmm_414 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_415 0.0086 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0090 ms 93.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0091 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_427 0.0095 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_422 0.0095 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_426 0.0095 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_423 0.0096 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_408 0.0099 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_425 0.0099 ms 85.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8197 seconds and 0.4184 seconds precompiling for 25 choices
[rank1]:W1028 11:21:47.835000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:47.912000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:48.013000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:48.062000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:48.141000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:48.203000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:48.244000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:48.280000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:48.299000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:48.376000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:48.379000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:48.434000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:48.439000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:48.475000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:48.510000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:48.516000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:48.535000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:48.568000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:48.609000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:48.612000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:48.616000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:48.646000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:48.711000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:48.747000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:51.867000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:51.944000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:52.049000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:52.522000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:52.598000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:52.698000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:52.778000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:52.857000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:52.960000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:52.981000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:53.058000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:53.060000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:53.137000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:53.154000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:53.159000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:53.166000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:53.230000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:53.238000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:53.243000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:53.302000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:53.330000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:53.345000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:53.380000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:53.482000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:54.066000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:54.080000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:54.144000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:54.153000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:54.156000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:54.233000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:54.256000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:54.345000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:54.358000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:54.402000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:54.435000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:54.480000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:54.524000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:54.542000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:54.572000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:54.586000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:54.605000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:54.629000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:54.687000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:54.710000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:54.731000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:54.764000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:54.823000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:54.864000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:55.442000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:55.451000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:55.471000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:55.510000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:55.527000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:55.822000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:55.834000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:55.846000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:55.998000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:56.007000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:56.018000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:56.067000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:56.083000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:56.096000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:56.107000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:56.121000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:56.451000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:56.461000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:56.473000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:56.484000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:56.495000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:56.504000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:56.513000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:56.557000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:58.125000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:58.134000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:58.165000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:58.173000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:58.185000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:58.193000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:58.199000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:58.204000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:58.213000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:58.243000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:58.250000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:21:58.254000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:21:58.263000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:58.265000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:58.270000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:58.276000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:21:58.293000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:21:58.299000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:58.303000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:21:58.316000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:21:58.319000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:21:58.326000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:58.380000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:21:58.429000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0111 ms 100.0% 
  triton_mm_431 0.0278 ms 40.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0418 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0451 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0491 ms 22.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0530 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0579 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0609 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0753 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2438 seconds and 0.7248 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0113 ms 100.0% 
  triton_mm_431 0.0277 ms 40.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0420 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 25.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0533 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0597 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0612 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0755 ms 15.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2506 seconds and 0.7190 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0110 ms 100.0% 
  triton_mm_431 0.0278 ms 39.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0420 ms 26.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0492 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0533 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 14.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2656 seconds and 0.6005 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0119 ms 100.0% 
  triton_mm_431 0.0279 ms 42.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0420 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 26.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0494 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0532 ms 22.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0597 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0610 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0758 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1880 seconds and 0.5947 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0112 ms 100.0% 
  triton_mm_431 0.0279 ms 40.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0420 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0455 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0492 ms 22.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0533 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0582 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0598 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0610 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0756 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2200 seconds and 0.4310 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0113 ms 100.0% 
  triton_mm_431 0.0278 ms 40.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0421 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0454 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 22.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0534 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0583 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0598 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0610 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.1755 seconds and 0.4902 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0111 ms 100.0% 
  triton_mm_431 0.0278 ms 40.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0418 ms 26.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0493 ms 22.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0531 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0610 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2671 seconds and 0.6840 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x256)
  mm 0.0112 ms 100.0% 
  triton_mm_431 0.0278 ms 40.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_437 0.0418 ms 26.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_430 0.0453 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_444 0.0492 ms 22.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_436 0.0531 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_445 0.0581 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_443 0.0596 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_442 0.0611 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_429 0.0754 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 8.2691 seconds and 0.4151 seconds precompiling for 39 choices
[rank2]:W1028 11:22:08.977000 814 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1d170>
[rank2]:W1028 11:22:09.000000 814 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1c750>
[rank3]:W1028 11:22:09.044000 815 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cff0>
[rank3]:W1028 11:22:09.067000 815 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cae0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:22:09 DP2 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1028 11:22:09.070000 818 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5586b70420>
[rank6]:W1028 11:22:09.093000 818 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5585401740>
[rank5]:W1028 11:22:09.099000 817 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72bd280420>
[rank5]:W1028 11:22:09.122000 817 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72ba9db9c0>
[rank0]:W1028 11:22:09.131000 812 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29c930>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:22:09 DP3 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1028 11:22:09.154000 812 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29cbd0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:22:09 DP6 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1028 11:22:09.177000 819 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8de6f91830>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:22:09 DP5 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1028 11:22:09.199000 819 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fcf975d0ae0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:22:09 DP0 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1028 11:22:09.261000 816 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28daa9f9f0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:22:09 DP7 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1028 11:22:09.284000 816 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28d957f9f0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:22:09 DP4 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1028 11:22:09.421000 813 torch/_dynamo/variables/builtin.py:1091] [68/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb2c27d61c0>
[rank1]:W1028 11:22:09.444000 813 torch/_dynamo/variables/builtin.py:1091] [69/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ff46b548ae0>
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:22:09 DP1 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1028 11:22:10.679000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:10.688000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:10.697000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:10.709000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:10.720000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:10.731000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:10.742000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:10.792000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:10.949000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:10.960000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:10.971000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:10.982000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:10.993000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:11.001000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:11.015000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:11.051000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:11.217000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:11.227000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:11.239000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:11.251000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:11.261000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:11.270000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:11.283000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:11.314000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:11.479000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:11.491000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:11.500000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:11.521000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:11.534000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:11.545000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:11.557000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:11.580000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:11.747000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:11.759000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:11.768000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:11.784000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:11.798000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:11.808000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:11.820000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:11.840000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:12.017000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:12.029000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:12.040000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:12.050000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:12.060000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:12.070000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:12.085000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:12.098000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:12.285000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:12.295000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:12.307000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:12.323000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:12.335000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:12.343000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:12.357000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:12.367000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:12.551000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:12.560000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:12.570000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:12.597000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:12.610000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:12.620000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:12.633000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:12.645000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:12.818000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:12.827000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:12.837000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:12.861000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:12.877000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:12.887000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:12.901000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:12.912000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:13.087000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:13.098000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:13.108000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:13.125000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:13.141000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:13.152000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:13.166000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:13.177000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:13.355000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:13.367000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:13.376000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:13.389000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:13.405000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:13.416000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:13.430000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:13.440000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:13.623000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:13.634000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:13.644000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:13.656000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:13.669000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:13.680000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:13.694000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:13.704000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:13.891000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:13.903000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:13.912000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:13.924000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:13.936000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:13.948000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:13.960000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:13.972000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:14.159000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:14.170000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:14.180000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:14.192000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:14.205000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:14.216000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:14.228000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:14.240000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:14.427000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:14.438000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:14.448000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:14.460000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:14.474000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:14.484000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:14.498000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:14.509000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:14.697000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:14.708000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:14.720000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:14.730000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:14.742000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:14.750000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:14.765000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:14.774000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:14.963000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:14.975000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:14.984000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:15.001000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:15.014000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:15.025000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:15.038000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:15.050000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:15.231000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:15.242000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:15.252000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:15.265000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:15.281000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:15.293000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:15.306000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:15.317000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:15.499000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:15.511000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:15.520000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:15.532000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:15.544000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:15.556000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:15.573000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:15.583000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:15.767000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:15.779000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:15.791000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:15.802000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:15.815000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:15.826000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:15.839000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:15.851000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:16.035000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:16.047000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:16.059000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:16.070000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:16.083000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:16.095000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:16.107000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:16.119000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:16.303000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:16.319000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:16.328000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:16.340000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:16.353000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:16.365000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:16.377000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:16.390000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:16.571000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:16.587000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:16.597000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:16.609000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:16.621000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:16.633000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:16.647000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:16.658000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:16.839000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:16.859000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:16.869000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:16.880000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:16.893000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:16.905000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:16.917000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:16.930000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:17.378000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:17.386000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:17.395000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:17.405000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:17.415000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:17.425000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:17.436000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:17.489000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:17.875000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:17.885000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:17.896000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:17.907000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:17.917000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:17.928000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:17.940000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:17.984000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:18.151000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:18.160000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:18.172000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:18.183000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:18.195000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:18.206000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:18.219000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:18.248000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:18.423000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:18.432000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:18.444000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:18.455000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:18.466000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:18.478000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:18.491000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:18.512000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:18.990000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:18.999000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:19.011000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:19.022000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:19.033000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:19.092000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:19.102000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:19.112000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:19.519000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:19.528000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:19.537000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:19.549000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:19.559000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:19.570000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:19.633000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:19.645000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:20.045000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:20.054000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:20.063000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:20.072000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:20.084000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:20.094000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:20.105000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:20.156000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:20.321000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:20.331000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:20.340000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:20.349000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:20.362000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:20.373000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:20.385000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:20.420000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:20.593000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:20.607000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:20.616000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:20.626000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:20.638000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:20.650000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:20.662000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:20.683000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:20.869000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:20.883000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:20.892000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:20.901000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:20.914000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:20.926000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:20.938000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:20.950000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:21.139000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:21.161000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:21.173000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:21.184000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:21.194000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:21.207000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:21.218000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:21.228000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:21.415000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:21.433000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:21.444000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:21.456000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:21.467000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:21.479000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:21.491000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:21.500000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:21.691000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:21.705000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:21.716000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:21.731000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:21.745000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:21.757000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:21.769000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:21.778000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:21.977000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:21.986000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:21.997000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:22.009000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:22.020000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:22.032000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:22.044000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:22.053000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:22.249000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:22.263000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:22.274000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:22.286000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:22.297000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:22.309000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:22.321000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:22.330000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:22.521000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:22.535000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:22.546000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:22.558000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:22.568000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:22.581000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:22.593000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:22.601000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:22.793000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:22.813000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:22.822000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:22.834000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:22.844000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:22.857000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:22.869000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:22.877000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:23.065000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:23.081000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:23.095000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:23.106000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:23.116000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:23.129000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:23.141000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:23.150000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:23.337000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:23.349000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:23.372000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:23.382000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:23.397000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:23.406000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:23.423000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:23.432000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:23.609000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:23.620000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:23.643000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:23.656000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:23.667000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:23.677000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:23.692000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:23.700000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:23.879000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:23.889000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:23.911000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:23.929000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:23.939000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:23.951000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:23.966000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:23.976000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:24.155000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:24.164000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:24.182000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:24.201000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:24.212000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:24.224000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:24.237000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:24.249000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:24.431000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:24.440000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:24.455000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:24.473000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:24.484000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:24.496000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:24.511000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:24.521000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:24.707000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:24.716000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:24.727000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:24.741000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:24.752000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:24.765000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:24.780000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:24.790000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:24.983000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:24.992000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:25.001000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:25.013000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:25.025000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:25.037000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:25.051000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:25.063000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:25.259000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:25.268000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:25.277000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:25.290000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:25.301000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:25.313000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:25.329000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:25.340000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:25.535000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:25.544000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:25.553000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:25.569000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:25.579000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:25.594000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:25.606000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:25.618000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:25.813000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:25.824000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:25.835000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:25.846000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:25.858000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:25.869000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:25.880000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:25.890000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:26.089000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:26.100000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:26.111000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:26.122000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:26.134000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:26.144000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:26.155000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:26.165000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:26.359000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:26.368000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:26.378000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:26.397000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:26.407000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:26.419000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:26.433000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:26.444000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:26.634000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:26.644000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:26.653000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:26.669000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:26.679000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:26.691000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:26.705000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:26.717000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:26.913000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:26.924000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:26.935000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:26.946000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:26.959000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:26.969000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:26.980000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:26.990000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:27.189000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:27.200000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:27.211000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:27.222000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:27.234000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:27.245000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:27.256000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:27.265000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:27.972000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:27.987000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:27.996000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:28.006000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:28.015000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:28.032000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:28.041000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:28.052000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0916 ms 100.0% 
  triton_mm_483 0.1463 ms 62.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1522 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1634 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1656 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1715 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1753 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1760 ms 52.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1809 ms 50.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1830 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.6469 seconds and 0.5483 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0927 ms 100.0% 
  triton_mm_483 0.1493 ms 62.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1545 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1654 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1681 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1740 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1772 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1774 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1842 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_475 0.1851 ms 50.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.6796 seconds and 0.5813 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0892 ms 100.0% 
  triton_mm_483 0.1441 ms 61.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1482 ms 60.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1624 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1640 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1692 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1751 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1754 ms 50.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1791 ms 49.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1826 ms 48.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7407 seconds and 0.5375 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0919 ms 100.0% 
  triton_mm_483 0.1480 ms 62.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1534 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1660 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1683 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1731 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1772 ms 51.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1781 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1828 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1849 ms 49.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7171 seconds and 0.7262 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0910 ms 100.0% 
  triton_mm_483 0.1470 ms 61.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1519 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1653 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1678 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1708 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1773 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1779 ms 51.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1831 ms 49.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1854 ms 49.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7133 seconds and 0.4567 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0923 ms 100.0% 
  triton_mm_483 0.1508 ms 61.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1528 ms 60.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1668 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1698 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1726 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1777 ms 51.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1779 ms 51.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_475 0.1840 ms 50.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1849 ms 49.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7283 seconds and 0.3539 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0901 ms 100.0% 
  triton_mm_483 0.1462 ms 61.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1482 ms 60.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1631 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1661 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1691 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_489 0.1769 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1770 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_475 0.1789 ms 50.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1841 ms 48.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 8.7659 seconds and 0.3990 seconds precompiling for 39 choices
AUTOTUNE mm(96x7168, 7168x16160)
  mm 0.0923 ms 100.0% 
  triton_mm_483 0.1467 ms 62.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1549 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_490 0.1668 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_491 0.1687 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.1742 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_488 0.1773 ms 52.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_489 0.1773 ms 52.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_481 0.1845 ms 50.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_475 0.1846 ms 50.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.7408 seconds and 0.5169 seconds precompiling for 39 choices
Capturing batches (bs=12 avail_mem=40.55 GB):  92%|| 48/52 [05:38<03:55, 58.75s/it]Capturing batches (bs=8 avail_mem=39.97 GB):  92%|| 48/52 [05:38<03:55, 58.75s/it] [rank7]:W1028 11:22:45.009000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:45.031000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:45.042000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:45.055000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:45.064000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:45.076000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:45.079000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:45.089000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:45.098000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:45.102000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:45.112000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:45.125000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:45.134000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:45.146000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:45.153000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:45.160000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:45.167000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:45.177000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:45.186000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:45.200000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:45.209000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:45.220000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:45.236000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:45.242000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:45.653000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:45.688000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:45.699000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:45.710000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:45.718000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:45.728000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:45.738000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:45.738000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:45.748000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:45.775000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:45.787000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:45.797000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:45.804000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:45.810000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:45.816000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:45.827000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:45.835000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:45.847000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:45.859000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:45.868000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:45.876000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:45.880000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:45.887000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:45.899000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:45.907000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:45.917000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:45.929000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:45.939000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:45.946000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:45.957000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:45.970000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:45.976000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:46.344000 819 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank6]:W1028 11:22:46.382000 818 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank4]:W1028 11:22:46.396000 816 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank5]:W1028 11:22:46.403000 817 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank0]:W1028 11:22:46.409000 812 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank3]:W1028 11:22:46.420000 815 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank2]:W1028 11:22:46.439000 814 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
[rank1]:W1028 11:22:46.440000 813 torch/_inductor/utils.py:1349] [27/4] Please pip install Composable Kernel package
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_516 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_517 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0086 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0087 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_524 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_527 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_525 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_523 0.0087 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0088 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8604 seconds and 0.2170 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_519 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_527 0.0087 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_526 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_516 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_525 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_520 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_523 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8751 seconds and 0.2406 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0088 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_519 0.0088 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_520 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_524 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_523 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_522 0.0091 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_525 0.0091 ms 96.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8918 seconds and 0.2272 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_519 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0087 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0087 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0088 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_520 0.0088 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0089 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_525 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0089 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_512 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8934 seconds and 0.1825 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_519 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_521 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_526 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_527 0.0088 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_512 0.0088 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_522 0.0088 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9022 seconds and 0.1521 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0088 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_521 0.0089 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_523 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_522 0.0090 ms 97.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_527 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8722 seconds and 0.1960 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_518 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_519 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_517 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_520 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_521 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_523 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_524 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_522 0.0088 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8860 seconds and 0.1151 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x128, 128x128x512)
  triton_bmm_519 0.0087 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_518 0.0088 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0088 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_516 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_521 0.0089 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_520 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_522 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_523 0.0089 ms 97.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_524 0.0090 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8452 seconds and 0.1415 seconds precompiling for 25 choices
[rank3]:W1028 11:22:56.658000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:56.702000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:57.103000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:57.158000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:57.172000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:22:57.216000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:57.265000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:57.363000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:57.601000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:57.619000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:22:57.672000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:22:57.771000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:22:57.862000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:22:58.100000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:22:58.122000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:22:58.410000 815 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank2]:W1028 11:22:58.452000 814 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank1]:W1028 11:22:58.634000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:22:59.051000 816 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank5]:W1028 11:22:59.084000 817 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank7]:W1028 11:22:59.092000 819 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank0]:W1028 11:22:59.155000 812 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank6]:W1028 11:22:59.377000 818 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
[rank1]:W1028 11:23:00.070000 813 torch/_inductor/utils.py:1349] [39/4_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_541 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0088 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_550 0.0093 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0093 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_547 0.0095 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0095 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0097 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0098 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8887 seconds and 0.4003 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0085 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0085 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0089 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0090 ms 90.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_532 0.0091 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_546 0.0092 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0092 ms 88.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_533 0.0095 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8484 seconds and 0.4111 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0088 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0088 ms 93.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_551 0.0094 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_546 0.0095 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_550 0.0095 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_547 0.0095 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0095 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0095 ms 86.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9488 seconds and 0.4628 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0084 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_541 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0087 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_550 0.0091 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0091 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_547 0.0095 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0095 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0098 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_549 0.0099 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9687 seconds and 0.4250 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0091 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0092 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_547 0.0093 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_533 0.0094 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0095 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_532 0.0096 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9668 seconds and 0.4367 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0091 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0091 ms 91.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_547 0.0093 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_546 0.0093 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0095 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0096 ms 86.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9062 seconds and 0.4062 seconds precompiling for 25 choices
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0087 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_541 0.0089 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0093 ms 91.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0094 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_546 0.0096 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0096 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_533 0.0099 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0099 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9556 seconds and 0.4188 seconds precompiling for 25 choices
[rank3]:W1028 11:23:05.238000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:05.268000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:05.317000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:05.347000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:05.420000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:05.450000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x8x512, 128x512x128)
  triton_bmm_538 0.0086 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_539 0.0087 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_541 0.0089 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_540 0.0090 ms 95.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_546 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_547 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_550 0.0095 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_551 0.0096 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_532 0.0099 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0099 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8995 seconds and 0.4133 seconds precompiling for 25 choices
[rank4]:W1028 11:23:05.713000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:05.724000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:05.740000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:05.748000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:05.790000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:05.802000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:05.817000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:05.825000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:05.891000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:05.902000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:05.916000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:05.927000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:05.990000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:06.068000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:06.168000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:07.065000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:07.143000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:07.245000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:10.017000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:10.097000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:10.154000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:10.203000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:10.234000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:10 DP3 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1028 11:23:10.341000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:10.368000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:10.379000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:10.390000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:10 DP2 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1028 11:23:10.446000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:10.457000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:10.466000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:10.488000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:10.548000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:10.559000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:10.566000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:10.567000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:10 DP4 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:10 DP5 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:10 DP7 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1028 11:23:10.678000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:10 DP0 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1028 11:23:10.735000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:10.812000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:10.917000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:10 DP6 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1028 11:23:11.248000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:11.328000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:11.432000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:11 DP1 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1028 11:23:12.006000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:12.017000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:12.026000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:12.043000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:12.052000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:12.063000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:12.084000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:12.096000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:12.102000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:12.114000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:12.121000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:12.130000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:12.142000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:12.187000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:12.191000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:12.199000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:12.204000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:12.223000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:12.233000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:23:12 DP3 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank2]:W1028 11:23:12.247000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:23:12 DP7 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:23:12 DP0 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:23:12 DP4 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:23:12 DP5 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank6]:W1028 11:23:12.293000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:23:12 DP2 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:23:12 DP6 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank1]:W1028 11:23:12.638000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:12.714000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:12.826000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-28 11:23:12 DP1 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[rank3]:W1028 11:23:13.395000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:13.409000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:13.421000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:13.433000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:13.442000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:13.455000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:13.466000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:13.478000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:13.671000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:13.683000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:13.694000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:13.706000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:13.716000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:13.729000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:13.740000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:13.752000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:13.950000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:13.960000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:13.969000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:13.979000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:13.991000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:14.003000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:14.016000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:14.030000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:14 DP3 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:14 DP7 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:14 DP5 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:14 DP6 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:14 DP0 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:14 DP4 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:14 DP2 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:23:14 DP1 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1028 11:23:15.619000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:15.631000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:15.645000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:15.651000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:15.678000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:15.689000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:15.697000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:15.707000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:15.710000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:15.724000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:15.729000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:15.758000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:15.776000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:15.785000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:15.790000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:15.809000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:23:15 DP3 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank2]:W1028 11:23:15.857000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:15.865000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:23:15 DP5 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:23:15 DP2 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:23:15 DP1 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank7]:W1028 11:23:16.192000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:16.257000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:16.274000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:16.276000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:16.325000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:23:16 DP7 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[rank4]:W1028 11:23:16.343000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:23:16 DP0 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:23:16 DP6 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-28 11:23:16 DP4 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0106 ms 100.0% 
  triton_mm_555 0.0275 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0430 ms 24.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0451 ms 23.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0495 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0533 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0580 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0605 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0638 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0750 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.5694 seconds and 0.5740 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0107 ms 100.0% 
  triton_mm_555 0.0273 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0431 ms 24.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0451 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0495 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0535 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0580 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0597 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0621 ms 17.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0743 ms 14.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.5836 seconds and 0.6065 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0112 ms 100.0% 
  triton_mm_555 0.0271 ms 41.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0402 ms 27.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0452 ms 24.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0493 ms 22.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0533 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0582 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0595 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0611 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0740 ms 15.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 7.0254 seconds and 0.5687 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0109 ms 100.0% 
  triton_mm_555 0.0272 ms 39.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0404 ms 26.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0452 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0497 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0533 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0583 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0599 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0616 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0741 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.5228 seconds and 0.4600 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0107 ms 100.0% 
  triton_mm_555 0.0276 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0406 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0452 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0494 ms 21.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0532 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0583 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0600 ms 17.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0626 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0750 ms 14.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.5449 seconds and 0.6611 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0109 ms 100.0% 
  triton_mm_555 0.0277 ms 39.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0424 ms 25.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0451 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0494 ms 22.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0534 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0581 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0596 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0612 ms 17.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0745 ms 14.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.5807 seconds and 0.4216 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0110 ms 100.0% 
  triton_mm_555 0.0271 ms 40.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0406 ms 27.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0453 ms 24.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0497 ms 22.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0533 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0583 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0596 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0613 ms 17.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0740 ms 14.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.5483 seconds and 0.6485 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x256)
  mm 0.0106 ms 100.0% 
  triton_mm_555 0.0277 ms 38.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_561 0.0415 ms 25.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0453 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_568 0.0503 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_560 0.0533 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_569 0.0581 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_567 0.0622 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_566 0.0652 ms 16.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_553 0.0749 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 6.5503 seconds and 0.5357 seconds precompiling for 31 choices
[rank2]:W1028 11:23:25.070000 814 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1d170>
[rank1]:W1028 11:23:25.073000 813 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb2c27d61c0>
[rank2]:W1028 11:23:25.093000 814 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1c750>
[rank1]:W1028 11:23:25.097000 813 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ff46b548ae0>
[rank4]:W1028 11:23:25.112000 816 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28daa9f9f0>
[rank0]:W1028 11:23:25.118000 812 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29c930>
[rank4]:W1028 11:23:25.136000 816 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28d957f9f0>
[rank0]:W1028 11:23:25.142000 812 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29cbd0>
[rank6]:W1028 11:23:25.145000 818 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5586b70420>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:23:25 DP2 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:23:25 DP1 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1028 11:23:25.168000 818 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5585401740>
[rank7]:W1028 11:23:25.199000 819 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8de6f91830>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:23:25 DP4 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:23:25 DP0 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1028 11:23:25.213000 815 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cff0>
[rank7]:W1028 11:23:25.222000 819 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fcf975d0ae0>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:23:25 DP6 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1028 11:23:25.236000 815 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cae0>
[rank5]:W1028 11:23:25.257000 817 torch/_dynamo/variables/builtin.py:1091] [68/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72bd280420>
[rank5]:W1028 11:23:25.280000 817 torch/_dynamo/variables/builtin.py:1091] [69/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72ba9db9c0>
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:23:25 DP7 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:23:25 DP3 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:23:25 DP5 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1028 11:23:26.963000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:26.973000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:26.985000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:26.997000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:27.007000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:27.016000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:27.028000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:27.079000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:27.239000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:27.250000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:27.265000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:27.278000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:27.290000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:27.299000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:27.312000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:27.351000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:27.515000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:27.526000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:27.538000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:27.550000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:27.563000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:27.573000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:27.585000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:27.624000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:27.793000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:27.802000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:27.813000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:27.823000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:27.837000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:27.850000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:27.862000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:27.897000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:28.069000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:28.078000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:28.092000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:28.102000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:28.114000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:28.127000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:28.138000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:28.169000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:28.340000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:28.353000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:28.369000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:28.383000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:28.393000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:28.405000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:28.417000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:28.440000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:28.615000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:28.626000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:28.641000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:28.657000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:28.669000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:28.680000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:28.693000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:28.711000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:28.891000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:28.903000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:28.915000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:28.929000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:28.943000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:28.953000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:28.966000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:28.984000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:29.167000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:29.178000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:29.191000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:29.204000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:29.219000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:29.229000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:29.242000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:29.259000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:29.449000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:29.459000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:29.469000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:29.479000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:29.497000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:29.509000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:29.521000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:29.534000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:29.719000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:29.733000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:29.745000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:29.758000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:29.770000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:29.781000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:29.797000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:29.810000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:29.995000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:30.006000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:30.022000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:30.034000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:30.047000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:30.057000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:30.070000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:30.083000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:30.271000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:30.282000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:30.295000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:30.308000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:30.320000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:30.330000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:30.343000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:30.359000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:30.547000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:30.558000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:30.571000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:30.584000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:30.596000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:30.606000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:30.619000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:30.635000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:30.825000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:30.834000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:30.844000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:30.855000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:30.873000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:30.885000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:30.896000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:30.913000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:31.101000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:31.118000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:31.131000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:31.143000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:31.156000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:31.168000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:31.185000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:31.195000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:31.371000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:31.405000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:31.422000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:31.434000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:31.446000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:31.456000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:31.467000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:31.481000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:31.647000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:31.676000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:31.697000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:31.711000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:31.725000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:31.735000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:31.748000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:31.759000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:31.923000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:31.948000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:31.973000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:31.988000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:32.000000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:32.012000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:32.024000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:32.036000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:32.199000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:32.220000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:32.249000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:32.264000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:32.276000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:32.287000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:32.301000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:32.312000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:32.477000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:32.490000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:32.523000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:32.541000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:32.552000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:32.565000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:32.579000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:32.590000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:32.753000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:32.762000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:32.799000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:32.817000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:32.829000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:32.843000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:32.858000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:32.870000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:33.033000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:33.046000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:33.087000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:33.099000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:33.113000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:33.123000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:33.136000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:33.147000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:33.310000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:33.318000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:33.364000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:33.376000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:33.388000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:33.400000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:33.412000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:33.423000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:33.589000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:33.598000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:33.640000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:33.651000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:33.664000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:33.675000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:33.688000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:33.700000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:33.867000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:33.878000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:33.918000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:33.928000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:33.939000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:33.953000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:33.965000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:33.978000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:34.147000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:34.158000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:34.193000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:34.204000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:34.215000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:34.229000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:34.245000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:34.256000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:34.429000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:34.438000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:34.467000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:34.481000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:34.494000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:34.505000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:34.522000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:34.532000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:34.709000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:34.718000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:34.743000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:34.758000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:34.770000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:34.783000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:34.797000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:34.809000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:34.985000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:34.994000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:35.019000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:35.034000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:35.045000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:35.059000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:35.073000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:35.085000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:35.262000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:35.278000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:35.296000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:35.309000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:35.322000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:35.335000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:35.349000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:35.367000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:35.541000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:35.554000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:35.575000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:35.588000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:35.600000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:35.611000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:35.625000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:35.643000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:35.817000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:35.830000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:35.855000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:35.867000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:35.879000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:35.890000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:35.903000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:35.919000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:36.093000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:36.106000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:36.135000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:36.147000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:36.159000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:36.171000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:36.183000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:36.195000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:36.369000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:36.382000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:36.415000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:36.428000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:36.440000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:36.452000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:36.463000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:36.475000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:36.649000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:36.658000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:36.695000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:36.707000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:36.719000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:36.730000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:36.743000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:36.755000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:36.929000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:36.942000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:36.975000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:36.988000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:36.999000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:37.011000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:37.023000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:37.034000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:37.209000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:37.222000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:37.255000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:37.268000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:37.280000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:37.292000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:37.303000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:37.315000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:37.489000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:37.498000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:37.535000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:37.547000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:37.560000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:37.570000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:37.584000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:37.595000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:37.770000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:37.786000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:37.825000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:37.835000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:37.847000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:37.860000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:37.870000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:37.883000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:38.049000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:38.062000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:38.101000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:38.117000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:38.126000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:38.139000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:38.150000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:38.162000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:38.329000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:38.338000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:38.377000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:38.389000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:38.407000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:38.419000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:38.430000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:38.442000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:38.608000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:38.619000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:38.652000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:38.662000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:38.689000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:38.700000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:38.713000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:38.727000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:38.893000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:38.902000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:38.931000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:38.941000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:38.965000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:38.980000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:38.992000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:39.006000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:39.169000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:39.183000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:39.211000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:39.221000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:39.241000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:39.260000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:39.272000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:39.286000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:39.448000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:39.463000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:39.491000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:39.501000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:39.517000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:39.539000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:39.552000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:39.566000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:39.729000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:39.743000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:39.771000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:39.781000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:39.805000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:39.819000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:39.837000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:39.854000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:40.415000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:40.424000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:40.438000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:40.450000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:40.463000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:40.475000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:40.527000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:40.539000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:41.068000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:41.077000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:41.090000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:41.101000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:41.111000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:41.124000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:41.133000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:41.182000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:41.659000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:41.671000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:41.681000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:41.691000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:41.703000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:41.715000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:41.775000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:41.789000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:41.962000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:41.970000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:41.980000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:41.990000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:42.004000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:42.016000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:42.056000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:42.070000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:42.581000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:42.593000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:42.605000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:42.615000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:42.624000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:42.634000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:42.647000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:42.695000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:43.221000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:43.231000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:43.243000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:43.254000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:43.266000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:43.274000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:43.285000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:43.331000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:43.502000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:43.515000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:43.526000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:43.539000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:43.552000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:43.561000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:43.573000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:43.611000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:43.782000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:43.799000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:43.810000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:43.822000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:43.836000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:43.845000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:43.856000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:43.891000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:44.064000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:44.086000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:44.096000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:44.106000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:44.118000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:44.129000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:44.143000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:44.173000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:44.347000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:44.370000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:44.380000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:44.391000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:44.403000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:44.414000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:44.428000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:44.449000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:45.183000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:45.196000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:45.206000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:45.218000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:45.228000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:45.241000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:45.250000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:45.260000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0771 ms 100.0% 
  triton_mm_598 0.1105 ms 69.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1108 ms 69.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1129 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1264 ms 61.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1316 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1332 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1401 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_596 0.1410 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_607 0.1427 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7342 seconds and 0.5827 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0760 ms 100.0% 
  triton_mm_598 0.1102 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1106 ms 68.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1126 ms 67.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1267 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_597 0.1297 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_585 0.1316 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_596 0.1379 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_606 0.1411 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_607 0.1432 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7917 seconds and 0.5630 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0745 ms 100.0% 
  triton_mm_598 0.1100 ms 67.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1105 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1126 ms 66.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1226 ms 60.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1301 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1309 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1373 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_596 0.1391 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_607 0.1412 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7616 seconds and 0.6290 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0754 ms 100.0% 
  triton_mm_598 0.1101 ms 68.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1105 ms 68.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1123 ms 67.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1249 ms 60.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1302 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1320 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1352 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_607 0.1386 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_596 0.1404 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 6.6971 seconds and 0.5250 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0752 ms 100.0% 
  triton_mm_598 0.1104 ms 68.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1107 ms 68.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1126 ms 66.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1254 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1308 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1311 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_596 0.1387 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_606 0.1393 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_607 0.1422 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7464 seconds and 0.4255 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0736 ms 100.0% 
  triton_mm_598 0.1098 ms 67.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1100 ms 66.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1122 ms 65.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1216 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_597 0.1288 ms 57.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_585 0.1296 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_596 0.1365 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_606 0.1365 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_607 0.1401 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.6444 seconds and 0.4328 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0760 ms 100.0% 
  triton_mm_598 0.1104 ms 68.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1108 ms 68.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1128 ms 67.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1255 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_597 0.1303 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_585 0.1317 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_596 0.1379 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_606 0.1404 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_607 0.1437 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7175 seconds and 0.6742 seconds precompiling for 31 choices
AUTOTUNE mm(64x7168, 7168x16160)
  mm 0.0762 ms 100.0% 
  triton_mm_598 0.1102 ms 69.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_599 0.1105 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_591 0.1129 ms 67.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_590 0.1273 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_585 0.1312 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_597 0.1329 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_606 0.1399 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_596 0.1414 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_607 0.1435 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.7244 seconds and 0.5763 seconds precompiling for 31 choices
Capturing batches (bs=8 avail_mem=39.97 GB):  94%|| 49/52 [06:52<03:10, 63.42s/it]Capturing batches (bs=4 avail_mem=39.38 GB):  94%|| 49/52 [06:52<03:10, 63.42s/it][rank6]:W1028 11:23:59.342000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:59.359000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:59.369000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:59.378000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:59.389000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:59.404000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:59.414000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:59.415000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:59.426000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:59.431000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:59.440000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:59.448000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:59.460000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:59.475000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:59.487000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:59.490000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:59.498000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:23:59.507000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:23:59.515000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:23:59.523000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:23:59.535000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:23:59.551000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:23:59.561000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:23:59.573000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:23:59.991000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:00.007000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:00.019000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:00.031000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:00.039000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:00.048000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:00.062000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:00.072000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:00.077000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:00.092000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:00.103000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:00.114000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:00.125000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:00.137000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:00.149000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:00.151000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:00.160000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:00.164000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:00.175000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:00.184000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:00.196000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:00.209000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:00.220000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:00.222000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:00.232000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:00.235000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:00.245000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:00.254000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:00.276000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:00.290000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:00.303000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:00.313000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:00.895000 818 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank3]:W1028 11:24:00.901000 815 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank5]:W1028 11:24:00.913000 817 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank4]:W1028 11:24:00.918000 816 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank7]:W1028 11:24:00.926000 819 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank0]:W1028 11:24:00.947000 812 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank2]:W1028 11:24:00.977000 814 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
[rank1]:W1028 11:24:00.991000 813 torch/_inductor/utils.py:1349] [27/5] Please pip install Composable Kernel package
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_625 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0085 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_634 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_635 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_628 0.0086 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_629 0.0087 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_632 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_630 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8574 seconds and 0.1657 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_626 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_625 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0086 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_628 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_635 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_620 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_630 0.0088 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_621 0.0088 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9015 seconds and 0.2026 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_626 0.0085 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0085 ms 99.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0086 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0086 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_629 0.0087 ms 97.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_631 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_635 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_634 0.0088 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_630 0.0088 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8728 seconds and 0.2182 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_624 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0084 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0084 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_629 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_628 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_634 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_635 0.0085 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_631 0.0085 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_630 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8733 seconds and 0.1607 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_624 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0084 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_625 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_634 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_635 0.0086 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_633 0.0086 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_630 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_632 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0087 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9081 seconds and 0.2329 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_625 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_634 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_621 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_630 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_635 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8465 seconds and 0.1423 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_626 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_624 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0085 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_629 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_630 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_621 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_631 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_620 0.0087 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8290 seconds and 0.1466 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x128, 128x128x512)
  triton_bmm_624 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_625 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0086 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_635 0.0086 ms 96.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_628 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_633 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_630 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_634 0.0087 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9125 seconds and 0.1442 seconds precompiling for 25 choices
[rank2]:W1028 11:24:11.067000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:11.279000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:11.301000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:11.328000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:11.576000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:11.778000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:11.808000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:11.814000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:11.831000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:11.877000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:12.199000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:12.321000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:12.379000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:12.410000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:12.720000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:12.905000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:13.049000 814 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank4]:W1028 11:24:13.191000 816 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank6]:W1028 11:24:13.248000 818 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank0]:W1028 11:24:13.794000 812 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank3]:W1028 11:24:13.812000 815 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank5]:W1028 11:24:13.825000 817 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank1]:W1028 11:24:14.025000 813 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
[rank7]:W1028 11:24:14.348000 819 torch/_inductor/utils.py:1349] [39/5_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0085 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0085 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_658 0.0092 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_659 0.0092 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_655 0.0094 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_654 0.0095 ms 87.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_657 0.0098 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0099 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9027 seconds and 0.4126 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_649 0.0085 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0087 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_658 0.0094 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_659 0.0095 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_654 0.0095 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0095 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_641 0.0097 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0098 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8066 seconds and 0.4056 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0086 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0087 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_659 0.0091 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_658 0.0092 ms 89.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_654 0.0093 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0093 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0097 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_641 0.0097 ms 85.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9418 seconds and 0.3978 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0083 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_649 0.0084 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0084 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0092 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_658 0.0093 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_654 0.0093 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_659 0.0094 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_641 0.0095 ms 85.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0096 ms 84.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8901 seconds and 0.4465 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_647 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_646 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_648 0.0086 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0086 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_659 0.0091 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_658 0.0092 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_654 0.0093 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0095 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_640 0.0096 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_641 0.0096 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8775 seconds and 0.4662 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_647 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_646 0.0085 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0086 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0086 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_658 0.0093 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_659 0.0093 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_655 0.0094 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_654 0.0094 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_656 0.0099 ms 83.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_657 0.0100 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9574 seconds and 0.4600 seconds precompiling for 25 choices
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0087 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_649 0.0087 ms 95.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_659 0.0092 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_658 0.0093 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_654 0.0093 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_640 0.0093 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0094 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_641 0.0097 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8726 seconds and 0.4152 seconds precompiling for 25 choices
[rank6]:W1028 11:24:19.686000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:19.765000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(128x4x512, 128x512x128)
  triton_bmm_646 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_647 0.0083 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_649 0.0085 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_648 0.0086 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_658 0.0092 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_659 0.0092 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_640 0.0093 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_655 0.0095 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_641 0.0095 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_654 0.0095 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9431 seconds and 0.4053 seconds precompiling for 25 choices
[rank4]:W1028 11:24:19.777000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:19.815000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:19.857000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:19.907000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:19.923000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:20.001000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:20.052000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:20.341000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:20.419000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:20.469000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:20.470000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:20.539000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:20.551000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:20.602000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:20.618000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:20.668000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:20.709000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:20.787000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:20.838000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:21.243000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:21.320000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:21.371000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:24 DP6 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:24 DP4 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:24 DP2 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:25 DP0 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:25 DP5 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:25 DP1 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:25 DP3 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:25 DP7 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1028 11:24:25.988000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:26.003000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:26.010000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:26.020000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:26.038000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:26.047000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:26.059000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:26.060000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:26.074000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:26.080000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:26.090000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:26.110000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:26.118000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:26.130000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:26.132000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:26.146000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:26.151000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:26.160000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:26.181000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:26 DP0 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1028 11:24:26.189000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:26.203000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:26 DP4 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:26 DP1 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1028 11:24:26.209000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:26 DP3 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:26 DP5 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:26 DP6 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:26 DP2 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1028 11:24:26.280000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:26.351000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:26 DP7 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1028 11:24:26.911000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:26.920000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:26.932000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:26.945000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:26.959000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:26.970000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:26.983000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:26.996000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:27.195000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:27.205000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:27.225000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:27.238000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:27.251000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:27.263000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:27.276000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:27.289000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:27.479000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:27.489000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:27.505000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:27.519000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:27.530000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:27.543000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:27.555000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:27.568000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:27 DP5 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:27 DP6 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:27 DP3 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:27 DP2 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:27 DP0 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:27 DP4 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:27 DP1 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:27 DP7 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1028 11:24:28.166000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:28.175000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:28.184000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:28.198000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:28.208000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:28.226000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:28.233000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:28.239000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:28.247000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:28.249000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:28.254000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:28.269000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:28.279000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:28.297000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:28.304000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:28.310000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:28.318000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:28.320000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:28.325000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:28.340000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:28.349000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:28.368000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:28 DP5 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1028 11:24:28.374000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:28 DP6 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:28 DP3 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1028 11:24:28.391000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:28 DP2 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:28 DP0 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:28 DP4 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:28 DP1 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:24:28 DP7 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1028 11:24:28.854000 817 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72bd280420>
[rank6]:W1028 11:24:28.862000 818 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5586b70420>
[rank3]:W1028 11:24:28.873000 815 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cff0>
[rank5]:W1028 11:24:28.877000 817 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72ba9db9c0>
[rank6]:W1028 11:24:28.885000 818 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5585401740>
[rank2]:W1028 11:24:28.888000 814 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1d170>
[rank3]:W1028 11:24:28.896000 815 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cae0>
[rank0]:W1028 11:24:28.898000 812 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29c930>
[rank2]:W1028 11:24:28.911000 814 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1c750>
[rank4]:W1028 11:24:28.911000 816 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28daa9f9f0>
[rank0]:W1028 11:24:28.921000 812 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29cbd0>
[rank1]:W1028 11:24:28.923000 813 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb2c27d61c0>
[rank4]:W1028 11:24:28.934000 816 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28d957f9f0>
[rank7]:W1028 11:24:28.938000 819 torch/_dynamo/variables/builtin.py:1091] [68/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8de6f91830>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:24:28 DP5 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1028 11:24:28.946000 813 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ff46b548ae0>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:24:28 DP6 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1028 11:24:28.961000 819 torch/_dynamo/variables/builtin.py:1091] [69/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fcf975d0ae0>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:24:28 DP3 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:24:28 DP2 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:24:28 DP0 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:24:29 DP4 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:24:29 DP1 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:24:29 DP7 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1028 11:24:30.617000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:30.627000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:30.638000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:30.651000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:30.661000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:30.671000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:30.712000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:30.737000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:30.905000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:30.915000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:30.927000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:30.940000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:30.951000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:30.962000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:30.988000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:31.017000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:31.181000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:31.200000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:31.212000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:31.225000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:31.236000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:31.247000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:31.264000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:31.293000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:31.457000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:31.484000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:31.496000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:31.509000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:31.520000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:31.531000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:31.543000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:31.569000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:31.733000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:31.768000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:31.786000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:31.798000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:31.810000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:31.822000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:31.832000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:31.857000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:32.021000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:32.052000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:32.071000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:32.084000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:32.095000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:32.106000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:32.117000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:32.133000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:32.309000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:32.336000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:32.358000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:32.370000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:32.381000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:32.393000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:32.404000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:32.416000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:32.589000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:32.620000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:32.642000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:32.654000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:32.666000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:32.678000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:32.688000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:32.701000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:32.869000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:32.903000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:32.926000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:32.939000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:32.950000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:32.962000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:32.972000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:32.985000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:33.149000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:33.188000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:33.210000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:33.223000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:33.234000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:33.246000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:33.256000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:33.269000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:33.987000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:33.999000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:34.009000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:34.019000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:34.031000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:34.042000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:34.055000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:34.096000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:34.286000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:34.298000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:34.309000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:34.319000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:34.333000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:34.344000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:34.357000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:34.372000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:35.004000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:35.014000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:35.025000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:35.038000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:35.048000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:35.059000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:35.122000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:35.136000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:35.724000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:35.736000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:35.747000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:35.758000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:35.770000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:35.783000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:35.840000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:35.850000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:36.406000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:36.418000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:36.429000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:36.441000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:36.451000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:36.461000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:36.471000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:36.510000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:36.686000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:36.706000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:36.718000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:36.730000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:36.741000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:36.752000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:36.764000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:36.786000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:36.968000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:36.992000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:37.002000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:37.012000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:37.029000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:37.043000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:37.057000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:37.070000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:37.255000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:37.280000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:37.289000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:37.300000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:37.313000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:37.327000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:37.341000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:37.355000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:37.543000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:37.568000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:37.577000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:37.588000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:37.601000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:37.616000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:37.628000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:37.642000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:37.835000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:37.856000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:37.865000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:37.876000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:37.898000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:37.911000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:37.926000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:37.939000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:38.123000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:38.144000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:38.154000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:38.164000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:38.181000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:38.195000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:38.209000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:38.223000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:38.415000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:38.432000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:38.441000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:38.451000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:38.465000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:38.479000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:38.493000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:38.506000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:38.707000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:38.720000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:38.729000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:38.740000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:38.753000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:38.767000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:38.781000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:38.794000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:38.999000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:39.010000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:39.019000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:39.029000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:39.042000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:39.056000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:39.070000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:39.083000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:39.291000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:39.302000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:39.311000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:39.321000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:39.334000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:39.348000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:39.362000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:39.375000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:39.580000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:39.591000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:39.601000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:39.612000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:39.625000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:39.638000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:39.653000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:39.666000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:39.867000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:39.879000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:39.889000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:39.900000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:39.913000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:39.927000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:39.941000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:39.954000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:40.156000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:40.167000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:40.177000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:40.188000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:40.205000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:40.219000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:40.233000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:40.247000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:40.444000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:40.456000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:40.466000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:40.476000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:40.489000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:40.503000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:40.517000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:40.530000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:40.731000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:40.744000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:40.753000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:40.764000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:40.777000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:40.791000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:40.805000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:40.818000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:41.019000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:41.032000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:41.042000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:41.052000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:41.065000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:41.078000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:41.093000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:41.106000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:41.307000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:41.320000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:41.330000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:41.341000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:41.353000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:41.366000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:41.381000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:41.394000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:41.595000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:41.608000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:41.618000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:41.628000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:41.641000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:41.654000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:41.669000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:41.682000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:41.883000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:41.895000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:41.906000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:41.916000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:41.928000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:41.942000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:41.956000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:41.969000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:42.174000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:42.186000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:42.198000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:42.210000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:42.221000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:42.232000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:42.244000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:42.255000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:42.460000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:42.472000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:42.482000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:42.493000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:42.517000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:42.531000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:42.546000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:42.558000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:42.747000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:42.760000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:42.770000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:42.780000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:42.801000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:42.815000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:42.829000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:42.842000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:43.034000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:43.050000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:43.062000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:43.074000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:43.084000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:43.095000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:43.107000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:43.118000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:43.322000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:43.338000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:43.350000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:43.362000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:43.372000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:43.383000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:43.396000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:43.405000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:43.604000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:43.620000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:43.629000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:43.640000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:43.661000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:43.675000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:43.690000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:43.701000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:43.891000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:43.908000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:43.917000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:43.927000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:43.945000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:43.959000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:43.973000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:43.986000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:44.183000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:44.198000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:44.210000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:44.222000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:44.233000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:44.244000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:44.255000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:44.267000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:44.468000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:44.484000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:44.493000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:44.504000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:44.521000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:44.536000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:44.548000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:44.562000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:44.755000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:44.771000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:44.782000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:44.791000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:44.805000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:44.821000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:44.835000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:44.848000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:45.043000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:45.062000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:45.075000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:45.087000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:45.097000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:45.108000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:45.120000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:45.131000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:45.334000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:45.350000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:45.362000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:45.374000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:45.385000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:45.396000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:45.407000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:45.418000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:45.620000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:45.636000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:45.646000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:45.656000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:45.673000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:45.689000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:45.703000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:45.716000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:45.907000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:45.928000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:45.938000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:45.948000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:45.960000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:45.977000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:45.993000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:46.007000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:46.198000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:46.221000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:46.233000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:46.246000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:46.256000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:46.267000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:46.278000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:46.289000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:46.485000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:46.508000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:46.518000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:46.529000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:46.546000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:46.559000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:46.574000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:46.587000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:46.772000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:46.796000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:46.805000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:46.816000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:46.829000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:46.845000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:46.859000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:46.873000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:47.059000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:47.088000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:47.097000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:47.107000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:47.120000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:47.134000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:47.148000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:47.161000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:47.351000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:47.379000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:47.389000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:47.400000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:47.412000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:47.426000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:47.440000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:47.454000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:47.639000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:47.672000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:47.682000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:47.692000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:47.704000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:47.718000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:47.732000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:47.746000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:47.927000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:47.964000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:47.974000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:47.984000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:47.997000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:48.011000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:48.025000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:48.038000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:48.219000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:48.256000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:48.266000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:48.276000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:48.288000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:48.302000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:48.316000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:48.330000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:48.511000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:48.547000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:48.557000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:48.568000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:48.580000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:48.594000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:48.608000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:48.622000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:24:49.351000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:24:49.363000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:24:49.374000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:24:49.385000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:24:49.397000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:24:49.407000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:24:49.416000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:24:49.425000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0634 ms 100.0% 
  triton_mm_663 0.1046 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1058 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1141 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1144 ms 55.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1162 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1194 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1203 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1565 ms 40.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_680 0.1567 ms 40.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8446 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0633 ms 100.0% 
  triton_mm_675 0.1054 ms 60.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_663 0.1059 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1145 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1147 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1164 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1185 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1197 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1567 ms 40.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_672 0.1569 ms 40.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.8552 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0619 ms 100.0% 
  triton_mm_663 0.1045 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1054 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1139 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1141 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1156 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1161 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1186 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_672 0.1550 ms 39.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_680 0.1578 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8780 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0623 ms 100.0% 
  triton_mm_663 0.1046 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1057 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1144 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1149 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1162 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1173 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1190 ms 52.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_672 0.1572 ms 39.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_681 0.1580 ms 39.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8500 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0625 ms 100.0% 
  triton_mm_675 0.1051 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_663 0.1059 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1145 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1148 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1162 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1185 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1218 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1569 ms 39.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_680 0.1570 ms 39.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8305 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0627 ms 100.0% 
  triton_mm_675 0.1054 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_663 0.1056 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1146 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_683 0.1147 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_673 0.1164 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1183 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1209 ms 51.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_672 0.1567 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_681 0.1568 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8391 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_663 0.1045 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1056 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1134 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1140 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_662 0.1151 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1153 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_674 0.1177 ms 51.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_672 0.1553 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_681 0.1565 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8133 seconds and 0.0001 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0621 ms 100.0% 
  triton_mm_663 0.1043 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1054 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_683 0.1132 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_682 0.1134 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_673 0.1151 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_662 0.1179 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_674 0.1191 ms 52.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_681 0.1564 ms 39.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_680 0.1564 ms 39.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8384 seconds and 0.0001 seconds precompiling for 27 choices
Capturing batches (bs=4 avail_mem=39.38 GB):  96%|| 50/52 [07:55<02:06, 63.25s/it]Capturing batches (bs=2 avail_mem=38.91 GB):  96%|| 50/52 [07:55<02:06, 63.25s/it][rank0]:W1028 11:25:02.215000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:02.238000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:02.249000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:02.263000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:02.283000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:02.286000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:02.293000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:02.306000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:02.309000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:02.320000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:02.320000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:02.334000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:02.355000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:02.361000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:02.365000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:02.377000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:02.385000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:02.392000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:02.395000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:02.410000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:02.431000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:02.440000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:02.452000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:02.467000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:02.866000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:02.888000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:02.900000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:02.912000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:02.939000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:02.950000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:02.953000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:02.960000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:02.975000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:02.975000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:02.986000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:02.998000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:03.026000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:03.030000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:03.036000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:03.047000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:03.050000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:03.058000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:03.066000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:03.071000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:03.097000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:03.103000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:03.109000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:03.118000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:03.122000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:03.129000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:03.139000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:03.141000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:03.175000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:03.181000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:03.193000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:03.211000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:03.786000 813 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank0]:W1028 11:25:03.787000 812 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank6]:W1028 11:25:03.800000 818 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank7]:W1028 11:25:03.809000 819 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank2]:W1028 11:25:03.809000 814 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank4]:W1028 11:25:03.837000 816 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank5]:W1028 11:25:03.866000 817 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
[rank3]:W1028 11:25:03.869000 815 torch/_inductor/utils.py:1349] [27/6] Please pip install Composable Kernel package
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_698 0.0079 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0080 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_701 0.0082 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0082 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_694 0.0083 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_702 0.0083 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_695 0.0083 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_703 0.0083 ms 95.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_704 0.0084 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_708 0.0084 ms 94.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8333 seconds and 0.1659 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_698 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0084 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_701 0.0084 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_694 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_703 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_709 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_702 0.0086 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_708 0.0086 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_695 0.0087 ms 94.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9422 seconds and 0.2463 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_701 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_699 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0082 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_709 0.0082 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_698 0.0082 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_702 0.0082 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_703 0.0082 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_708 0.0082 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_705 0.0083 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_704 0.0083 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9527 seconds and 0.2377 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_701 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_698 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_700 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_703 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_699 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_702 0.0084 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_707 0.0084 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_708 0.0084 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_706 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_704 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9617 seconds and 0.1627 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_698 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0082 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0084 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_701 0.0084 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_708 0.0085 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_709 0.0085 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_702 0.0085 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_694 0.0086 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_695 0.0086 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_703 0.0086 ms 94.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9503 seconds and 0.2823 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_698 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_700 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_701 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_699 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_706 0.0084 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_708 0.0084 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_709 0.0084 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_707 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_694 0.0085 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_702 0.0085 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9661 seconds and 0.2773 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_701 0.0082 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_700 0.0083 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_698 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_703 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_708 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_702 0.0085 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_694 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_706 0.0086 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_704 0.0086 ms 95.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9464 seconds and 0.1208 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x128, 128x128x512)
  triton_bmm_700 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_699 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_701 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_698 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_694 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_695 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_707 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_708 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_706 0.0085 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_702 0.0085 ms 97.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9423 seconds and 0.1231 seconds precompiling for 25 choices
[rank2]:W1028 11:25:13.908000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:14.210000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:14.251000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:14.272000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:14.353000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:14.369000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:14.416000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:14.500000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:14.626000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:14.723000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:14.768000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:14.778000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:14.859000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:14.879000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:15.007000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:15.152000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:16.068000 814 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank5]:W1028 11:25:16.101000 817 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank0]:W1028 11:25:16.115000 812 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank6]:W1028 11:25:16.128000 818 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank7]:W1028 11:25:16.190000 819 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank3]:W1028 11:25:16.312000 815 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank4]:W1028 11:25:16.431000 816 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
[rank1]:W1028 11:25:16.468000 813 torch/_inductor/utils.py:1349] [39/6_1] Please pip install Composable Kernel package
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_721 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_720 0.0083 ms 97.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0084 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0084 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_732 0.0091 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_733 0.0092 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_728 0.0093 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0093 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_715 0.0096 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_714 0.0096 ms 83.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9164 seconds and 0.4147 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0082 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_723 0.0084 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0084 ms 96.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_732 0.0091 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_733 0.0091 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_714 0.0092 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0093 ms 87.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_729 0.0094 ms 86.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_728 0.0095 ms 86.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8812 seconds and 0.4188 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_721 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_720 0.0084 ms 99.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_722 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_732 0.0093 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_733 0.0093 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_728 0.0094 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0095 ms 88.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_715 0.0100 ms 83.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_714 0.0101 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9039 seconds and 0.4202 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0082 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0083 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0083 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_733 0.0091 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_732 0.0092 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_714 0.0095 ms 85.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_728 0.0095 ms 85.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0095 ms 85.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_715 0.0096 ms 84.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9278 seconds and 0.4520 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0082 ms 98.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_723 0.0082 ms 98.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0083 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_732 0.0091 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_728 0.0092 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0092 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_733 0.0093 ms 86.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_714 0.0096 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0096 ms 83.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9310 seconds and 0.4224 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0084 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0085 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_733 0.0091 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_732 0.0094 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_715 0.0096 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_728 0.0097 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_714 0.0097 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0097 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9336 seconds and 0.4068 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_721 0.0083 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0083 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0084 ms 98.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_720 0.0085 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_732 0.0092 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_733 0.0093 ms 89.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_728 0.0094 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0095 ms 86.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_714 0.0097 ms 84.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_715 0.0099 ms 83.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8090 seconds and 0.4010 seconds precompiling for 25 choices
AUTOTUNE bmm(128x2x512, 128x512x128)
  triton_bmm_720 0.0081 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_721 0.0083 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_722 0.0084 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_723 0.0085 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_732 0.0092 ms 87.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_733 0.0093 ms 87.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_714 0.0093 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_728 0.0095 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_729 0.0095 ms 84.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_715 0.0096 ms 84.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9243 seconds and 0.4229 seconds precompiling for 25 choices
[rank0]:W1028 11:25:22.662000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:22.681000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:22.737000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:22.739000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:22.761000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:22.789000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:22.812000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:22.817000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:22.821000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:22.869000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:22.880000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:22.900000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:22.929000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:22.950000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:22.951000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:22.960000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:22.990000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:23.007000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:23.010000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:23.030000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:23.057000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:23.069000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:23.081000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:23.120000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:26 DP5 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:26 DP0 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:26 DP3 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:26 DP1 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:27 DP6 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:27 DP7 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:27 DP4 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:27 DP2 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1028 11:25:27.777000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:27.783000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:27.799000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:27.808000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:27.817000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:27.846000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:27.850000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:27.854000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:27.870000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:27.880000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:27.888000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:27.893000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:27.897000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:27.913000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:27.917000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:27.922000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:27.931000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:27 DP3 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:27 DP0 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1028 11:25:27.960000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:27 DP5 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:27 DP1 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1028 11:25:27.987000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:27 DP6 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1028 11:25:28.004000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:28 DP7 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1028 11:25:28.059000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:28.076000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:28.102000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:28.120000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:28 DP4 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:28 DP2 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1028 11:25:28.686000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:28.697000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:28.708000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:28.720000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:28.732000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:28.744000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:28.756000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:28.770000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:28.987000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:28.998000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:29.009000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:29.019000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:29.034000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:29.045000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:29.056000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:29.070000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:29.286000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:29.298000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:29.308000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:29.319000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:29.333000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:29.344000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:29.356000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:29.370000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:29 DP3 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:29 DP0 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:29 DP7 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:29 DP5 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:29 DP1 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:29 DP6 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:29 DP4 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:29 DP2 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1028 11:25:29.979000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:29.987000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:29.998000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:30.011000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:30.029000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:30.037000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:30.051000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:30.053000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:30.059000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:30.069000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:30.070000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:30.082000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:30.101000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:30.109000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:30.123000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:30.125000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:30.130000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:30.141000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:30.142000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:30.154000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:30.172000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:30.181000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:30 DP3 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:30 DP0 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1028 11:25:30.195000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:30 DP7 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:30 DP5 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1028 11:25:30.213000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:30 DP1 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:30 DP6 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:30 DP4 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-28 11:25:30 DP2 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1028 11:25:30.667000 815 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cff0>
[rank0]:W1028 11:25:30.679000 812 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29c930>
[rank3]:W1028 11:25:30.690000 815 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cae0>
[rank7]:W1028 11:25:30.691000 819 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8de6f91830>
[rank0]:W1028 11:25:30.702000 812 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29cbd0>
[rank5]:W1028 11:25:30.703000 817 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72bd280420>
[rank1]:W1028 11:25:30.714000 813 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb2c27d61c0>
[rank7]:W1028 11:25:30.715000 819 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fcf975d0ae0>
[rank5]:W1028 11:25:30.726000 817 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72ba9db9c0>
[rank6]:W1028 11:25:30.728000 818 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5586b70420>
[rank1]:W1028 11:25:30.737000 813 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ff46b548ae0>
[rank4]:W1028 11:25:30.741000 816 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28daa9f9f0>
[rank6]:W1028 11:25:30.751000 818 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5585401740>
[rank2]:W1028 11:25:30.755000 814 torch/_dynamo/variables/builtin.py:1091] [68/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1d170>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:25:30 DP3 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1028 11:25:30.764000 816 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28d957f9f0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:25:30 DP0 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1028 11:25:30.778000 814 torch/_dynamo/variables/builtin.py:1091] [69/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1c750>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:25:30 DP7 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:25:30 DP5 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:25:30 DP1 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:25:30 DP6 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:25:30 DP4 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:25:30 DP2 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1028 11:25:32.350000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:32.362000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:32.372000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:32.385000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:32.396000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:32.407000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:32.431000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:32.458000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:32.638000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:32.654000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:32.668000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:32.680000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:32.693000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:32.705000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:32.719000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:32.738000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:32.926000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:32.942000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:32.959000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:32.972000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:32.985000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:32.997000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:33.009000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:33.020000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:33.218000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:33.230000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:33.258000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:33.269000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:33.282000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:33.295000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:33.309000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:33.318000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:33.510000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:33.522000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:33.542000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:33.563000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:33.576000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:33.587000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:33.600000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:33.609000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:33.802000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:33.814000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:33.829000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:33.855000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:33.869000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:33.880000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:33.892000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:33.902000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:34.094000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:34.106000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:34.119000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:34.148000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:34.160000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:34.171000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:34.183000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:34.194000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:34.384000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:34.395000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:34.406000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:34.442000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:34.453000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:34.467000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:34.482000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:34.495000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:34.678000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:34.690000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:34.703000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:34.739000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:34.752000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:34.764000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:34.776000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:34.793000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:34.970000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:34.982000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:34.995000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:35.041000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:35.052000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:35.063000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:35.076000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:35.085000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:35.260000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:35.271000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:35.282000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:35.327000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:35.346000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:35.360000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:35.375000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:35.387000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:35.556000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:35.565000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:35.575000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:35.619000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:35.634000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:35.648000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:35.663000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:35.676000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:35.852000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:35.861000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:35.871000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:35.911000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:35.926000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:35.940000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:35.954000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:35.967000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:36.144000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:36.155000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:36.165000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:36.203000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:36.226000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:36.240000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:36.254000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:36.268000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:36.447000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:36.457000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:36.467000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:36.495000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:36.526000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:36.540000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:36.554000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:36.568000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:36.742000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:36.754000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:36.767000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:36.789000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:36.816000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:36.827000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:36.840000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:36.851000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:37.038000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:37.051000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:37.063000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:37.077000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:37.111000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:37.121000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:37.133000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:37.144000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:37.328000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:37.339000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:37.350000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:37.363000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:37.406000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:37.420000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:37.436000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:37.447000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:37.623000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:37.633000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:37.643000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:37.655000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:37.696000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:37.708000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:37.720000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:37.732000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:37.918000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:37.930000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:37.943000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:37.955000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:37.999000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:38.010000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:38.022000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:38.033000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:38.214000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:38.226000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:38.239000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:38.252000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:38.303000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:38.313000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:38.327000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:38.336000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:39.003000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:39.013000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:39.023000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:39.033000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:39.046000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:39.059000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:39.072000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:39.118000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:39.291000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:39.304000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:39.316000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:39.328000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:39.355000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:39.369000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:39.411000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:39.985000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:40.164000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:40.174000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:40.189000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:40.198000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:40.209000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:40.274000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:40.287000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:40.299000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:41.044000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:41.054000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:41.064000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:41.077000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:41.090000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:41.102000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:41.125000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:41.158000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:41.804000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:41.816000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:41.828000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:41.841000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:41.854000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:41.865000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:41.875000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:41.920000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:42.105000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:42.116000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:42.129000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:42.142000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:42.156000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:42.168000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:42.179000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:42.212000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:42.407000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:42.420000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:42.432000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:42.448000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:42.460000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:42.473000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:42.487000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:42.506000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:42.699000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:42.718000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:42.729000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:42.748000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:42.760000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:42.772000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:42.788000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:42.805000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:43.006000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:43.016000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:43.038000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:43.059000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:43.069000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:43.082000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:43.094000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:43.107000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:43.298000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:43.316000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:43.334000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:43.354000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:43.365000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:43.379000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:43.393000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:43.404000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:43.587000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:43.619000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:43.629000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:43.648000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:43.661000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:43.673000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:43.690000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:43.705000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:43.879000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:43.918000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:43.928000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:43.944000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:43.957000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:43.969000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:43.983000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:43.998000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:44.171000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:44.215000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:44.228000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:44.240000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:44.253000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:44.264000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:44.278000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:44.293000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:44.467000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:44.514000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:44.536000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:44.549000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:44.559000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:44.572000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:44.585000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:44.599000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:44.771000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:44.810000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:44.832000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:44.845000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:44.859000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:44.870000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:44.884000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:44.898000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:45.071000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:45.106000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:45.128000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:45.140000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:45.160000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:45.170000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:45.184000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:45.204000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:45.379000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:45.402000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:45.430000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:45.440000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:45.468000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:45.481000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:45.493000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:45.507000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:45.683000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:45.698000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:45.722000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:45.736000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:45.765000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:45.775000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:45.789000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:45.804000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:45.979000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:45.995000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:46.014000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:46.032000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:46.061000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:46.071000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:46.085000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:46.099000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:46.275000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:46.290000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:46.303000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:46.328000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:46.357000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:46.369000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:46.381000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:46.396000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:46.571000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:46.586000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:46.599000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:46.624000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:46.652000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:46.663000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:46.677000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:46.691000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:46.863000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:46.882000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:46.895000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:46.928000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:46.956000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:46.968000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:46.981000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:46.995000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:47.167000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:47.180000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:47.192000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:47.232000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:47.264000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:47.278000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:47.289000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:47.303000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:47.479000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:47.491000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:47.503000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:47.528000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:47.561000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:47.573000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:47.585000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:47.599000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:47.771000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:47.784000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:47.797000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:47.824000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:47.857000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:47.869000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:47.881000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:47.896000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:48.071000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:48.084000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:48.097000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:48.119000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:48.153000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:48.165000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:48.177000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:48.192000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:48.363000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:48.378000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:48.391000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:48.416000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:48.448000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:48.461000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:48.473000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:48.487000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:48.666000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:48.676000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:48.686000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:48.714000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:48.746000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:48.758000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:48.772000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:48.785000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:48.966000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:48.976000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:48.986000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:49.007000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:49.042000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:49.056000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:49.068000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:49.081000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:49.269000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:49.280000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:49.290000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:49.303000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:49.338000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:49.352000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:49.364000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:49.377000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:49.561000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:49.576000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:49.588000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:49.600000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:49.630000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:49.648000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:49.661000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:49.673000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:49.853000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:49.872000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:49.884000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:49.897000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:49.926000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:49.945000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:49.958000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:49.971000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:50.147000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:50.170000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:50.183000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:50.194000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:50.220000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:50.242000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:50.254000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:50.268000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:50.443000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:50.466000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:50.478000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:50.489000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:50.516000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:50.538000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:50.550000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:50.565000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:50.742000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:50.760000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:50.771000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:50.786000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:50.822000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:50.834000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:50.854000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:50.866000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:51.043000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:51.059000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:51.071000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:51.083000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:51.116000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:51.134000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:51.146000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:51.162000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:25:51.892000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:25:51.907000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:25:51.920000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:25:51.930000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:25:51.941000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:25:51.953000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:25:51.963000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:25:51.974000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0620 ms 100.0% 
  triton_mm_736 0.1037 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1037 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_747 0.1047 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_746 0.1047 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1090 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1091 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1128 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1129 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_741 0.1479 ms 41.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1276 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0619 ms 100.0% 
  triton_mm_736 0.1037 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1037 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_747 0.1043 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_746 0.1044 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_754 0.1092 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1092 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_745 0.1129 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1129 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_741 0.1485 ms 41.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2125 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0619 ms 100.0% 
  triton_mm_736 0.1037 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1038 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_747 0.1045 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_746 0.1047 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1094 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1094 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_744 0.1132 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1132 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_741 0.1485 ms 41.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1678 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0620 ms 100.0% 
  triton_mm_737 0.1037 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_736 0.1037 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_747 0.1043 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_746 0.1046 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_754 0.1094 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1094 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1132 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1133 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_740 0.1482 ms 41.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2219 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0616 ms 100.0% 
  triton_mm_737 0.1035 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_736 0.1035 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_746 0.1046 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_747 0.1047 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1090 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1090 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1125 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1125 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_741 0.1474 ms 41.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2219 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0620 ms 100.0% 
  triton_mm_736 0.1037 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1038 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_746 0.1047 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_747 0.1048 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_755 0.1092 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1094 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_744 0.1130 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1131 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_740 0.1482 ms 41.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2237 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0620 ms 100.0% 
  triton_mm_737 0.1039 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_736 0.1040 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_746 0.1050 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_747 0.1050 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_754 0.1096 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1096 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_745 0.1132 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1135 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_741 0.1491 ms 41.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1609 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0623 ms 100.0% 
  triton_mm_736 0.1039 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_737 0.1039 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_747 0.1043 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_746 0.1044 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_754 0.1094 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_755 0.1094 ms 57.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_744 0.1132 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_745 0.1132 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_740 0.1484 ms 42.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1044 seconds and 0.0001 seconds precompiling for 25 choices
Capturing batches (bs=2 avail_mem=38.91 GB):  98%|| 51/52 [08:58<01:03, 63.08s/it]Capturing batches (bs=1 avail_mem=38.46 GB):  98%|| 51/52 [08:58<01:03, 63.08s/it][rank5]:W1028 11:26:04.879000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:04.890000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:04.909000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:04.924000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:04.945000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:04.952000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:04.963000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:04.970000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:04.981000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:04.996000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:05.018000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:05.029000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:05.039000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:05.043000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:05.056000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:05.072000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:05.094000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:05.096000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:05.121000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:05.167000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:05.243000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:05.430000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:05.503000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:05.531000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:05.539000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:05.556000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:05.573000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:05.580000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/40] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:05.600000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:05.623000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:05.625000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:05.631000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:05.644000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:05.662000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:05.689000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:05.698000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:05.706000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:05.714000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:05.718000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:05.735000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:05.757000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:05.764000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:05.771000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:05.778000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:05.788000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:05.790000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:05.807000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:05.837000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:05.845000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:05.861000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:05.918000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:05.990000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:06.092000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [24/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:06.181000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:06.255000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:06.328000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/41] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:12.218000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:12.344000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:12.564000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:12.633000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:12.696000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:12.748000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:12.842000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:12.898000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:13.045000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:13.094000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:13.146000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:13.199000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:13.404000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:13.547000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:13.757000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [32/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:14.273000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:14.931000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:14.998000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:15.011000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:15.062000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:15.080000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:15.132000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:15.348000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:15.431000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:15.432000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:15.478000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:15.486000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:15.513000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:15.559000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:15.566000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:15.611000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:15.723000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:15.748000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:15.802000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:15.831000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:15.853000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:15.884000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:16.633000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:16.714000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:16.766000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/42] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:20.432000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:20.443000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:20.454000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:20.464000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:20.478000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:20.488000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:20.504000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:20.505000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:20.515000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:20.526000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:20.535000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:20.548000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:20.550000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:20.559000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:20.559000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:20.569000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:20.577000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:20.578000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:20.592000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:20.602000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:20.620000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:20.657000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:20.739000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:20.782000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/43] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:21.351000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:21.362000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:21.374000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:21.388000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:21.400000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:21.410000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:21.423000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:21.437000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:21.647000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:21.658000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:21.672000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:21.686000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:21.698000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:21.711000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:21.721000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:21.735000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:21.943000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:21.954000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:21.972000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:21.985000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:21.997000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:22.010000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:22.021000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:22.035000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:22.636000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:22.647000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:22.678000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:22.692000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:22.707000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:22.708000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:22.718000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:22.718000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:22.729000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:22.743000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [11/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:22.752000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:22.765000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:22.779000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:22.780000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:22.790000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:22.790000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:22.801000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:22.816000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [13/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:22.824000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:22.837000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:22.853000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:22.862000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:22.873000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:22.887000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [17/44] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:23.302000 813 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fb2c27d61c0>
[rank0]:W1028 11:26:23.311000 812 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29c930>
[rank1]:W1028 11:26:23.325000 813 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ff46b548ae0>
[rank0]:W1028 11:26:23.334000 812 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f166b29cbd0>
[rank6]:W1028 11:26:23.348000 818 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5586b70420>
[rank2]:W1028 11:26:23.360000 814 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1d170>
[rank6]:W1028 11:26:23.371000 818 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5585401740>
[rank5]:W1028 11:26:23.377000 817 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72bd280420>
[rank2]:W1028 11:26:23.384000 814 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f639fc1c750>
[rank4]:W1028 11:26:23.389000 816 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28daa9f9f0>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:23 DP1 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1028 11:26:23.400000 817 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f72ba9db9c0>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:23 DP0 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1028 11:26:23.403000 819 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f8de6f91830>
[rank4]:W1028 11:26:23.412000 816 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f28d957f9f0>
[rank3]:W1028 11:26:23.418000 815 torch/_dynamo/variables/builtin.py:1091] [68/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cff0>
[rank7]:W1028 11:26:23.426000 819 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7fcf975d0ae0>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:23 DP6 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1028 11:26:23.441000 815 torch/_dynamo/variables/builtin.py:1091] [69/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7ec7a782cae0>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:23 DP2 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:23 DP5 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:23 DP4 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:23 DP7 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:23 DP3 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1028 11:26:24.942000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:24.954000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:24.963000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:24.973000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:24.983000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:24.993000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:25.042000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:25.058000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:25.242000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:25.255000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:25.264000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:25.275000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:25.287000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:25.298000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:25.338000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:25.353000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:25.542000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:25.555000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:25.564000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:25.576000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:25.588000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:25.599000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:25.634000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:25.649000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:25.842000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:25.855000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:25.864000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:25.876000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:25.887000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:25.898000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:25.931000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:25.946000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:26.142000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:26.155000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:26.171000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:26.188000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:26.199000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:26.211000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:26.226000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:26.241000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:26.442000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:26.455000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:26.479000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:26.500000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:26.513000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:26.524000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:26.537000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:26.552000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:26.742000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:26.755000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:26.775000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:26.800000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:26.812000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:26.823000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:26.838000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:26.853000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:27.042000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:27.055000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:27.071000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:27.100000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:27.111000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:27.123000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:27.137000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:27.152000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:27.342000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:27.355000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:27.367000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:27.400000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:27.411000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:27.423000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:27.438000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:27.452000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:27.642000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:27.655000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:27.664000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:27.700000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:27.711000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:27.723000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:27.738000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:27.752000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:27.940000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:27.952000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:27.964000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:28.002000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:28.017000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:28.031000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:28.044000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:28.056000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:28.244000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:28.255000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:28.266000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:28.302000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:28.318000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:28.331000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:28.346000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:28.356000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:28.551000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:28.564000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:28.573000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:28.608000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:28.619000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:28.630000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:28.646000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:28.661000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:28.855000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:28.867000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:28.876000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:28.912000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:28.925000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:28.939000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:28.952000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:28.965000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:29.156000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:29.168000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:29.179000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:29.214000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:29.229000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:29.243000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:29.257000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:29.268000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:29.460000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:29.471000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:29.482000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:29.514000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:29.528000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:29.542000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:29.558000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:29.568000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:29.764000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:29.774000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:29.786000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:29.814000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:29.828000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:29.842000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:29.860000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:29.871000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:30.071000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:30.083000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:30.093000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:30.116000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:30.129000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:30.139000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:30.170000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:30.184000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:30.374000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:30.387000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:30.397000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:30.416000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:30.436000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:30.448000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:30.470000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:30.485000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:30.674000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:30.687000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:30.696000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:30.716000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:30.736000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:30.747000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:30.772000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:30.785000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:30.973000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:30.984000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:30.997000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:31.022000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:31.039000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:31.052000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:31.069000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:31.081000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:31.276000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:31.288000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:31.309000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:31.334000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:31.350000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:31.364000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:31.377000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:31.389000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:31.580000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:31.592000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:31.605000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:31.634000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:31.649000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:31.663000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:31.680000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:31.691000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:31.884000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:31.896000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:31.907000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:31.934000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:31.948000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:31.962000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:31.982000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:31.992000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:32.188000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:32.200000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:32.211000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:32.234000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:32.248000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:32.263000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:32.284000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:32.295000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:32.492000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:32.504000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:32.515000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:32.534000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:32.548000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:32.562000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:32.584000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:32.597000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:32.796000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:32.808000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:32.819000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:32.842000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:32.856000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:32.870000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:32.886000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:32.896000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:33.100000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:33.112000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:33.123000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:33.154000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:33.169000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:33.182000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:33.197000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:33.208000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:33.404000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:33.416000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:33.428000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:33.454000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:33.468000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:33.483000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:33.500000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:33.511000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:33.708000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:33.720000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:33.732000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:33.754000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:33.768000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:33.782000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:33.804000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:33.815000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:34.020000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:34.030000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:34.042000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:34.056000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:34.070000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:34.086000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:34.104000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:34.117000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:34.328000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:34.338000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:34.350000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:34.364000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:34.378000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:34.392000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:34.406000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:34.417000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:34.643000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:34.655000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:34.665000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:34.676000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:34.688000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:34.699000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:34.715000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:34.729000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:34.954000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:34.967000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:34.977000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:34.988000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:35.000000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:35.013000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:35.027000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:35.042000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:35.259000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:35.272000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:35.281000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:35.293000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:35.304000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:35.316000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:35.331000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:35.347000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:35.563000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:35.576000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:35.585000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:35.597000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:35.608000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:35.619000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:35.636000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:35.649000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:35.867000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:35.880000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:35.889000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:35.901000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:35.912000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:35.924000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:35.940000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:35.953000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:36.168000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:36.180000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:36.193000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:36.207000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:36.222000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:36.236000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:36.250000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:36.261000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:36.476000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:36.487000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:36.506000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:36.522000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:36.537000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:36.551000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:36.565000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:36.576000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:36.787000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:36.800000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:36.815000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:36.836000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:36.848000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:36.861000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:36.876000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:36.890000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:37.748000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:37.759000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:37.772000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:37.782000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:37.796000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:37.808000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:37.823000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:37.865000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:38.795000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:38.805000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:38.816000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:38.827000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:38.838000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:38.882000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:38.902000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:38.916000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:39.690000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:39.703000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:39.714000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:39.724000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:39.734000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:39.747000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:39.761000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:39.809000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:39.994000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:40.011000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:40.024000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:40.034000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:40.046000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:40.059000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:40.074000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:40.112000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:40.298000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:40.319000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:40.332000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:40.343000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:40.356000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:40.368000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:40.383000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:40.416000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:40.602000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:40.627000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:40.640000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:40.651000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:40.664000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:40.676000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:40.691000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:40.720000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:40.905000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:40.935000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:40.948000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:40.959000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:40.972000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:40.984000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:40.999000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:41.024000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:41.210000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:41.251000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:41.261000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:41.272000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:41.283000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:41.298000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:41.314000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:41.328000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:41.514000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:41.563000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:41.574000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:41.584000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:41.595000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:41.609000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:41.624000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:41.637000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:41.822000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:41.875000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:41.886000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:41.897000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:41.908000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:41.923000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:41.937000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:41.950000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:42.138000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:42.195000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:42.206000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:42.216000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:42.227000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:42.241000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:42.256000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:42.269000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:42.454000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:42.503000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:42.516000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:42.526000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:42.538000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:42.551000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:42.566000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:42.578000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:42.762000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:42.815000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:42.826000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:42.837000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:42.848000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:42.862000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:42.877000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:42.889000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:43.074000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:43.123000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:43.136000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:43.146000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:43.158000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:43.171000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:43.186000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:43.199000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:43.386000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:43.431000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:43.448000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:43.458000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:43.470000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:43.483000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:43.498000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:43.510000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:43.698000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:43.738000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:43.764000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:43.774000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:43.786000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:43.799000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:43.815000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:43.828000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:44.014000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:44.059000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:44.076000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:44.087000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:44.098000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:44.114000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:44.129000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:44.142000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [34/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1028 11:26:44.893000 817 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1028 11:26:44.903000 819 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1028 11:26:44.913000 818 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1028 11:26:44.930000 812 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1028 11:26:44.947000 814 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1028 11:26:44.959000 813 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1028 11:26:44.972000 815 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1028 11:26:44.982000 816 torch/_functorch/_aot_autograd/autograd_cache.py:852] [81/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0617 ms 100.0% 
  triton_mm_760 0.1039 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_761 0.1039 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_770 0.1043 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1044 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1087 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1087 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1124 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1126 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_764 0.1403 ms 44.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.2019 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0618 ms 100.0% 
  triton_mm_761 0.1044 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_760 0.1045 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_770 0.1045 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1047 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1088 ms 56.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1090 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_769 0.1127 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1128 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_765 0.1407 ms 43.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1915 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0618 ms 100.0% 
  triton_mm_770 0.1045 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_760 0.1046 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_761 0.1046 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_771 0.1046 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_779 0.1090 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1090 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_768 0.1128 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1130 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_764 0.1413 ms 43.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1968 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0619 ms 100.0% 
  triton_mm_760 0.1044 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_761 0.1044 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_771 0.1046 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_770 0.1047 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_778 0.1087 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1087 ms 56.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_769 0.1124 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1124 ms 55.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_764 0.1403 ms 44.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1892 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_760 0.1039 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_761 0.1039 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_771 0.1043 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_770 0.1044 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1084 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1085 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_768 0.1120 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1122 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_765 0.1399 ms 43.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1976 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0614 ms 100.0% 
  triton_mm_760 0.1039 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_761 0.1040 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_770 0.1041 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_771 0.1042 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1084 ms 56.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1084 ms 56.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1119 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1120 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_764 0.1399 ms 43.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1797 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0615 ms 100.0% 
  triton_mm_771 0.1045 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_760 0.1045 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_761 0.1046 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_770 0.1047 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1088 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_778 0.1088 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1121 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1122 ms 54.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_765 0.1405 ms 43.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1808 seconds and 0.0001 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0616 ms 100.0% 
  triton_mm_761 0.1041 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_760 0.1042 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_771 0.1045 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_770 0.1046 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_778 0.1090 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_779 0.1090 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_768 0.1128 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_769 0.1128 ms 54.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_765 0.1415 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1961 seconds and 0.0001 seconds precompiling for 25 choices
Capturing batches (bs=1 avail_mem=38.46 GB): 100%|| 52/52 [09:49<00:00, 59.68s/it]Capturing batches (bs=1 avail_mem=38.46 GB): 100%|| 52/52 [09:49<00:00, 11.34s/it]
[2025-10-28 11:26:55 DP0 TP0] Registering 22 cuda graph addresses
[2025-10-28 11:26:56 DP7 TP7] Capture cuda graph end. Time elapsed: 590.17 s. mem usage=13.51 GB. avail mem=37.53 GB.
[2025-10-28 11:26:56 DP0 TP0] Capture cuda graph end. Time elapsed: 591.40 s. mem usage=13.22 GB. avail mem=38.10 GB.
[2025-10-28 11:26:56 DP5 TP5] Capture cuda graph end. Time elapsed: 591.36 s. mem usage=13.50 GB. avail mem=37.55 GB.
[2025-10-28 11:26:56 DP4 TP4] Capture cuda graph end. Time elapsed: 591.43 s. mem usage=13.63 GB. avail mem=37.33 GB.
[2025-10-28 11:26:56 DP1 TP1] Capture cuda graph end. Time elapsed: 591.44 s. mem usage=13.64 GB. avail mem=37.27 GB.
[2025-10-28 11:26:56 DP6 TP6] Capture cuda graph end. Time elapsed: 590.29 s. mem usage=13.49 GB. avail mem=37.54 GB.
[2025-10-28 11:26:56 DP3 TP3] Capture cuda graph end. Time elapsed: 591.37 s. mem usage=13.65 GB. avail mem=37.27 GB.
[2025-10-28 11:26:56 DP2 TP2] Capture cuda graph end. Time elapsed: 591.51 s. mem usage=13.63 GB. avail mem=37.27 GB.
[2025-10-28 11:26:56 DP0 TP0] max_total_num_tokens=676049, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=2112, context_len=163840, available_gpu_mem=38.10 GB
[2025-10-28 11:26:57] INFO:     Started server process [42]
[2025-10-28 11:26:57] INFO:     Waiting for application startup.
[2025-10-28 11:26:57] INFO:     Application startup complete.
[2025-10-28 11:26:57] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-28 11:26:58] INFO:     127.0.0.1:44168 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-28 11:26:58 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:26:58 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:58 DP7 TP7] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:58 DP6 TP6] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:58 DP5 TP5] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:58 DP4 TP4] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:58 DP3 TP3] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:58 DP2 TP2] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:59 DP0 TP0] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:59 DP1 TP1] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:59 DP4 TP4] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:26:59 DP6 TP6] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:26:59 DP2 TP2] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:26:59 DP7 TP7] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:26:59 DP5 TP5] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:26:59 DP3 TP3] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:59 DP0 TP0] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:26:59 DP1 TP1] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:00 DP7 TP7] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:00 DP5 TP5] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:00 DP4 TP4] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:00 DP3 TP3] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:00 DP6 TP6] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:00 DP2 TP2] [fused_moe] using default for (44, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:00] INFO:     127.0.0.1:44170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:00] The server is fired up and ready to roll!
[2025-10-28 11:27:00] INFO:     127.0.0.1:44184 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-28 11:27:08] INFO:     127.0.0.1:45940 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-28 11:27:08 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 666, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP1 TP1] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP3 TP3] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP2 TP2] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP7 TP7] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP4 TP4] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP5 TP5] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP6 TP6] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP0 TP0] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08] INFO:     127.0.0.1:45944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:08 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 696, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP0 TP0] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP2 TP2] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP3 TP3] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP4 TP4] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP5 TP5] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP7 TP7] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP6 TP6] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP1 TP1] [fused_moe] using default for (696, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:08 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 1450, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:27:08 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 1443, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:27:08 DP2 TP2] Prefill batch, #new-seq: 3, #new-token: 2180, #cached-token: 3, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:27:08 DP0 TP0] Prefill batch, #new-seq: 2, #new-token: 162, #cached-token: 1334, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:27:08 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 1513, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:27:08 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 1436, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:27:08 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 4377, #cached-token: 6, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-28 11:27:08 DP3 TP3] Prefill batch, #new-seq: 3, #new-token: 2129, #cached-token: 3, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_pfl_bf16_a16w16_causal_subQ128_mqa128.co GetFunction: _ZN5aiter41mla_pfl_bf16_a16w16_causal_subQ128_mqa128E Success
[2025-10-28 11:27:08 DP4 TP4] Prefill batch, #new-seq: 22, #new-token: 16147, #cached-token: 22, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:27:08 DP6 TP6] Prefill batch, #new-seq: 21, #new-token: 15181, #cached-token: 21, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:27:08 DP2 TP2] Prefill batch, #new-seq: 21, #new-token: 15298, #cached-token: 21, token usage: 0.00, #running-req: 3, #queue-req: 0, 
[2025-10-28 11:27:08 DP5 TP5] Prefill batch, #new-seq: 21, #new-token: 15396, #cached-token: 21, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:27:08 DP7 TP7] Prefill batch, #new-seq: 21, #new-token: 15315, #cached-token: 21, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:27:08 DP3 TP3] Prefill batch, #new-seq: 21, #new-token: 15240, #cached-token: 21, token usage: 0.00, #running-req: 3, #queue-req: 0, 
[2025-10-28 11:27:08 DP1 TP1] Prefill batch, #new-seq: 17, #new-token: 1103, #cached-token: 11373, token usage: 0.01, #running-req: 7, #queue-req: 0, 
[2025-10-28 11:27:08 DP0 TP0] Prefill batch, #new-seq: 21, #new-token: 1353, #cached-token: 14007, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:27:17 DP6 TP6] Prefill batch, #new-seq: 142, #new-token: 8384, #cached-token: 95004, token usage: 0.02, #running-req: 23, #queue-req: 0, 
[2025-10-28 11:27:17 DP5 TP5] Prefill batch, #new-seq: 142, #new-token: 8370, #cached-token: 94998, token usage: 0.02, #running-req: 23, #queue-req: 0, 
[2025-10-28 11:27:17 DP2 TP2] Prefill batch, #new-seq: 141, #new-token: 8943, #cached-token: 94331, token usage: 0.02, #running-req: 24, #queue-req: 0, 
[2025-10-28 11:27:17 DP3 TP3] Prefill batch, #new-seq: 141, #new-token: 8448, #cached-token: 94352, token usage: 0.02, #running-req: 24, #queue-req: 0, 
[2025-10-28 11:27:17 DP4 TP4] Prefill batch, #new-seq: 141, #new-token: 8139, #cached-token: 94329, token usage: 0.03, #running-req: 24, #queue-req: 0, 
[2025-10-28 11:27:17 DP7 TP7] Prefill batch, #new-seq: 142, #new-token: 8946, #cached-token: 94999, token usage: 0.02, #running-req: 23, #queue-req: 0, 
[2025-10-28 11:27:17 DP0 TP0] Prefill batch, #new-seq: 141, #new-token: 8839, #cached-token: 94348, token usage: 0.00, #running-req: 23, #queue-req: 0, 
[2025-10-28 11:27:17 DP1 TP1] Prefill batch, #new-seq: 141, #new-token: 8393, #cached-token: 94356, token usage: 0.00, #running-req: 24, #queue-req: 0, 
[2025-10-28 11:27:24] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:25] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:25 DP4 TP4] Decode batch, #running-req: 165, #token: 15757, token usage: 0.02, cuda graph: True, gen throughput (token/s): 183.94, #queue-req: 0, 
[2025-10-28 11:27:25 DP6 TP6] Decode batch, #running-req: 165, #token: 15562, token usage: 0.02, cuda graph: True, gen throughput (token/s): 183.97, #queue-req: 0, 
[2025-10-28 11:27:25 DP0 TP0] Decode batch, #running-req: 164, #token: 16340, token usage: 0.02, cuda graph: True, gen throughput (token/s): 182.86, #queue-req: 0, 
[2025-10-28 11:27:25 DP5 TP5] Decode batch, #running-req: 164, #token: 15875, token usage: 0.02, cuda graph: True, gen throughput (token/s): 183.76, #queue-req: 0, 
[2025-10-28 11:27:25 DP1 TP1] Decode batch, #running-req: 165, #token: 15962, token usage: 0.02, cuda graph: True, gen throughput (token/s): 183.94, #queue-req: 0, 
[2025-10-28 11:27:25 DP2 TP2] Decode batch, #running-req: 165, #token: 16448, token usage: 0.02, cuda graph: True, gen throughput (token/s): 183.97, #queue-req: 0, 
[2025-10-28 11:27:25 DP7 TP7] Decode batch, #running-req: 165, #token: 16395, token usage: 0.02, cuda graph: True, gen throughput (token/s): 183.94, #queue-req: 0, 
[2025-10-28 11:27:25 DP3 TP3] Decode batch, #running-req: 165, #token: 15857, token usage: 0.02, cuda graph: True, gen throughput (token/s): 183.94, #queue-req: 0, 
[2025-10-28 11:27:25] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:25] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:45970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:46150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:49572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:47188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:50528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:50334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:56430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:49090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:47012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:56444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:48352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:56000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:51656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:50332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:46714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:46666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:50482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:53750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:46272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:46330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:48372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:52708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:51430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:46752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:26] INFO:     127.0.0.1:55752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:54684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:57784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:49730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:46374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:53934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:54214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:56576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:54102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:49388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:50296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:48828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:47628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:51982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:45946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:46092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:48016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:48288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:46354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:56146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:52258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:52256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:47162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:47524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:49864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:50240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:50030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:53976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:51402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:48924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:55920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:51724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:50066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:52142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:54522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:47362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:47728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:49268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:47254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:51822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:53904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:27] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:49548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:47456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:47614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:51130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:55026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:56556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:56116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:46446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:47086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:50320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:52050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:47582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:52372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:49304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:47684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:54880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:47688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:48786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:54306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:56122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:52364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:48464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:53776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:51474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:47888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:52310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:47154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:51798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:52618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:49250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:50236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:48126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:52466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:53066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:57168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:48820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:50504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:50874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:54144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:50330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:54956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:49454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:50696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:52530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:50932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:47706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:46240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:46678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:46804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:51466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:51672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:56934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:47142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:51582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:28] INFO:     127.0.0.1:53298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:51244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:53556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:53986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:46182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:49392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:51342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:48770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:49290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:45994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:51952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:56378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:49956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:53714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:54032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:48600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:53840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:46192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:56228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:46774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:50386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:54358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:48330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:46208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:46652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:48616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:48432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:50198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:51046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:48798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:48072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:51716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:46120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:46268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:53334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:56966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:50816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:51626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:54194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:51896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:55322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:49474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:51642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:54374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:49316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:50652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:56076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:46960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:54488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:49012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:50054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:50748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:49830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:51546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:55406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:48160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:55416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:55608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:50980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:53014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:48810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:55718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:54898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:55470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:53174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:46390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:46950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:47382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:49920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:29] INFO:     127.0.0.1:52406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30 DP1 TP1] Decode batch, #running-req: 117, #token: 16625, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1302.04, #queue-req: 0, 
[2025-10-28 11:27:30 DP4 TP4] Decode batch, #running-req: 122, #token: 16426, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1302.96, #queue-req: 0, 
[2025-10-28 11:27:30 DP6 TP6] Decode batch, #running-req: 109, #token: 14933, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1273.68, #queue-req: 0, 
[2025-10-28 11:27:30 DP5 TP5] Decode batch, #running-req: 119, #token: 16368, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1290.73, #queue-req: 0, 
[2025-10-28 11:27:30 DP7 TP7] Decode batch, #running-req: 123, #token: 17609, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1341.41, #queue-req: 0, 
[2025-10-28 11:27:30 DP0 TP0] Decode batch, #running-req: 135, #token: 19264, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1340.70, #queue-req: 0, 
[2025-10-28 11:27:30] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30 DP3 TP3] Decode batch, #running-req: 127, #token: 17336, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1302.88, #queue-req: 0, 
[2025-10-28 11:27:30 DP2 TP2] Decode batch, #running-req: 121, #token: 16948, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1310.28, #queue-req: 0, 
[2025-10-28 11:27:30] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:49744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:56838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:48384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:52060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:55138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:46518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:56742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:56270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:46288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:49462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:46812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:52476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:49170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:52046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:56210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:55490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:56820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:53800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:48478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:48430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:51892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:53520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:49186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:50354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:48050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:51224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:51964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:48936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:51388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:53850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:49828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:50968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:53586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:56940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:46510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:48976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:46694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:49970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:52854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:50750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:46880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:51080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:52824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:51172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:55468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:56306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:55018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:52004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:46262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:52334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:30] INFO:     127.0.0.1:53470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:51752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:54776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:51492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:52120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:53648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:46360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:48400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:53110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:57060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:46838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:48248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:52270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:50458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:46726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:51158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:53480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:48766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:46486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:50974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:52292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:53050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:51518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:53312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:46444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:51810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:53128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:48262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:55184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:50636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:52780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:51022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:53098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:46596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:55586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:50218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:53246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:50256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:46318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:52408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:48254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:54430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:50800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:51306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:54324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:53782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:48030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:51108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:52738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:46070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:46788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:46978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:57742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:49198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:51404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:46432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:51142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:47038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:52440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:31] INFO:     127.0.0.1:56890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:48682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:54564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:47244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:52898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:45984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:46994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:46312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:51708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:53918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:53536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:53962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:53730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:52318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:48538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:46730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:46300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:46500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:47104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:51256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:52320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:54612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:53392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:51922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:52404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:56744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:46558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:48890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:47396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:52686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:51690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:51062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:46162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:47974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:46136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:52352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:56718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:54748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:53494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:51606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:52136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:48690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:53476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:47484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:46998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:52236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:56102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:46040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:53732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:46482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:56614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:53860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:53932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:47380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:49782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:55352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:32] INFO:     127.0.0.1:50476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:50760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:50576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:46248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:51294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:51874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:55708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:47298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:49660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:50292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:56980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:50046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:46048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:55648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:50996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:49912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:53834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:46976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:50738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:49656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:57356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:46746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:56678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:57138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:46116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:46700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:53024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:55206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:47422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:55820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:49542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:47532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:49526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:55394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:51282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:49052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:51910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:46164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:47502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:55440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:56296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:47116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:57188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33 DP6 TP6] Decode batch, #running-req: 48, #token: 8817, token usage: 0.01, cuda graph: True, gen throughput (token/s): 849.84, #queue-req: 0, 
[2025-10-28 11:27:33 DP7 TP7] Decode batch, #running-req: 52, #token: 10402, token usage: 0.02, cuda graph: True, gen throughput (token/s): 932.75, #queue-req: 0, 
[2025-10-28 11:27:33 DP1 TP1] Decode batch, #running-req: 56, #token: 10321, token usage: 0.02, cuda graph: True, gen throughput (token/s): 928.19, #queue-req: 0, 
[2025-10-28 11:27:33 DP5 TP5] Decode batch, #running-req: 56, #token: 10329, token usage: 0.02, cuda graph: True, gen throughput (token/s): 983.22, #queue-req: 0, 
[2025-10-28 11:27:33 DP4 TP4] Decode batch, #running-req: 48, #token: 9668, token usage: 0.01, cuda graph: True, gen throughput (token/s): 849.24, #queue-req: 0, 
[2025-10-28 11:27:33 DP0 TP0] Decode batch, #running-req: 61, #token: 12030, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1064.40, #queue-req: 0, 
[2025-10-28 11:27:33 DP3 TP3] Decode batch, #running-req: 60, #token: 10884, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1031.16, #queue-req: 0, 
[2025-10-28 11:27:33 DP2 TP2] Decode batch, #running-req: 64, #token: 12474, token usage: 0.02, cuda graph: True, gen throughput (token/s): 977.35, #queue-req: 0, 
[2025-10-28 11:27:33] INFO:     127.0.0.1:49366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:51850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:56836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:49230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:46176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:54060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:51940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:50368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:47378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:56734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:51364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:56860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:57340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:52648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:51824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:46610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:33] INFO:     127.0.0.1:46680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:51440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:46786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:56988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:52570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:46046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:49608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:47682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:49222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:49962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:54352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:56388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:56590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:56350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:46342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:52514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:47314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:57406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:51796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:52492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:46076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:47122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:51484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:51974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:54726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:46030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:49176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:54220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:56274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:47730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:51980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:57162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:47786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:47332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:55646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:52304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:52214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:56500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:49794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:51962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:46024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:52020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:57812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:49072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:55958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:49708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:55548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:46464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:49916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:55696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:55660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:55272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:45990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:46570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:54276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:57282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:56558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:55082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:55076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:56810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:47740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:51750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:45978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:50684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:46220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:34] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:51380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:47076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:49510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:56532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:52454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:57076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:47482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:51698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:48904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:49064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:52628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:55518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:49254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:46418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:51574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:55686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:50656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:53570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:52080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:53360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:47278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:50772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:49848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:48028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:51450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:47654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:47772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:49784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:47220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:52934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:56366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:47726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:53788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:46472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:51978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:53182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:52694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:47712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:56090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:48342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:51334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:53124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:52104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:47930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:52656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:54774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:54534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:50108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:48512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:50156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:47212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:54128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:46260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:49902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:51192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:35] INFO:     127.0.0.1:51524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:56566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:46294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:48704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:54554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:57418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:49408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:51298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:49702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:55788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36 DP0 TP0] Decode batch, #running-req: 26, #token: 6350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 655.28, #queue-req: 0, 
[2025-10-28 11:27:36 DP4 TP4] Decode batch, #running-req: 14, #token: 4077, token usage: 0.01, cuda graph: True, gen throughput (token/s): 438.26, #queue-req: 0, 
[2025-10-28 11:27:36 DP5 TP5] Decode batch, #running-req: 17, #token: 4678, token usage: 0.01, cuda graph: True, gen throughput (token/s): 547.69, #queue-req: 0, 
[2025-10-28 11:27:36 DP7 TP7] Decode batch, #running-req: 23, #token: 5438, token usage: 0.01, cuda graph: True, gen throughput (token/s): 571.30, #queue-req: 0, 
[2025-10-28 11:27:36 DP2 TP2] Decode batch, #running-req: 16, #token: 3943, token usage: 0.01, cuda graph: True, gen throughput (token/s): 575.90, #queue-req: 0, 
[2025-10-28 11:27:36 DP3 TP3] Decode batch, #running-req: 18, #token: 4524, token usage: 0.01, cuda graph: True, gen throughput (token/s): 524.93, #queue-req: 0, 
[2025-10-28 11:27:36 DP1 TP1] Decode batch, #running-req: 19, #token: 4886, token usage: 0.01, cuda graph: True, gen throughput (token/s): 484.27, #queue-req: 0, 
[2025-10-28 11:27:36] INFO:     127.0.0.1:52178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36 DP6 TP6] Decode batch, #running-req: 22, #token: 4864, token usage: 0.01, cuda graph: True, gen throughput (token/s): 509.04, #queue-req: 0, 
[2025-10-28 11:27:36] INFO:     127.0.0.1:52632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:51780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:48974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:47402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:52280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:52418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:55040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:48438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:51562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:47830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:47334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:46006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:54848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:48018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:46908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:48148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:49760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:49026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:46546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:46008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:47248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:51092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:49528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:49106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:55236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:52088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:53220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:53132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:49634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:48642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:50338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:55050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:49146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:48972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:36] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:52974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:51322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:48220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:46064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:51408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:46448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:52130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:57226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:46488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:52436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:53824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:57756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:57260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:46892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:47310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:48228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:47864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:48700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:48002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:56166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:52586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:37] INFO:     127.0.0.1:51176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:47374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:56510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:53884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:47410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:55980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38 DP1 TP1] Decode batch, #running-req: 5, #token: 1782, token usage: 0.00, cuda graph: True, gen throughput (token/s): 213.89, #queue-req: 0, 
[2025-10-28 11:27:38 DP7 TP7] Decode batch, #running-req: 9, #token: 3036, token usage: 0.00, cuda graph: True, gen throughput (token/s): 309.75, #queue-req: 0, 
[2025-10-28 11:27:38 DP4 TP4] Decode batch, #running-req: 9, #token: 3245, token usage: 0.00, cuda graph: True, gen throughput (token/s): 245.44, #queue-req: 0, 
[2025-10-28 11:27:38 DP5 TP5] Decode batch, #running-req: 6, #token: 2378, token usage: 0.00, cuda graph: True, gen throughput (token/s): 240.72, #queue-req: 0, 
[2025-10-28 11:27:38 DP2 TP2] Decode batch, #running-req: 4, #token: 1782, token usage: 0.00, cuda graph: True, gen throughput (token/s): 162.25, #queue-req: 0, 
[2025-10-28 11:27:38 DP0 TP0] Decode batch, #running-req: 5, #token: 2049, token usage: 0.00, cuda graph: True, gen throughput (token/s): 291.78, #queue-req: 0, 
[2025-10-28 11:27:38 DP3 TP3] Decode batch, #running-req: 5, #token: 2059, token usage: 0.00, cuda graph: True, gen throughput (token/s): 211.24, #queue-req: 0, 
[2025-10-28 11:27:38 DP6 TP6] Decode batch, #running-req: 9, #token: 3225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 268.18, #queue-req: 0, 
[2025-10-28 11:27:38] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:49620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:51696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:54388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:48562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:47272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:50886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:46728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:49058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:50396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:53048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:46848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:48628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:55542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:38] INFO:     127.0.0.1:52302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:56034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:53232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:47234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39 DP4 TP4] Decode batch, #running-req: 4, #token: 1950, token usage: 0.00, cuda graph: True, gen throughput (token/s): 157.32, #queue-req: 0, 
[2025-10-28 11:27:39 DP0 TP0] Decode batch, #running-req: 1, #token: 988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 66.14, #queue-req: 0, 
[2025-10-28 11:27:39 DP1 TP1] Decode batch, #running-req: 1, #token: 973, token usage: 0.00, cuda graph: True, gen throughput (token/s): 64.85, #queue-req: 0, 
[2025-10-28 11:27:39 DP3 TP3] Decode batch, #running-req: 1, #token: 979, token usage: 0.00, cuda graph: True, gen throughput (token/s): 57.15, #queue-req: 0, 
[2025-10-28 11:27:39 DP7 TP7] Decode batch, #running-req: 1, #token: 1029, token usage: 0.00, cuda graph: True, gen throughput (token/s): 141.25, #queue-req: 0, 
[2025-10-28 11:27:39 DP5 TP5] Decode batch, #running-req: 4, #token: 2007, token usage: 0.00, cuda graph: True, gen throughput (token/s): 130.98, #queue-req: 0, 
[2025-10-28 11:27:39 DP6 TP6] Decode batch, #running-req: 2, #token: 1240, token usage: 0.00, cuda graph: True, gen throughput (token/s): 132.28, #queue-req: 0, 
[2025-10-28 11:27:39 DP2 TP2] Decode batch, #running-req: 3, #token: 1662, token usage: 0.00, cuda graph: True, gen throughput (token/s): 80.24, #queue-req: 0, 
[2025-10-28 11:27:39] INFO:     127.0.0.1:53380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:53274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:39] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:40] INFO:     127.0.0.1:48656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:40] INFO:     127.0.0.1:47510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:40] INFO:     127.0.0.1:55160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:40] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:40] INFO:     127.0.0.1:55114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:40] INFO:     127.0.0.1:56704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:40] INFO:     127.0.0.1:54336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:40] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:40] INFO:     127.0.0.1:49922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:40] INFO:     127.0.0.1:46762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:40 DP4 TP4] Decode batch, #running-req: 2, #token: 1030, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.18, #queue-req: 0, 
[2025-10-28 11:27:40 DP2 TP2] Decode batch, #running-req: 1, #token: 1030, token usage: 0.00, cuda graph: True, gen throughput (token/s): 54.42, #queue-req: 0, 
[2025-10-28 11:27:40 DP5 TP5] Decode batch, #running-req: 2, #token: 1436, token usage: 0.00, cuda graph: True, gen throughput (token/s): 92.49, #queue-req: 0, 
[2025-10-28 11:27:41] INFO:     127.0.0.1:52918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:41] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:41] INFO:     127.0.0.1:55508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:41] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:54] INFO:     127.0.0.1:38162 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-28 11:27:54 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP4 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP7 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP5 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP6 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP1 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP2 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP3 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP0 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54] INFO:     127.0.0.1:38168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:27:54 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 65, #cached-token: 669, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP0 TP0] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP4 TP4] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP5 TP5] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP7 TP7] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP6 TP6] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP3 TP3] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP2 TP2] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP1 TP1] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 34, #cached-token: 1403, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:27:54 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1443, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:27:54 DP2 TP2] Prefill batch, #new-seq: 3, #new-token: 29, #cached-token: 2117, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:27:54 DP0 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1494, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:27:54 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1513, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:27:54 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1436, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:27:54 DP3 TP3] Prefill batch, #new-seq: 2, #new-token: 49, #cached-token: 1400, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:27:54 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 87, #cached-token: 3527, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP0 TP0] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP1 TP1] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP7 TP7] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP5 TP5] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP6 TP6] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP3 TP3] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP4 TP4] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP2 TP2] [fused_moe] using default for (207, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:54 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 441, #cached-token: 6761, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:27:54 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 494, #cached-token: 6693, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:27:54 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 547, #cached-token: 6723, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:27:54 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 599, #cached-token: 6754, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:27:54 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 289, #cached-token: 4792, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[2025-10-28 11:27:54 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 373, #cached-token: 6822, token usage: 0.00, #running-req: 3, #queue-req: 0, 
[2025-10-28 11:27:54 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 618, #cached-token: 6828, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:27:54 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 512, #cached-token: 6775, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:27:54 DP4 TP4] Prefill batch, #new-seq: 16, #new-token: 873, #cached-token: 10770, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:27:54 DP7 TP7] Prefill batch, #new-seq: 15, #new-token: 895, #cached-token: 10199, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:27:54 DP6 TP6] Prefill batch, #new-seq: 15, #new-token: 778, #cached-token: 10281, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:27:54 DP3 TP3] Prefill batch, #new-seq: 16, #new-token: 758, #cached-token: 10899, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:27:54 DP2 TP2] Prefill batch, #new-seq: 15, #new-token: 626, #cached-token: 10420, token usage: 0.00, #running-req: 13, #queue-req: 0, 
[2025-10-28 11:27:54 DP1 TP1] Prefill batch, #new-seq: 15, #new-token: 694, #cached-token: 10196, token usage: 0.00, #running-req: 13, #queue-req: 0, 
[2025-10-28 11:27:54 DP5 TP5] Prefill batch, #new-seq: 16, #new-token: 820, #cached-token: 10860, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:27:54 DP0 TP0] Prefill batch, #new-seq: 15, #new-token: 528, #cached-token: 10395, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:27:54 DP4 TP4] Prefill batch, #new-seq: 16, #new-token: 995, #cached-token: 10771, token usage: 0.00, #running-req: 28, #queue-req: 0, 
[2025-10-28 11:27:54 DP2 TP2] Prefill batch, #new-seq: 17, #new-token: 782, #cached-token: 11520, token usage: 0.00, #running-req: 28, #queue-req: 0, 
[2025-10-28 11:27:54 DP1 TP1] Prefill batch, #new-seq: 17, #new-token: 787, #cached-token: 11487, token usage: 0.00, #running-req: 28, #queue-req: 0, 
[2025-10-28 11:27:54 DP0 TP0] Prefill batch, #new-seq: 17, #new-token: 702, #cached-token: 11549, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-28 11:27:54 DP6 TP6] Prefill batch, #new-seq: 17, #new-token: 774, #cached-token: 11614, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-28 11:27:54 DP3 TP3] Prefill batch, #new-seq: 16, #new-token: 782, #cached-token: 10796, token usage: 0.00, #running-req: 28, #queue-req: 0, 
[2025-10-28 11:27:54 DP7 TP7] Prefill batch, #new-seq: 17, #new-token: 957, #cached-token: 11465, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-28 11:27:54 DP5 TP5] Prefill batch, #new-seq: 16, #new-token: 1015, #cached-token: 10761, token usage: 0.00, #running-req: 28, #queue-req: 0, 
[2025-10-28 11:27:55 DP4 TP4] Prefill batch, #new-seq: 31, #new-token: 1166, #cached-token: 21294, token usage: 0.01, #running-req: 44, #queue-req: 0, 
[2025-10-28 11:27:55 DP5 TP5] Prefill batch, #new-seq: 31, #new-token: 1392, #cached-token: 21144, token usage: 0.01, #running-req: 44, #queue-req: 0, 
[2025-10-28 11:27:55 DP1 TP1] Prefill batch, #new-seq: 31, #new-token: 1024, #cached-token: 21578, token usage: 0.01, #running-req: 45, #queue-req: 0, 
[2025-10-28 11:27:55 DP7 TP7] Prefill batch, #new-seq: 31, #new-token: 947, #cached-token: 21507, token usage: 0.01, #running-req: 44, #queue-req: 0, 
[2025-10-28 11:27:55 DP3 TP3] Prefill batch, #new-seq: 32, #new-token: 776, #cached-token: 22512, token usage: 0.01, #running-req: 44, #queue-req: 0, 
[2025-10-28 11:27:55 DP6 TP6] Prefill batch, #new-seq: 31, #new-token: 1203, #cached-token: 21301, token usage: 0.01, #running-req: 44, #queue-req: 0, 
[2025-10-28 11:27:55 DP2 TP2] Prefill batch, #new-seq: 31, #new-token: 679, #cached-token: 22034, token usage: 0.01, #running-req: 45, #queue-req: 0, 
[2025-10-28 11:27:55 DP0 TP0] Prefill batch, #new-seq: 31, #new-token: 989, #cached-token: 21855, token usage: 0.01, #running-req: 44, #queue-req: 0, 
[2025-10-28 11:27:55 DP4 TP4] Prefill batch, #new-seq: 26, #new-token: 691, #cached-token: 18305, token usage: 0.01, #running-req: 75, #queue-req: 0, 
[2025-10-28 11:27:55 DP2 TP2] Prefill batch, #new-seq: 25, #new-token: 516, #cached-token: 17692, token usage: 0.01, #running-req: 76, #queue-req: 0, 
[2025-10-28 11:27:55 DP6 TP6] Prefill batch, #new-seq: 26, #new-token: 646, #cached-token: 18292, token usage: 0.01, #running-req: 75, #queue-req: 0, 
[2025-10-28 11:27:55 DP7 TP7] Prefill batch, #new-seq: 26, #new-token: 875, #cached-token: 18042, token usage: 0.01, #running-req: 75, #queue-req: 0, 
[2025-10-28 11:27:55 DP0 TP0] Prefill batch, #new-seq: 26, #new-token: 884, #cached-token: 17947, token usage: 0.01, #running-req: 75, #queue-req: 0, 
[2025-10-28 11:27:55 DP3 TP3] Prefill batch, #new-seq: 25, #new-token: 424, #cached-token: 17835, token usage: 0.01, #running-req: 76, #queue-req: 0, 
[2025-10-28 11:27:55 DP5 TP5] Prefill batch, #new-seq: 26, #new-token: 633, #cached-token: 18403, token usage: 0.01, #running-req: 75, #queue-req: 0, 
[2025-10-28 11:27:55 DP1 TP1] Prefill batch, #new-seq: 26, #new-token: 774, #cached-token: 18204, token usage: 0.01, #running-req: 76, #queue-req: 0, 
[2025-10-28 11:27:56 DP4 TP4] Prefill batch, #new-seq: 31, #new-token: 1036, #cached-token: 21419, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-28 11:27:56 DP2 TP2] Prefill batch, #new-seq: 32, #new-token: 787, #cached-token: 22492, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-28 11:27:56 DP6 TP6] Prefill batch, #new-seq: 31, #new-token: 766, #cached-token: 21928, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-28 11:27:56 DP7 TP7] Prefill batch, #new-seq: 31, #new-token: 1343, #cached-token: 21679, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-28 11:27:56 DP5 TP5] Prefill batch, #new-seq: 31, #new-token: 1126, #cached-token: 21618, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-28 11:27:56 DP0 TP0] Prefill batch, #new-seq: 31, #new-token: 1385, #cached-token: 21295, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-28 11:27:56 DP1 TP1] Prefill batch, #new-seq: 31, #new-token: 1221, #cached-token: 21511, token usage: 0.01, #running-req: 102, #queue-req: 0, 
[2025-10-28 11:27:56 DP3 TP3] Prefill batch, #new-seq: 31, #new-token: 861, #cached-token: 21721, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-28 11:27:56 DP4 TP4] Prefill batch, #new-seq: 12, #new-token: 218, #cached-token: 8411, token usage: 0.01, #running-req: 132, #queue-req: 0, 
[2025-10-28 11:27:56 DP6 TP6] Prefill batch, #new-seq: 12, #new-token: 287, #cached-token: 8609, token usage: 0.01, #running-req: 132, #queue-req: 0, 
[2025-10-28 11:27:56 DP2 TP2] Prefill batch, #new-seq: 11, #new-token: 306, #cached-token: 7649, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-28 11:27:56 DP7 TP7] Prefill batch, #new-seq: 12, #new-token: 385, #cached-token: 8451, token usage: 0.01, #running-req: 132, #queue-req: 0, 
[2025-10-28 11:27:56 DP5 TP5] Prefill batch, #new-seq: 12, #new-token: 247, #cached-token: 8448, token usage: 0.01, #running-req: 132, #queue-req: 0, 
[2025-10-28 11:27:56 DP0 TP0] Prefill batch, #new-seq: 12, #new-token: 420, #cached-token: 8357, token usage: 0.01, #running-req: 132, #queue-req: 0, 
[2025-10-28 11:27:56 DP1 TP1] Prefill batch, #new-seq: 12, #new-token: 309, #cached-token: 8324, token usage: 0.01, #running-req: 133, #queue-req: 0, 
[2025-10-28 11:27:56 DP3 TP3] Prefill batch, #new-seq: 12, #new-token: 63, #cached-token: 8660, token usage: 0.01, #running-req: 132, #queue-req: 0, 
[2025-10-28 11:27:57 DP4 TP4] Prefill batch, #new-seq: 20, #new-token: 481, #cached-token: 13915, token usage: 0.01, #running-req: 144, #queue-req: 0, 
[2025-10-28 11:27:57 DP5 TP5] Prefill batch, #new-seq: 20, #new-token: 620, #cached-token: 13947, token usage: 0.02, #running-req: 144, #queue-req: 0, 
[2025-10-28 11:27:57 DP6 TP6] Prefill batch, #new-seq: 20, #new-token: 623, #cached-token: 13998, token usage: 0.01, #running-req: 144, #queue-req: 0, 
[2025-10-28 11:27:57 DP7 TP7] Prefill batch, #new-seq: 20, #new-token: 437, #cached-token: 14332, token usage: 0.02, #running-req: 144, #queue-req: 0, 
[2025-10-28 11:27:57 DP3 TP3] Prefill batch, #new-seq: 20, #new-token: 638, #cached-token: 14016, token usage: 0.01, #running-req: 144, #queue-req: 0, 
[2025-10-28 11:27:57 DP1 TP1] Prefill batch, #new-seq: 20, #new-token: 932, #cached-token: 13836, token usage: 0.01, #running-req: 145, #queue-req: 0, 
[2025-10-28 11:27:57 DP2 TP2] Prefill batch, #new-seq: 21, #new-token: 767, #cached-token: 14649, token usage: 0.01, #running-req: 144, #queue-req: 0, 
[2025-10-28 11:27:57 DP0 TP0] Prefill batch, #new-seq: 20, #new-token: 678, #cached-token: 14028, token usage: 0.02, #running-req: 144, #queue-req: 0, 
[2025-10-28 11:27:57 DP4 TP4] Prefill batch, #new-seq: 1, #new-token: 59, #cached-token: 670, token usage: 0.01, #running-req: 164, #queue-req: 0, 
[2025-10-28 11:27:57 DP6 TP6] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 706, token usage: 0.02, #running-req: 164, #queue-req: 0, 
[2025-10-28 11:27:57 DP7 TP7] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 716, token usage: 0.02, #running-req: 164, #queue-req: 0, 
[2025-10-28 11:27:57 DP5 TP5] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 741, token usage: 0.02, #running-req: 164, #queue-req: 0, 
[2025-10-28 11:27:57 DP3 TP3] Prefill batch, #new-seq: 1, #new-token: 57, #cached-token: 671, token usage: 0.02, #running-req: 164, #queue-req: 0, 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:57 DP6 TP6] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:57 DP7 TP7] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:57 DP4 TP4] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:57 DP5 TP5] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:57 DP3 TP3] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:57 DP0 TP0] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:57 DP1 TP1] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:57 DP2 TP2] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:27:58 DP1 TP1] Decode batch, #running-req: 165, #token: 12010, token usage: 0.02, cuda graph: True, gen throughput (token/s): 72.25, #queue-req: 0, 
[2025-10-28 11:27:58 DP7 TP7] Decode batch, #running-req: 165, #token: 13052, token usage: 0.02, cuda graph: True, gen throughput (token/s): 96.12, #queue-req: 0, 
[2025-10-28 11:27:59 DP6 TP6] Decode batch, #running-req: 165, #token: 13231, token usage: 0.02, cuda graph: True, gen throughput (token/s): 120.88, #queue-req: 0, 
[2025-10-28 11:27:59 DP4 TP4] Decode batch, #running-req: 165, #token: 12907, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.97, #queue-req: 0, 
[2025-10-28 11:28:00 DP5 TP5] Decode batch, #running-req: 165, #token: 14828, token usage: 0.02, cuda graph: True, gen throughput (token/s): 197.72, #queue-req: 0, 
[2025-10-28 11:28:00 DP3 TP3] Decode batch, #running-req: 165, #token: 14370, token usage: 0.02, cuda graph: True, gen throughput (token/s): 184.98, #queue-req: 0, 
[2025-10-28 11:28:00] INFO:     127.0.0.1:41140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:01] INFO:     127.0.0.1:38562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:01] INFO:     127.0.0.1:41200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:01 DP2 TP2] Decode batch, #running-req: 165, #token: 16582, token usage: 0.02, cuda graph: True, gen throughput (token/s): 286.40, #queue-req: 0, 
[2025-10-28 11:28:01 DP0 TP0] Decode batch, #running-req: 162, #token: 16866, token usage: 0.02, cuda graph: True, gen throughput (token/s): 273.33, #queue-req: 0, 
[2025-10-28 11:28:01] INFO:     127.0.0.1:42930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:41820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:38968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:41758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:41772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:42452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:42902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:41018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:38212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:39344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:43052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:39672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:38462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:40194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:43860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:41166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:41210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:38782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:42564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:43110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:42484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:43884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:39830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:43184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:38122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:38758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:43088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:42716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:40320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:39060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:39126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:40090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:42668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:39276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:39960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:40990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:41516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:43674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:39822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:39978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:39320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:39732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:41412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:40668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:41958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:41946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:02] INFO:     127.0.0.1:42972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:44134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:40698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:44044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:38992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03 DP1 TP1] Decode batch, #running-req: 156, #token: 17384, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1376.05, #queue-req: 0, 
[2025-10-28 11:28:03] INFO:     127.0.0.1:40798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:38180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:38648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:38882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:40040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:38982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:40514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:44016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:43264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:37564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:38964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:40182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:43762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:44096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:38464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:38596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:43630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:44370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:40728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:40470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03 DP7 TP7] Decode batch, #running-req: 162, #token: 19386, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1377.67, #queue-req: 0, 
[2025-10-28 11:28:03] INFO:     127.0.0.1:41566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:40150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:40370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:37670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:40840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:43958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:40416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:43896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:44028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:38432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:40524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:40784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:42712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:43378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03 DP6 TP6] Decode batch, #running-req: 147, #token: 17955, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1352.22, #queue-req: 0, 
[2025-10-28 11:28:03] INFO:     127.0.0.1:38084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:44268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:39238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:41696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:38196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:37728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:03] INFO:     127.0.0.1:38294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:41366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:41554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:42046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:38860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:40870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:43696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:42526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:37578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:40764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:40140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:40088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:42632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:41238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:44138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:40196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:40982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:40600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04 DP4 TP4] Decode batch, #running-req: 139, #token: 16774, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1329.01, #queue-req: 0, 
[2025-10-28 11:28:04] INFO:     127.0.0.1:38016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:43080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:40478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:40462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:38458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:41518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:37704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:42738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:44208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:38908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:42556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:42578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:40316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:37842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:40292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:42954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:41858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:37896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:42456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:38622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:43222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:43018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:43146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:38666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:43236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:41836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:38330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:40306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:41250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:42270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:43706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:43784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:38662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:38210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:44390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:43158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:40690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:43450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:38786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:37602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:42836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:38336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:39322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:42764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:42944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:04] INFO:     127.0.0.1:42718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05 DP5 TP5] Decode batch, #running-req: 140, #token: 18399, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1341.43, #queue-req: 0, 
[2025-10-28 11:28:05] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05 DP3 TP3] Decode batch, #running-req: 142, #token: 18081, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1324.45, #queue-req: 0, 
[2025-10-28 11:28:05] INFO:     127.0.0.1:44144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:37552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:44104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:44272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:37944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:37930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:43210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:44264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:38762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:39442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:44224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:37744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:41712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:40328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:05] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:37640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:37864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06 DP2 TP2] Decode batch, #running-req: 110, #token: 15890, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1285.22, #queue-req: 0, 
[2025-10-28 11:28:06] INFO:     127.0.0.1:39886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:44162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06 DP0 TP0] Decode batch, #running-req: 115, #token: 17093, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1254.85, #queue-req: 0, 
[2025-10-28 11:28:06] INFO:     127.0.0.1:38240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:37852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:44242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:44446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:37702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:43068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:44332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:37592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:39444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:41830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:40202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:44018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:38316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:06] INFO:     127.0.0.1:42600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07 DP1 TP1] Decode batch, #running-req: 91, #token: 14571, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1188.63, #queue-req: 0, 
[2025-10-28 11:28:07] INFO:     127.0.0.1:43142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:37874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:37766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:37914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07 DP7 TP7] Decode batch, #running-req: 92, #token: 15002, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1280.80, #queue-req: 0, 
[2025-10-28 11:28:07] INFO:     127.0.0.1:37750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:43460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:40074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:37778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:38222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:41222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:37804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:42740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:37674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:44198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:07] INFO:     127.0.0.1:39314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08 DP6 TP6] Decode batch, #running-req: 84, #token: 14175, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1101.34, #queue-req: 0, 
[2025-10-28 11:28:08] INFO:     127.0.0.1:41234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:44222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:44160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:44126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08 DP4 TP4] Decode batch, #running-req: 72, #token: 12049, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1049.10, #queue-req: 0, 
[2025-10-28 11:28:08] INFO:     127.0.0.1:38514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:44082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:37714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:37904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:43832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:37832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08 DP3 TP3] Decode batch, #running-req: 76, #token: 13218, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1102.00, #queue-req: 0, 
[2025-10-28 11:28:08] INFO:     127.0.0.1:43474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08 DP5 TP5] Decode batch, #running-req: 83, #token: 14235, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1181.22, #queue-req: 0, 
[2025-10-28 11:28:08] INFO:     127.0.0.1:42726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:42924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:37980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:37550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:39544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:44072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:40952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:38994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:08] INFO:     127.0.0.1:41824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:44358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:37696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:44114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:44274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:37966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:44058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:44406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:44046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:37822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09 DP2 TP2] Decode batch, #running-req: 44, #token: 8201, token usage: 0.01, cuda graph: True, gen throughput (token/s): 815.68, #queue-req: 0, 
[2025-10-28 11:28:09] INFO:     127.0.0.1:37956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:44008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09 DP0 TP0] Decode batch, #running-req: 44, #token: 9015, token usage: 0.01, cuda graph: True, gen throughput (token/s): 873.95, #queue-req: 0, 
[2025-10-28 11:28:09] INFO:     127.0.0.1:42168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:38418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:41446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:40872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:43680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:09] INFO:     127.0.0.1:39538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:38460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:43284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:44172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:38636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:38980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:37656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:43268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:44226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:44248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:43046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:44148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:44422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:38276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:43790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:38948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:44432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:38976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:44048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:38658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10 DP1 TP1] Decode batch, #running-req: 38, #token: 7526, token usage: 0.01, cuda graph: True, gen throughput (token/s): 731.05, #queue-req: 0, 
[2025-10-28 11:28:10] INFO:     127.0.0.1:41008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:43960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:37882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:38738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:38258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:44288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:38924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:38062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:38202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:43932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10 DP7 TP7] Decode batch, #running-req: 36, #token: 8465, token usage: 0.01, cuda graph: True, gen throughput (token/s): 768.08, #queue-req: 0, 
[2025-10-28 11:28:10] INFO:     127.0.0.1:38224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:43312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:42430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:41386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:43584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:39532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:40058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:10] INFO:     127.0.0.1:38772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:43392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:43694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11 DP6 TP6] Decode batch, #running-req: 33, #token: 7413, token usage: 0.01, cuda graph: True, gen throughput (token/s): 699.81, #queue-req: 0, 
[2025-10-28 11:28:11] INFO:     127.0.0.1:42134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:44318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:43130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11 DP4 TP4] Decode batch, #running-req: 22, #token: 4950, token usage: 0.01, cuda graph: True, gen throughput (token/s): 609.43, #queue-req: 0, 
[2025-10-28 11:28:11] INFO:     127.0.0.1:41942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:37814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:43406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:43324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:43220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11 DP3 TP3] Decode batch, #running-req: 29, #token: 6427, token usage: 0.01, cuda graph: True, gen throughput (token/s): 736.02, #queue-req: 0, 
[2025-10-28 11:28:11 DP5 TP5] Decode batch, #running-req: 27, #token: 6409, token usage: 0.01, cuda graph: True, gen throughput (token/s): 693.93, #queue-req: 0, 
[2025-10-28 11:28:11] INFO:     127.0.0.1:43564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:44184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:43422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:43538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:40704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:39838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:42174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:11] INFO:     127.0.0.1:41010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:44380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:43112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:43760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:41922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:37610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:41122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:40322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:42348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:44348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:40530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:37754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:40114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:41022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:40496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12 DP2 TP2] Decode batch, #running-req: 14, #token: 4036, token usage: 0.01, cuda graph: True, gen throughput (token/s): 393.72, #queue-req: 0, 
[2025-10-28 11:28:12] INFO:     127.0.0.1:43786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12 DP0 TP0] Decode batch, #running-req: 17, #token: 4593, token usage: 0.01, cuda graph: True, gen throughput (token/s): 478.66, #queue-req: 0, 
[2025-10-28 11:28:12] INFO:     127.0.0.1:42240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:41466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:40710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:41258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:43020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:43402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:40078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:38950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:42754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:41784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:41624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:42682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:38532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:42274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:38742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:41874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:41188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:43332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:40744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:40772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:40304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:44314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:38608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:43988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:40388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:42108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:38548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:39850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12 DP1 TP1] Decode batch, #running-req: 10, #token: 3115, token usage: 0.00, cuda graph: True, gen throughput (token/s): 375.79, #queue-req: 0, 
[2025-10-28 11:28:12] INFO:     127.0.0.1:40630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:37760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:44056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:41042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:40264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:42476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:43894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:41384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:12] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:43870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:38542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:39564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:42242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:37788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13 DP7 TP7] Decode batch, #running-req: 17, #token: 5185, token usage: 0.01, cuda graph: True, gen throughput (token/s): 448.41, #queue-req: 0, 
[2025-10-28 11:28:13] INFO:     127.0.0.1:40576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:40212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:38752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:43186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:39088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13 DP6 TP6] Decode batch, #running-req: 13, #token: 3993, token usage: 0.01, cuda graph: True, gen throughput (token/s): 398.17, #queue-req: 0, 
[2025-10-28 11:28:13] INFO:     127.0.0.1:42450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:39432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:43022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:41330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:43660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:38790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:39394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:39196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13 DP4 TP4] Decode batch, #running-req: 4, #token: 1746, token usage: 0.00, cuda graph: True, gen throughput (token/s): 232.43, #queue-req: 0, 
[2025-10-28 11:28:13] INFO:     127.0.0.1:43272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:39860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:39494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:39604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:42562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:42400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:38260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13 DP5 TP5] Decode batch, #running-req: 8, #token: 2531, token usage: 0.00, cuda graph: True, gen throughput (token/s): 324.18, #queue-req: 0, 
[2025-10-28 11:28:13 DP3 TP3] Decode batch, #running-req: 10, #token: 3378, token usage: 0.00, cuda graph: True, gen throughput (token/s): 336.59, #queue-req: 0, 
[2025-10-28 11:28:13] INFO:     127.0.0.1:40354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:41392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:38812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:41220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:42128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:38364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:38314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:42662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:39390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:40154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:39240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:39272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:13] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:41848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:43924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:43056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:41442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:43302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:41424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14 DP2 TP2] Decode batch, #running-req: 7, #token: 2408, token usage: 0.00, cuda graph: True, gen throughput (token/s): 198.98, #queue-req: 0, 
[2025-10-28 11:28:14] INFO:     127.0.0.1:40060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14 DP0 TP0] Decode batch, #running-req: 4, #token: 1840, token usage: 0.00, cuda graph: True, gen throughput (token/s): 200.34, #queue-req: 0, 
[2025-10-28 11:28:14] INFO:     127.0.0.1:40980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:42692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:39456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:38070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:39108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:41440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:42796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:41852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:40930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:37988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:42220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14 DP1 TP1] Decode batch, #running-req: 1, #token: 955, token usage: 0.00, cuda graph: True, gen throughput (token/s): 88.56, #queue-req: 0, 
[2025-10-28 11:28:14] INFO:     127.0.0.1:38846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:41938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:41072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:42014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:38624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14 DP7 TP7] Decode batch, #running-req: 7, #token: 2570, token usage: 0.00, cuda graph: True, gen throughput (token/s): 293.22, #queue-req: 0, 
[2025-10-28 11:28:14] INFO:     127.0.0.1:38498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14] INFO:     127.0.0.1:37624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:14 DP6 TP6] Decode batch, #running-req: 6, #token: 2206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 206.01, #queue-req: 0, 
[2025-10-28 11:28:14] INFO:     127.0.0.1:41226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:15 DP4 TP4] Decode batch, #running-req: 4, #token: 1906, token usage: 0.00, cuda graph: True, gen throughput (token/s): 93.84, #queue-req: 0, 
[2025-10-28 11:28:15] INFO:     127.0.0.1:39230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:15] INFO:     127.0.0.1:41510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:15] INFO:     127.0.0.1:41934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:15] INFO:     127.0.0.1:39808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:15 DP5 TP5] Decode batch, #running-req: 3, #token: 1546, token usage: 0.00, cuda graph: True, gen throughput (token/s): 109.97, #queue-req: 0, 
[2025-10-28 11:28:15 DP3 TP3] Decode batch, #running-req: 1, #token: 978, token usage: 0.00, cuda graph: True, gen throughput (token/s): 114.19, #queue-req: 0, 
[2025-10-28 11:28:15] INFO:     127.0.0.1:38638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:15] INFO:     127.0.0.1:38440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:15] INFO:     127.0.0.1:42180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:15] INFO:     127.0.0.1:39668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:15] INFO:     127.0.0.1:41020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:15] INFO:     127.0.0.1:38754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:15] INFO:     127.0.0.1:39730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:15] INFO:     127.0.0.1:39122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:15 DP2 TP2] Decode batch, #running-req: 4, #token: 1950, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.53, #queue-req: 0, 
[2025-10-28 11:28:15] INFO:     127.0.0.1:41978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:16] INFO:     127.0.0.1:41882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:16 DP1 TP1] Decode batch, #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 27.96, #queue-req: 0, 
[2025-10-28 11:28:16] INFO:     127.0.0.1:40458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:16] INFO:     127.0.0.1:39306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:16] INFO:     127.0.0.1:37740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:16 DP7 TP7] Decode batch, #running-req: 2, #token: 1440, token usage: 0.00, cuda graph: True, gen throughput (token/s): 92.57, #queue-req: 0, 
[2025-10-28 11:28:16] INFO:     127.0.0.1:41266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:16 DP6 TP6] Decode batch, #running-req: 1, #token: 964, token usage: 0.00, cuda graph: True, gen throughput (token/s): 76.86, #queue-req: 0, 
[2025-10-28 11:28:16] INFO:     127.0.0.1:41498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:16 DP4 TP4] Decode batch, #running-req: 1, #token: 1007, token usage: 0.00, cuda graph: True, gen throughput (token/s): 83.58, #queue-req: 0, 
[2025-10-28 11:28:16] INFO:     127.0.0.1:40620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:16] INFO:     127.0.0.1:42068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:16] INFO:     127.0.0.1:42200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:16] INFO:     127.0.0.1:39602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:16 DP2 TP2] Decode batch, #running-req: 1, #token: 1034, token usage: 0.00, cuda graph: True, gen throughput (token/s): 72.98, #queue-req: 0, 
[2025-10-28 11:28:16] INFO:     127.0.0.1:38306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:17] INFO:     127.0.0.1:40340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:29] INFO:     127.0.0.1:53952 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-28 11:28:30 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:28:30] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:30 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:28:30 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 60, #cached-token: 1389, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:28:30 DP6 TP6] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 724, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:28:30 DP2 TP2] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1421, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:28:30 DP7 TP7] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 709, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:28:30 DP0 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1494, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:28:30 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1513, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:28:30 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 260, #cached-token: 3448, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-28 11:28:30 DP3 TP3] Prefill batch, #new-seq: 2, #new-token: 64, #cached-token: 1373, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:30 DP0 TP0] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:30 DP1 TP1] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:30 DP7 TP7] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:30 DP5 TP5] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:30 DP6 TP6] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:30 DP3 TP3] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:30 DP4 TP4] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:30 DP2 TP2] [fused_moe] using default for (392, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:30 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 413, #cached-token: 6778, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:28:30 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 283, #cached-token: 4809, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[2025-10-28 11:28:30 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 568, #cached-token: 6804, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:28:30 DP7 TP7] Prefill batch, #new-seq: 11, #new-token: 327, #cached-token: 7690, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-28 11:28:30 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 439, #cached-token: 6755, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:28:30 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 350, #cached-token: 6838, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:28:30 DP2 TP2] Prefill batch, #new-seq: 11, #new-token: 649, #cached-token: 7448, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:28:30 DP6 TP6] Prefill batch, #new-seq: 11, #new-token: 468, #cached-token: 7591, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-28 11:28:30 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 325, #cached-token: 4811, token usage: 0.00, #running-req: 13, #queue-req: 0, 
[2025-10-28 11:28:30 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 255, #cached-token: 5635, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:28:30 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 420, #cached-token: 5356, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:28:30 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 385, #cached-token: 5395, token usage: 0.00, #running-req: 13, #queue-req: 0, 
[2025-10-28 11:28:30 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 427, #cached-token: 5441, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:28:30 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 337, #cached-token: 5592, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:28:30 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 479, #cached-token: 5356, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:28:30 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 255, #cached-token: 5623, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:28:30 DP4 TP4] Prefill batch, #new-seq: 16, #new-token: 593, #cached-token: 11097, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:28:30 DP2 TP2] Prefill batch, #new-seq: 17, #new-token: 358, #cached-token: 11988, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:28:30 DP6 TP6] Prefill batch, #new-seq: 16, #new-token: 703, #cached-token: 11101, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:28:30 DP7 TP7] Prefill batch, #new-seq: 16, #new-token: 751, #cached-token: 11006, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:28:30 DP0 TP0] Prefill batch, #new-seq: 16, #new-token: 486, #cached-token: 11151, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:28:30 DP1 TP1] Prefill batch, #new-seq: 16, #new-token: 197, #cached-token: 11306, token usage: 0.00, #running-req: 21, #queue-req: 0, 
[2025-10-28 11:28:30 DP3 TP3] Prefill batch, #new-seq: 17, #new-token: 559, #cached-token: 11802, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:28:30 DP5 TP5] Prefill batch, #new-seq: 16, #new-token: 511, #cached-token: 11129, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:28:30 DP1 TP1] Prefill batch, #new-seq: 13, #new-token: 246, #cached-token: 9167, token usage: 0.00, #running-req: 37, #queue-req: 0, 
[2025-10-28 11:28:30 DP5 TP5] Prefill batch, #new-seq: 14, #new-token: 268, #cached-token: 9976, token usage: 0.01, #running-req: 36, #queue-req: 0, 
[2025-10-28 11:28:30 DP0 TP0] Prefill batch, #new-seq: 13, #new-token: 143, #cached-token: 9264, token usage: 0.00, #running-req: 36, #queue-req: 0, 
[2025-10-28 11:28:30 DP4 TP4] Prefill batch, #new-seq: 14, #new-token: 230, #cached-token: 9984, token usage: 0.01, #running-req: 36, #queue-req: 0, 
[2025-10-28 11:28:30 DP2 TP2] Prefill batch, #new-seq: 13, #new-token: 418, #cached-token: 9068, token usage: 0.00, #running-req: 37, #queue-req: 0, 
[2025-10-28 11:28:30 DP6 TP6] Prefill batch, #new-seq: 14, #new-token: 227, #cached-token: 9857, token usage: 0.01, #running-req: 36, #queue-req: 0, 
[2025-10-28 11:28:30 DP3 TP3] Prefill batch, #new-seq: 13, #new-token: 467, #cached-token: 8956, token usage: 0.00, #running-req: 37, #queue-req: 0, 
[2025-10-28 11:28:30 DP7 TP7] Prefill batch, #new-seq: 14, #new-token: 295, #cached-token: 9962, token usage: 0.01, #running-req: 36, #queue-req: 0, 
[2025-10-28 11:28:31 DP5 TP5] Prefill batch, #new-seq: 18, #new-token: 608, #cached-token: 12483, token usage: 0.01, #running-req: 50, #queue-req: 0, 
[2025-10-28 11:28:31 DP7 TP7] Prefill batch, #new-seq: 18, #new-token: 482, #cached-token: 12657, token usage: 0.01, #running-req: 50, #queue-req: 0, 
[2025-10-28 11:28:31 DP3 TP3] Prefill batch, #new-seq: 18, #new-token: 523, #cached-token: 12591, token usage: 0.01, #running-req: 50, #queue-req: 0, 
[2025-10-28 11:28:31 DP1 TP1] Prefill batch, #new-seq: 19, #new-token: 606, #cached-token: 13225, token usage: 0.01, #running-req: 50, #queue-req: 0, 
[2025-10-28 11:28:31 DP4 TP4] Prefill batch, #new-seq: 19, #new-token: 492, #cached-token: 13368, token usage: 0.01, #running-req: 50, #queue-req: 0, 
[2025-10-28 11:28:31 DP0 TP0] Prefill batch, #new-seq: 19, #new-token: 393, #cached-token: 13568, token usage: 0.01, #running-req: 49, #queue-req: 0, 
[2025-10-28 11:28:31 DP6 TP6] Prefill batch, #new-seq: 18, #new-token: 460, #cached-token: 12499, token usage: 0.01, #running-req: 50, #queue-req: 0, 
[2025-10-28 11:28:31 DP2 TP2] Prefill batch, #new-seq: 19, #new-token: 517, #cached-token: 13382, token usage: 0.01, #running-req: 50, #queue-req: 0, 
[2025-10-28 11:28:31 DP1 TP1] Prefill batch, #new-seq: 12, #new-token: 207, #cached-token: 8579, token usage: 0.01, #running-req: 69, #queue-req: 0, 
[2025-10-28 11:28:31 DP5 TP5] Prefill batch, #new-seq: 13, #new-token: 169, #cached-token: 9172, token usage: 0.01, #running-req: 68, #queue-req: 0, 
[2025-10-28 11:28:31 DP3 TP3] Prefill batch, #new-seq: 13, #new-token: 252, #cached-token: 9169, token usage: 0.01, #running-req: 68, #queue-req: 0, 
[2025-10-28 11:28:31 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 119, #cached-token: 9316, token usage: 0.01, #running-req: 68, #queue-req: 0, 
[2025-10-28 11:28:31 DP4 TP4] Prefill batch, #new-seq: 12, #new-token: 374, #cached-token: 8364, token usage: 0.01, #running-req: 69, #queue-req: 0, 
[2025-10-28 11:28:31 DP0 TP0] Prefill batch, #new-seq: 12, #new-token: 112, #cached-token: 8697, token usage: 0.01, #running-req: 68, #queue-req: 0, 
[2025-10-28 11:28:31 DP2 TP2] Prefill batch, #new-seq: 12, #new-token: 269, #cached-token: 8593, token usage: 0.01, #running-req: 69, #queue-req: 0, 
[2025-10-28 11:28:31 DP6 TP6] Prefill batch, #new-seq: 13, #new-token: 321, #cached-token: 9085, token usage: 0.01, #running-req: 68, #queue-req: 0, 
[2025-10-28 11:28:31 DP2 TP2] Prefill batch, #new-seq: 17, #new-token: 519, #cached-token: 11978, token usage: 0.01, #running-req: 81, #queue-req: 0, 
[2025-10-28 11:28:31 DP6 TP6] Prefill batch, #new-seq: 17, #new-token: 324, #cached-token: 12099, token usage: 0.01, #running-req: 81, #queue-req: 0, 
[2025-10-28 11:28:31 DP4 TP4] Prefill batch, #new-seq: 17, #new-token: 279, #cached-token: 12195, token usage: 0.01, #running-req: 81, #queue-req: 0, 
[2025-10-28 11:28:31 DP3 TP3] Prefill batch, #new-seq: 17, #new-token: 375, #cached-token: 12101, token usage: 0.01, #running-req: 81, #queue-req: 0, 
[2025-10-28 11:28:31 DP5 TP5] Prefill batch, #new-seq: 17, #new-token: 292, #cached-token: 12081, token usage: 0.01, #running-req: 81, #queue-req: 0, 
[2025-10-28 11:28:31 DP1 TP1] Prefill batch, #new-seq: 18, #new-token: 383, #cached-token: 12582, token usage: 0.01, #running-req: 81, #queue-req: 0, 
[2025-10-28 11:28:31 DP7 TP7] Prefill batch, #new-seq: 17, #new-token: 135, #cached-token: 12173, token usage: 0.01, #running-req: 81, #queue-req: 0, 
[2025-10-28 11:28:31 DP0 TP0] Prefill batch, #new-seq: 18, #new-token: 519, #cached-token: 12660, token usage: 0.01, #running-req: 80, #queue-req: 0, 
[2025-10-28 11:28:31 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 54, #cached-token: 7342, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-28 11:28:31 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 223, #cached-token: 7051, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-28 11:28:31 DP2 TP2] Prefill batch, #new-seq: 11, #new-token: 101, #cached-token: 7882, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-28 11:28:31 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 151, #cached-token: 7118, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-28 11:28:31 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 74, #cached-token: 7176, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-28 11:28:31 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 76, #cached-token: 7355, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-28 11:28:31 DP3 TP3] Prefill batch, #new-seq: 11, #new-token: 305, #cached-token: 7706, token usage: 0.01, #running-req: 98, #queue-req: 0, 
[2025-10-28 11:28:31 DP1 TP1] Prefill batch, #new-seq: 10, #new-token: 65, #cached-token: 7195, token usage: 0.01, #running-req: 99, #queue-req: 0, 
[2025-10-28 11:28:32 DP5 TP5] Prefill batch, #new-seq: 12, #new-token: 170, #cached-token: 8688, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-28 11:28:32 DP1 TP1] Prefill batch, #new-seq: 12, #new-token: 257, #cached-token: 8631, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-28 11:28:32 DP3 TP3] Prefill batch, #new-seq: 12, #new-token: 214, #cached-token: 8456, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-28 11:28:32 DP7 TP7] Prefill batch, #new-seq: 12, #new-token: 132, #cached-token: 8625, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-28 11:28:32 DP0 TP0] Prefill batch, #new-seq: 12, #new-token: 306, #cached-token: 8452, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-28 11:28:32 DP4 TP4] Prefill batch, #new-seq: 13, #new-token: 239, #cached-token: 9221, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-28 11:28:32 DP6 TP6] Prefill batch, #new-seq: 12, #new-token: 218, #cached-token: 8499, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-28 11:28:32 DP2 TP2] Prefill batch, #new-seq: 12, #new-token: 116, #cached-token: 8592, token usage: 0.01, #running-req: 109, #queue-req: 0, 
[2025-10-28 11:28:32 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 404, #cached-token: 6126, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-28 11:28:32 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 571, #cached-token: 6124, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-28 11:28:32 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 407, #cached-token: 6920, token usage: 0.01, #running-req: 120, #queue-req: 0, 
[2025-10-28 11:28:32 DP3 TP3] Prefill batch, #new-seq: 9, #new-token: 329, #cached-token: 6132, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-28 11:28:32 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 469, #cached-token: 6878, token usage: 0.01, #running-req: 120, #queue-req: 0, 
[2025-10-28 11:28:32 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 443, #cached-token: 6970, token usage: 0.01, #running-req: 120, #queue-req: 0, 
[2025-10-28 11:28:32 DP1 TP1] Prefill batch, #new-seq: 10, #new-token: 334, #cached-token: 7053, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-28 11:28:32 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 234, #cached-token: 7029, token usage: 0.01, #running-req: 120, #queue-req: 0, 
[2025-10-28 11:28:32 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 359, #cached-token: 6164, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-28 11:28:32 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 466, #cached-token: 6235, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-28 11:28:32 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 283, #cached-token: 6254, token usage: 0.01, #running-req: 131, #queue-req: 0, 
[2025-10-28 11:28:32 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 328, #cached-token: 6907, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-28 11:28:32 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 217, #cached-token: 6568, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-28 11:28:32 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 311, #cached-token: 6287, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-28 11:28:32 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 193, #cached-token: 7052, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-28 11:28:32 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 144, #cached-token: 7074, token usage: 0.01, #running-req: 130, #queue-req: 0, 
[2025-10-28 11:28:32 DP2 TP2] Prefill batch, #new-seq: 16, #new-token: 310, #cached-token: 11460, token usage: 0.01, #running-req: 140, #queue-req: 0, 
[2025-10-28 11:28:32 DP6 TP6] Prefill batch, #new-seq: 15, #new-token: 358, #cached-token: 10526, token usage: 0.01, #running-req: 139, #queue-req: 0, 
[2025-10-28 11:28:32 DP4 TP4] Prefill batch, #new-seq: 15, #new-token: 78, #cached-token: 10848, token usage: 0.01, #running-req: 140, #queue-req: 0, 
[2025-10-28 11:28:32 DP3 TP3] Prefill batch, #new-seq: 15, #new-token: 151, #cached-token: 10751, token usage: 0.01, #running-req: 140, #queue-req: 0, 
[2025-10-28 11:28:32 DP1 TP1] Prefill batch, #new-seq: 16, #new-token: 426, #cached-token: 11260, token usage: 0.01, #running-req: 140, #queue-req: 0, 
[2025-10-28 11:28:32 DP0 TP0] Prefill batch, #new-seq: 16, #new-token: 266, #cached-token: 11525, token usage: 0.01, #running-req: 139, #queue-req: 0, 
[2025-10-28 11:28:32 DP5 TP5] Prefill batch, #new-seq: 15, #new-token: 273, #cached-token: 10595, token usage: 0.01, #running-req: 139, #queue-req: 0, 
[2025-10-28 11:28:32 DP7 TP7] Prefill batch, #new-seq: 15, #new-token: 285, #cached-token: 10709, token usage: 0.02, #running-req: 139, #queue-req: 0, 
[2025-10-28 11:28:32 DP4 TP4] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 722, token usage: 0.01, #running-req: 155, #queue-req: 0, 
[2025-10-28 11:28:32 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1439, token usage: 0.01, #running-req: 154, #queue-req: 0, 
[2025-10-28 11:28:32 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1487, token usage: 0.01, #running-req: 154, #queue-req: 0, 
[2025-10-28 11:28:32 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1448, token usage: 0.02, #running-req: 154, #queue-req: 0, 
[2025-10-28 11:28:32 DP3 TP3] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 704, token usage: 0.01, #running-req: 155, #queue-req: 0, 
[aiter] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:32 DP5 TP5] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:32 DP6 TP6] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:32 DP7 TP7] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:32 DP4 TP4] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:32 DP3 TP3] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:32 DP0 TP0] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:32 DP1 TP1] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:32 DP2 TP2] [fused_moe] using default for (475, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:33 DP6 TP6] Prefill batch, #new-seq: 6, #new-token: 182, #cached-token: 4149, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[2025-10-28 11:28:33 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 53, #cached-token: 4957, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[2025-10-28 11:28:33 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 378, #cached-token: 4792, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[2025-10-28 11:28:33 DP7 TP7] Prefill batch, #new-seq: 6, #new-token: 123, #cached-token: 4298, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[2025-10-28 11:28:33 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 381, #cached-token: 4824, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[2025-10-28 11:28:33 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 80, #cached-token: 5191, token usage: 0.02, #running-req: 155, #queue-req: 0, 
[2025-10-28 11:28:33 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 458, #cached-token: 4730, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[2025-10-28 11:28:33 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 106, #cached-token: 4960, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[2025-10-28 11:28:33 DP7 TP7] Decode batch, #running-req: 156, #token: 11294, token usage: 0.02, cuda graph: True, gen throughput (token/s): 21.30, #queue-req: 0, 
[2025-10-28 11:28:33 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1437, token usage: 0.02, #running-req: 163, #queue-req: 0, 
[2025-10-28 11:28:33 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1443, token usage: 0.02, #running-req: 163, #queue-req: 0, 
[2025-10-28 11:28:33 DP2 TP2] Prefill batch, #new-seq: 2, #new-token: 49, #cached-token: 1389, token usage: 0.02, #running-req: 163, #queue-req: 0, 
[2025-10-28 11:28:33 DP1 TP1] Prefill batch, #new-seq: 2, #new-token: 34, #cached-token: 1399, token usage: 0.02, #running-req: 163, #queue-req: 0, 
[2025-10-28 11:28:33 DP3 TP3] Prefill batch, #new-seq: 2, #new-token: 60, #cached-token: 1473, token usage: 0.02, #running-req: 163, #queue-req: 0, 
[2025-10-28 11:28:33 DP6 TP6] Prefill batch, #new-seq: 3, #new-token: 58, #cached-token: 2099, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-28 11:28:33 DP0 TP0] Prefill batch, #new-seq: 2, #new-token: 62, #cached-token: 1403, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[2025-10-28 11:28:33 DP7 TP7] Prefill batch, #new-seq: 3, #new-token: 73, #cached-token: 2119, token usage: 0.02, #running-req: 162, #queue-req: 0, 
[aiter] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:33 DP5 TP5] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:33 DP7 TP7] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:33 DP4 TP4] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:33 DP6 TP6] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:33 DP1 TP1] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:33 DP2 TP2] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:33 DP3 TP3] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:33 DP0 TP0] [fused_moe] using default for (340, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:28:33 DP5 TP5] Decode batch, #running-req: 165, #token: 11664, token usage: 0.02, cuda graph: True, gen throughput (token/s): 47.04, #queue-req: 0, 
[2025-10-28 11:28:36 DP0 TP0] Decode batch, #running-req: 164, #token: 15139, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.95, #queue-req: 0, 
[2025-10-28 11:28:36 DP3 TP3] Decode batch, #running-req: 165, #token: 15198, token usage: 0.02, cuda graph: True, gen throughput (token/s): 222.77, #queue-req: 0, 
[2025-10-28 11:28:36] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:36] INFO:     127.0.0.1:34394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:36] INFO:     127.0.0.1:54360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:37 DP2 TP2] Decode batch, #running-req: 163, #token: 16521, token usage: 0.02, cuda graph: True, gen throughput (token/s): 281.03, #queue-req: 0, 
[2025-10-28 11:28:37] INFO:     127.0.0.1:56862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:37 DP6 TP6] Decode batch, #running-req: 165, #token: 16371, token usage: 0.02, cuda graph: True, gen throughput (token/s): 278.69, #queue-req: 0, 
[2025-10-28 11:28:37 DP1 TP1] Decode batch, #running-req: 164, #token: 16886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 294.79, #queue-req: 0, 
[2025-10-28 11:28:37 DP4 TP4] Decode batch, #running-req: 165, #token: 16928, token usage: 0.03, cuda graph: True, gen throughput (token/s): 292.04, #queue-req: 0, 
[2025-10-28 11:28:37] INFO:     127.0.0.1:37454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:37] INFO:     127.0.0.1:54654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:37] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:37] INFO:     127.0.0.1:58322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:37] INFO:     127.0.0.1:58624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:37] INFO:     127.0.0.1:58478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:36962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:55242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:36260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:53992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:55132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:56012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:37584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38 DP7 TP7] Decode batch, #running-req: 164, #token: 18098, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1311.22, #queue-req: 0, 
[2025-10-28 11:28:38] INFO:     127.0.0.1:58878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:35004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:34370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:35794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:54510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:34632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:35540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:33584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:34884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:33746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:34474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:36558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38 DP5 TP5] Decode batch, #running-req: 160, #token: 17689, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1378.96, #queue-req: 0, 
[2025-10-28 11:28:38] INFO:     127.0.0.1:36408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:57140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:36188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:59212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:34486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:34036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:33940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:36780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:37408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:59814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:53986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:54476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:34560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:38] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:35190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:33710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:35714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:56818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:54124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:33438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:37496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:59610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:58832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:60750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:60352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:58910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:35028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:59488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:37010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:36800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:36382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:33788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:34344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:35938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:57260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:35814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:36916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:54778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:34944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:59886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:59214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:33714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:35878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:35924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:36508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:55588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:35064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:58476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:34698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:58464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:60832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:53982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:60996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:55356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:55642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:60106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:35940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:55842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:39] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:55140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:58544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:55378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:32912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:35662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:57434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:60302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:59874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:37068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:59692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:58298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:58586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:60674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:60620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:36366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:58506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:35836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:36302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:60856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:55646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:34070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:36322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:60756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:34136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:34290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:35328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:59326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:37734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:37736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:60922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:35964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:60912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:54928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:59404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:54036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:59998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:36172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:54562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:59168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:36020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:37360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:54992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:34190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:37228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:36244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:37604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40 DP0 TP0] Decode batch, #running-req: 136, #token: 18228, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1330.36, #queue-req: 0, 
[2025-10-28 11:28:40] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:34850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:35780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:35978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:37708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:32932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:36312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:55352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:33042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:34900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:57390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:58844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:40] INFO:     127.0.0.1:37462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:35642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:33884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:54570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:56028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:56964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:56108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:33638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:54150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:33168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:33826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:58598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:33596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:37160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:33656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:35058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:56240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:35886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:37216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:37540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41 DP3 TP3] Decode batch, #running-req: 136, #token: 18103, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1340.41, #queue-req: 0, 
[2025-10-28 11:28:41] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:54998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:56758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:33130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:37624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:37178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:35256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:35514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:56532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:32876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:33010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:58014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:37038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:35346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:33692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:37050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:58162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:35834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:35034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:37298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:35032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:35232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:54960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:33978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:54178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41 DP2 TP2] Decode batch, #running-req: 122, #token: 17522, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1281.31, #queue-req: 0, 
[2025-10-28 11:28:41] INFO:     127.0.0.1:33518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:35380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:58682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:36914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:35798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:54324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:58676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:34592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:56576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:33172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:41] INFO:     127.0.0.1:35858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42 DP6 TP6] Decode batch, #running-req: 103, #token: 14867, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1272.29, #queue-req: 0, 
[2025-10-28 11:28:42] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:55308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:36310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:60964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:34278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:37442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:37802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:56884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:37066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:37152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:37476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:37834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:58636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:34336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:37694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:37744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:34010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:34316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42 DP4 TP4] Decode batch, #running-req: 100, #token: 14852, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1240.77, #queue-req: 0, 
[2025-10-28 11:28:42 DP1 TP1] Decode batch, #running-req: 111, #token: 16280, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1270.98, #queue-req: 0, 
[2025-10-28 11:28:42] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:59046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:34604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:36674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:35708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:59292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:58388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:56354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:59026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:60952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:35212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:57170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:36766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:37772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:55276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:58710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:35314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:36518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:36858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:32978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:35130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:36094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42 DP7 TP7] Decode batch, #running-req: 105, #token: 16228, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1304.95, #queue-req: 0, 
[2025-10-28 11:28:42] INFO:     127.0.0.1:59386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:36752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:56224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:37768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:32958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:60166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:57204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:32780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:59224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:37548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:59038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:32906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:35274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:36040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:57936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:58574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:59588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:37274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:36206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:37504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:56974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:35286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:35972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42 DP5 TP5] Decode batch, #running-req: 99, #token: 15440, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1193.82, #queue-req: 0, 
[2025-10-28 11:28:42] INFO:     127.0.0.1:32968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:54076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:35866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:60932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:35914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:57364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:34856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:42] INFO:     127.0.0.1:33304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:35430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:54192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:35646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:58320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:36982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:36970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:58802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:36070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:36530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:58860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:37666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:57334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:37764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:54466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:35026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:37432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:54866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:54790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:57348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:33606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:37596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:36726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:37418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:56852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:37384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:36428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:58194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:35752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:54006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:35810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:54306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:36022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:35700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:36288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:33560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:35412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:33742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:37684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:37560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:35140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:37678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:35352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:37098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:36746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:37372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:33238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:33388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:34408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:59774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:55160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:43] INFO:     127.0.0.1:35046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:37290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:34546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:59684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:37392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:59176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:34680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:32866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:36576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:37498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:57066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:33690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:54988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:33358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:34566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:36660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:32834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:37484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:33758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:36548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:36990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:33988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:37358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:33552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:37184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:56588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:54338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:37748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:54066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:54576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:33344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:34086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:37552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:37618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:55080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:32790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:33320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:34418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:36434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:54840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:59208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:59914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:36314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:36886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44 DP0 TP0] Decode batch, #running-req: 60, #token: 11181, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1033.20, #queue-req: 0, 
[2025-10-28 11:28:44] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:34806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:33572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:60272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:33476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:34990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:59650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:37164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:55462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:33466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:36494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:54144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:34762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:55126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:36998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:56944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:33104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:33664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:59362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:34206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:35396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:44] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:34786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:60080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:34384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:34334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45 DP3 TP3] Decode batch, #running-req: 74, #token: 13158, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1101.79, #queue-req: 0, 
[2025-10-28 11:28:45] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:34946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:59992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:54598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:58766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:37108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:37268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:34538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:37524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:34428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:58752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:58772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:59372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:35116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:33448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:54438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:32856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:37508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45 DP2 TP2] Decode batch, #running-req: 57, #token: 10856, token usage: 0.02, cuda graph: True, gen throughput (token/s): 997.61, #queue-req: 0, 
[2025-10-28 11:28:45] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:33902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:54662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:33372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:59908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:32818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:35420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:59870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:55618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:33290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:55040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:33636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:37250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:60592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45 DP6 TP6] Decode batch, #running-req: 49, #token: 8791, token usage: 0.01, cuda graph: True, gen throughput (token/s): 848.43, #queue-req: 0, 
[2025-10-28 11:28:45] INFO:     127.0.0.1:55666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:33506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:54084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:35254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:32888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:59454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:37614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:54394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:55722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:37118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45 DP1 TP1] Decode batch, #running-req: 46, #token: 9418, token usage: 0.01, cuda graph: True, gen throughput (token/s): 859.54, #queue-req: 0, 
[2025-10-28 11:28:45] INFO:     127.0.0.1:36832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45 DP4 TP4] Decode batch, #running-req: 40, #token: 7941, token usage: 0.01, cuda graph: True, gen throughput (token/s): 766.70, #queue-req: 0, 
[2025-10-28 11:28:45] INFO:     127.0.0.1:60634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:60036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:59856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:32926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:60760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:60220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:37356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:60228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:34028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:37346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:60374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:57968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:36460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:55450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:60056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:34056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:45] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:60342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:37136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:35502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:56060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:37134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46 DP7 TP7] Decode batch, #running-req: 42, #token: 8527, token usage: 0.01, cuda graph: True, gen throughput (token/s): 815.61, #queue-req: 0, 
[2025-10-28 11:28:46] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:58172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:58886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:59582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:60790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:34172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:34524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:35902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:34502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:56724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:33432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:33524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:35450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:36204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46 DP5 TP5] Decode batch, #running-req: 33, #token: 6850, token usage: 0.01, cuda graph: True, gen throughput (token/s): 798.45, #queue-req: 0, 
[2025-10-28 11:28:46] INFO:     127.0.0.1:54694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:57886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:33258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:33136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:37570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:36318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:33834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:54038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:35744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:34726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:60940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:33198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:37564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:34974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:58640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:59830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:60666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:58056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:60598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:35670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:58712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:37200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:33210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:33484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:35532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:36626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:35626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:55718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:37306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:34926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:54004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:35730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:54536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:57992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:58728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:34582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:36642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:36944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:37024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:34508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:57618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:59354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:60156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:36144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:54414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:57092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:60040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:36154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:59604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:59444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:35710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:34262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:36442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:35564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:56624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:34868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:35102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:35750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:46] INFO:     127.0.0.1:35462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:33056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:35446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:33622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:36170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:59662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:32798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:56752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:60114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:36354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:37638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:35660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:35688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:37244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:54046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:34930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:33778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:55652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:34452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:34354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:37814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:37826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:34802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:34664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:54882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:37144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:37258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47 DP0 TP0] Decode batch, #running-req: 23, #token: 6045, token usage: 0.01, cuda graph: True, gen throughput (token/s): 558.78, #queue-req: 0, 
[2025-10-28 11:28:47] INFO:     127.0.0.1:57506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:34862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:32928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:54212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:56646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:37084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:35262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:34228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:37786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:55542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:60980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:56524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47 DP3 TP3] Decode batch, #running-req: 26, #token: 5900, token usage: 0.01, cuda graph: True, gen throughput (token/s): 701.78, #queue-req: 0, 
[2025-10-28 11:28:47] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:36386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:36458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:34368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:58830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:59646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:60204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:35010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:47] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48 DP2 TP2] Decode batch, #running-req: 18, #token: 4909, token usage: 0.01, cuda graph: True, gen throughput (token/s): 490.89, #queue-req: 0, 
[2025-10-28 11:28:48] INFO:     127.0.0.1:34710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:60168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:36928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:55458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48 DP6 TP6] Decode batch, #running-req: 17, #token: 4640, token usage: 0.01, cuda graph: True, gen throughput (token/s): 486.25, #queue-req: 0, 
[2025-10-28 11:28:48] INFO:     127.0.0.1:35080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:35146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:33120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:36864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48 DP4 TP4] Decode batch, #running-req: 16, #token: 4427, token usage: 0.01, cuda graph: True, gen throughput (token/s): 419.96, #queue-req: 0, 
[2025-10-28 11:28:48 DP1 TP1] Decode batch, #running-req: 19, #token: 5241, token usage: 0.01, cuda graph: True, gen throughput (token/s): 455.50, #queue-req: 0, 
[2025-10-28 11:28:48] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:34832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:37170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:36608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:36792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:59192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:34616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:57178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48 DP7 TP7] Decode batch, #running-req: 21, #token: 5377, token usage: 0.01, cuda graph: True, gen throughput (token/s): 490.50, #queue-req: 0, 
[2025-10-28 11:28:48] INFO:     127.0.0.1:33070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:57522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:60774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:35092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48 DP5 TP5] Decode batch, #running-req: 12, #token: 3740, token usage: 0.01, cuda graph: True, gen throughput (token/s): 308.41, #queue-req: 0, 
[2025-10-28 11:28:48] INFO:     127.0.0.1:32994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:37652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:54346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:57132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:36550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:56004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:35956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:35612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:36702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:35852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:59926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:60848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:60554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:33274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:59750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:36328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:37330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:59398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:34128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:48] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:56102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:36734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:59126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:33662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:37686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:34750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49 DP0 TP0] Decode batch, #running-req: 5, #token: 2101, token usage: 0.00, cuda graph: True, gen throughput (token/s): 271.73, #queue-req: 0, 
[2025-10-28 11:28:49] INFO:     127.0.0.1:54102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:34220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:60748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49 DP3 TP3] Decode batch, #running-req: 8, #token: 2709, token usage: 0.00, cuda graph: True, gen throughput (token/s): 261.74, #queue-req: 0, 
[2025-10-28 11:28:49] INFO:     127.0.0.1:54734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:54534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:35768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:59380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:33700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:55988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:36484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:56388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:49 DP2 TP2] Decode batch, #running-req: 7, #token: 2336, token usage: 0.00, cuda graph: True, gen throughput (token/s): 208.83, #queue-req: 0, 
[2025-10-28 11:28:49] INFO:     127.0.0.1:54612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:58666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:35298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:33886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50 DP6 TP6] Decode batch, #running-req: 9, #token: 2636, token usage: 0.00, cuda graph: True, gen throughput (token/s): 250.23, #queue-req: 0, 
[2025-10-28 11:28:50] INFO:     127.0.0.1:34744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:36412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:59910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50 DP4 TP4] Decode batch, #running-req: 7, #token: 2418, token usage: 0.00, cuda graph: True, gen throughput (token/s): 245.00, #queue-req: 0, 
[2025-10-28 11:28:50 DP1 TP1] Decode batch, #running-req: 5, #token: 2067, token usage: 0.00, cuda graph: True, gen throughput (token/s): 250.77, #queue-req: 0, 
[2025-10-28 11:28:50] INFO:     127.0.0.1:56370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:35994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:58924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:55334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50 DP7 TP7] Decode batch, #running-req: 12, #token: 4255, token usage: 0.01, cuda graph: True, gen throughput (token/s): 328.87, #queue-req: 0, 
[2025-10-28 11:28:50] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:37322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:32904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50 DP5 TP5] Decode batch, #running-req: 5, #token: 2263, token usage: 0.00, cuda graph: True, gen throughput (token/s): 184.04, #queue-req: 0, 
[2025-10-28 11:28:50] INFO:     127.0.0.1:59282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:35684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:37670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:33090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:54804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:35822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:36700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:37724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:57778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:60358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:55024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:32840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:33116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:50] INFO:     127.0.0.1:36398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:51] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:51] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:51] INFO:     127.0.0.1:34700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:51] INFO:     127.0.0.1:36370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:51 DP0 TP0] Decode batch, #running-req: 1, #token: 981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 90.67, #queue-req: 0, 
[2025-10-28 11:28:51] INFO:     127.0.0.1:54876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:51] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:51] INFO:     127.0.0.1:32960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:51] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:51] INFO:     127.0.0.1:35568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:51] INFO:     127.0.0.1:33222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:51 DP2 TP2] Decode batch, #running-req: 4, #token: 1614, token usage: 0.00, cuda graph: True, gen throughput (token/s): 118.69, #queue-req: 0, 
[2025-10-28 11:28:51] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:51 DP6 TP6] Decode batch, #running-req: 2, #token: 1265, token usage: 0.00, cuda graph: True, gen throughput (token/s): 74.62, #queue-req: 0, 
[2025-10-28 11:28:51 DP4 TP4] Decode batch, #running-req: 2, #token: 1385, token usage: 0.00, cuda graph: True, gen throughput (token/s): 120.10, #queue-req: 0, 
[2025-10-28 11:28:51 DP1 TP1] Decode batch, #running-req: 2, #token: 1304, token usage: 0.00, cuda graph: True, gen throughput (token/s): 97.92, #queue-req: 0, 
[2025-10-28 11:28:51 DP7 TP7] Decode batch, #running-req: 2, #token: 1373, token usage: 0.00, cuda graph: True, gen throughput (token/s): 137.81, #queue-req: 0, 
[2025-10-28 11:28:51 DP5 TP5] Decode batch, #running-req: 1, #token: 992, token usage: 0.00, cuda graph: True, gen throughput (token/s): 67.35, #queue-req: 0, 
[2025-10-28 11:28:51] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:52] INFO:     127.0.0.1:35186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:52] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:52] INFO:     127.0.0.1:55486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:52] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:52] INFO:     127.0.0.1:36046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:52] INFO:     127.0.0.1:35134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:52] INFO:     127.0.0.1:36540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:52] INFO:     127.0.0.1:34160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:52 DP2 TP2] Decode batch, #running-req: 1, #token: 1, token usage: 0.00, cuda graph: True, gen throughput (token/s): 66.96, #queue-req: 0, 
[2025-10-28 11:28:52] INFO:     127.0.0.1:60998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:52 DP6 TP6] Decode batch, #running-req: 1, #token: 1028, token usage: 0.00, cuda graph: True, gen throughput (token/s): 53.29, #queue-req: 0, 
[2025-10-28 11:28:52 DP4 TP4] Decode batch, #running-req: 1, #token: 1088, token usage: 0.00, cuda graph: True, gen throughput (token/s): 45.22, #queue-req: 0, 
[2025-10-28 11:28:53] INFO:     127.0.0.1:34714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:28:53] INFO:     127.0.0.1:55730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:05] INFO:     127.0.0.1:48758 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-28 11:29:05 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:05] INFO:     127.0.0.1:48766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:05 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:05 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 68, #cached-token: 1374, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:05 DP2 TP2] Prefill batch, #new-seq: 2, #new-token: 60, #cached-token: 1366, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:05 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1451, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:05 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 734, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:05 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 64, #cached-token: 1446, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:05 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1428, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:05 DP3 TP3] Prefill batch, #new-seq: 2, #new-token: 58, #cached-token: 1388, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:05 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 71, #cached-token: 3626, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:05 DP0 TP0] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:05 DP1 TP1] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:05 DP7 TP7] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:05 DP4 TP4] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:05 DP6 TP6] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:05 DP3 TP3] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:05 DP5 TP5] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:05 DP2 TP2] [fused_moe] using default for (326, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:05 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 281, #cached-token: 6995, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:29:05 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 256, #cached-token: 7043, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:29:05 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 505, #cached-token: 6775, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:29:05 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 167, #cached-token: 7116, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:29:05 DP0 TP0] Prefill batch, #new-seq: 11, #new-token: 230, #cached-token: 7862, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-28 11:29:05 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 143, #cached-token: 4183, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[2025-10-28 11:29:05 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 401, #cached-token: 6883, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:29:05 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 440, #cached-token: 6812, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:29:06 DP4 TP4] Prefill batch, #new-seq: 15, #new-token: 713, #cached-token: 10246, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:06 DP6 TP6] Prefill batch, #new-seq: 14, #new-token: 481, #cached-token: 9710, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:06 DP5 TP5] Prefill batch, #new-seq: 14, #new-token: 408, #cached-token: 9807, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:06 DP7 TP7] Prefill batch, #new-seq: 14, #new-token: 679, #cached-token: 9652, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:06 DP1 TP1] Prefill batch, #new-seq: 16, #new-token: 652, #cached-token: 10949, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:06 DP0 TP0] Prefill batch, #new-seq: 15, #new-token: 565, #cached-token: 10414, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:06 DP3 TP3] Prefill batch, #new-seq: 16, #new-token: 722, #cached-token: 10934, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:06 DP2 TP2] Prefill batch, #new-seq: 16, #new-token: 788, #cached-token: 10964, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:06 DP4 TP4] Prefill batch, #new-seq: 13, #new-token: 194, #cached-token: 9419, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-28 11:29:06 DP0 TP0] Prefill batch, #new-seq: 12, #new-token: 443, #cached-token: 8238, token usage: 0.00, #running-req: 27, #queue-req: 0, 
[2025-10-28 11:29:06 DP2 TP2] Prefill batch, #new-seq: 12, #new-token: 262, #cached-token: 8445, token usage: 0.00, #running-req: 28, #queue-req: 0, 
[2025-10-28 11:29:06 DP6 TP6] Prefill batch, #new-seq: 13, #new-token: 609, #cached-token: 8958, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-28 11:29:06 DP1 TP1] Prefill batch, #new-seq: 12, #new-token: 204, #cached-token: 8409, token usage: 0.00, #running-req: 28, #queue-req: 0, 
[2025-10-28 11:29:06 DP5 TP5] Prefill batch, #new-seq: 14, #new-token: 388, #cached-token: 9715, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-28 11:29:06 DP3 TP3] Prefill batch, #new-seq: 12, #new-token: 191, #cached-token: 8490, token usage: 0.00, #running-req: 28, #queue-req: 0, 
[2025-10-28 11:29:06 DP7 TP7] Prefill batch, #new-seq: 13, #new-token: 287, #cached-token: 9138, token usage: 0.00, #running-req: 26, #queue-req: 0, 
[2025-10-28 11:29:06 DP4 TP4] Prefill batch, #new-seq: 20, #new-token: 551, #cached-token: 14164, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-28 11:29:06 DP5 TP5] Prefill batch, #new-seq: 19, #new-token: 339, #cached-token: 13537, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-28 11:29:06 DP3 TP3] Prefill batch, #new-seq: 20, #new-token: 712, #cached-token: 13909, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-28 11:29:06 DP2 TP2] Prefill batch, #new-seq: 21, #new-token: 434, #cached-token: 14929, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-28 11:29:06 DP1 TP1] Prefill batch, #new-seq: 21, #new-token: 341, #cached-token: 14862, token usage: 0.01, #running-req: 40, #queue-req: 0, 
[2025-10-28 11:29:06 DP7 TP7] Prefill batch, #new-seq: 20, #new-token: 360, #cached-token: 14251, token usage: 0.01, #running-req: 39, #queue-req: 0, 
[2025-10-28 11:29:06 DP6 TP6] Prefill batch, #new-seq: 21, #new-token: 256, #cached-token: 15034, token usage: 0.01, #running-req: 39, #queue-req: 0, 
[2025-10-28 11:29:06 DP0 TP0] Prefill batch, #new-seq: 21, #new-token: 490, #cached-token: 14936, token usage: 0.01, #running-req: 39, #queue-req: 0, 
[2025-10-28 11:29:06 DP4 TP4] Prefill batch, #new-seq: 14, #new-token: 332, #cached-token: 9741, token usage: 0.01, #running-req: 60, #queue-req: 0, 
[2025-10-28 11:29:06 DP1 TP1] Prefill batch, #new-seq: 14, #new-token: 350, #cached-token: 9830, token usage: 0.01, #running-req: 61, #queue-req: 0, 
[2025-10-28 11:29:06 DP5 TP5] Prefill batch, #new-seq: 15, #new-token: 309, #cached-token: 10591, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-28 11:29:06 DP3 TP3] Prefill batch, #new-seq: 14, #new-token: 254, #cached-token: 9853, token usage: 0.01, #running-req: 60, #queue-req: 0, 
[2025-10-28 11:29:06 DP7 TP7] Prefill batch, #new-seq: 15, #new-token: 517, #cached-token: 10389, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-28 11:29:06 DP6 TP6] Prefill batch, #new-seq: 14, #new-token: 678, #cached-token: 9456, token usage: 0.01, #running-req: 60, #queue-req: 0, 
[2025-10-28 11:29:06 DP2 TP2] Prefill batch, #new-seq: 14, #new-token: 312, #cached-token: 9844, token usage: 0.01, #running-req: 61, #queue-req: 0, 
[2025-10-28 11:29:06 DP0 TP0] Prefill batch, #new-seq: 14, #new-token: 297, #cached-token: 10045, token usage: 0.01, #running-req: 60, #queue-req: 0, 
[2025-10-28 11:29:07 DP4 TP4] Prefill batch, #new-seq: 17, #new-token: 118, #cached-token: 12209, token usage: 0.01, #running-req: 74, #queue-req: 0, 
[2025-10-28 11:29:07 DP5 TP5] Prefill batch, #new-seq: 17, #new-token: 258, #cached-token: 12135, token usage: 0.01, #running-req: 74, #queue-req: 0, 
[2025-10-28 11:29:07 DP7 TP7] Prefill batch, #new-seq: 17, #new-token: 407, #cached-token: 11973, token usage: 0.01, #running-req: 74, #queue-req: 0, 
[2025-10-28 11:29:07 DP1 TP1] Prefill batch, #new-seq: 17, #new-token: 169, #cached-token: 12193, token usage: 0.01, #running-req: 75, #queue-req: 0, 
[2025-10-28 11:29:07 DP3 TP3] Prefill batch, #new-seq: 18, #new-token: 91, #cached-token: 12978, token usage: 0.01, #running-req: 74, #queue-req: 0, 
[2025-10-28 11:29:07 DP6 TP6] Prefill batch, #new-seq: 17, #new-token: 328, #cached-token: 12146, token usage: 0.01, #running-req: 74, #queue-req: 0, 
[2025-10-28 11:29:07 DP0 TP0] Prefill batch, #new-seq: 17, #new-token: 118, #cached-token: 12219, token usage: 0.01, #running-req: 74, #queue-req: 0, 
[2025-10-28 11:29:07 DP2 TP2] Prefill batch, #new-seq: 17, #new-token: 150, #cached-token: 12269, token usage: 0.01, #running-req: 75, #queue-req: 0, 
[2025-10-28 11:29:07 DP3 TP3] Prefill batch, #new-seq: 15, #new-token: 327, #cached-token: 10719, token usage: 0.01, #running-req: 92, #queue-req: 0, 
[2025-10-28 11:29:07 DP4 TP4] Prefill batch, #new-seq: 16, #new-token: 203, #cached-token: 11440, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-10-28 11:29:07 DP5 TP5] Prefill batch, #new-seq: 16, #new-token: 318, #cached-token: 11444, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-10-28 11:29:07 DP2 TP2] Prefill batch, #new-seq: 16, #new-token: 396, #cached-token: 11282, token usage: 0.01, #running-req: 92, #queue-req: 0, 
[2025-10-28 11:29:07 DP6 TP6] Prefill batch, #new-seq: 16, #new-token: 164, #cached-token: 11522, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-10-28 11:29:07 DP0 TP0] Prefill batch, #new-seq: 16, #new-token: 265, #cached-token: 11532, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-10-28 11:29:07 DP7 TP7] Prefill batch, #new-seq: 16, #new-token: 88, #cached-token: 11564, token usage: 0.01, #running-req: 91, #queue-req: 0, 
[2025-10-28 11:29:07 DP1 TP1] Prefill batch, #new-seq: 16, #new-token: 195, #cached-token: 11324, token usage: 0.01, #running-req: 92, #queue-req: 0, 
[2025-10-28 11:29:07 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 87, #cached-token: 7270, token usage: 0.01, #running-req: 107, #queue-req: 0, 
[2025-10-28 11:29:07 DP4 TP4] Prefill batch, #new-seq: 11, #new-token: 259, #cached-token: 7728, token usage: 0.01, #running-req: 107, #queue-req: 0, 
[2025-10-28 11:29:07 DP1 TP1] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7304, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-28 11:29:07 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 43, #cached-token: 7262, token usage: 0.01, #running-req: 107, #queue-req: 0, 
[2025-10-28 11:29:07 DP3 TP3] Prefill batch, #new-seq: 11, #new-token: 261, #cached-token: 7718, token usage: 0.01, #running-req: 107, #queue-req: 0, 
[2025-10-28 11:29:07 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 142, #cached-token: 7129, token usage: 0.01, #running-req: 107, #queue-req: 0, 
[2025-10-28 11:29:07 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 67, #cached-token: 7129, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-28 11:29:07 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 88, #cached-token: 7239, token usage: 0.01, #running-req: 107, #queue-req: 0, 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:07 DP0 TP0] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:07 DP7 TP7] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:07 DP5 TP5] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:07 DP4 TP4] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:07 DP6 TP6] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:07 DP1 TP1] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:07 DP2 TP2] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:07 DP3 TP3] [fused_moe] using default for (957, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:07 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 138, #cached-token: 6418, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-28 11:29:07 DP1 TP1] Prefill batch, #new-seq: 10, #new-token: 385, #cached-token: 7072, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-28 11:29:07 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 340, #cached-token: 7183, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-28 11:29:07 DP3 TP3] Prefill batch, #new-seq: 9, #new-token: 228, #cached-token: 6305, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-28 11:29:07 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 286, #cached-token: 7105, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-28 11:29:07 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 193, #cached-token: 6449, token usage: 0.01, #running-req: 118, #queue-req: 0, 
[2025-10-28 11:29:07 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 267, #cached-token: 7054, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-28 11:29:07 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 217, #cached-token: 7005, token usage: 0.01, #running-req: 117, #queue-req: 0, 
[2025-10-28 11:29:07 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 274, #cached-token: 6349, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-28 11:29:07 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 225, #cached-token: 6282, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-28 11:29:07 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 239, #cached-token: 6423, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-28 11:29:07 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 115, #cached-token: 6427, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-28 11:29:07 DP3 TP3] Prefill batch, #new-seq: 9, #new-token: 138, #cached-token: 6279, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-28 11:29:07 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 203, #cached-token: 6443, token usage: 0.01, #running-req: 128, #queue-req: 0, 
[2025-10-28 11:29:07 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 58, #cached-token: 6671, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-28 11:29:07 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 159, #cached-token: 7075, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-28 11:29:08 DP4 TP4] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8680, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-28 11:29:08 DP1 TP1] Prefill batch, #new-seq: 11, #new-token: 88, #cached-token: 7845, token usage: 0.01, #running-req: 137, #queue-req: 0, 
[2025-10-28 11:29:08 DP5 TP5] Prefill batch, #new-seq: 12, #new-token: 130, #cached-token: 8564, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-28 11:29:08 DP3 TP3] Prefill batch, #new-seq: 12, #new-token: 49, #cached-token: 8617, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-28 11:29:08 DP0 TP0] Prefill batch, #new-seq: 11, #new-token: 292, #cached-token: 7813, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-28 11:29:08 DP7 TP7] Prefill batch, #new-seq: 12, #new-token: 76, #cached-token: 8761, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-28 11:29:08 DP6 TP6] Prefill batch, #new-seq: 12, #new-token: 188, #cached-token: 8477, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-28 11:29:08 DP2 TP2] Prefill batch, #new-seq: 11, #new-token: 130, #cached-token: 7881, token usage: 0.01, #running-req: 137, #queue-req: 0, 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP4 TP4] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP7 TP7] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP1 TP1] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP3 TP3] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP0 TP0] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP6 TP6] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP5 TP5] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP2 TP2] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5101, token usage: 0.01, #running-req: 148, #queue-req: 0, 
[2025-10-28 11:29:08 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 45, #cached-token: 4979, token usage: 0.01, #running-req: 148, #queue-req: 0, 
[2025-10-28 11:29:08 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5777, token usage: 0.01, #running-req: 148, #queue-req: 0, 
[2025-10-28 11:29:08 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 62, #cached-token: 5112, token usage: 0.02, #running-req: 148, #queue-req: 0, 
[2025-10-28 11:29:08 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 221, #cached-token: 5749, token usage: 0.01, #running-req: 148, #queue-req: 0, 
[2025-10-28 11:29:08 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5067, token usage: 0.01, #running-req: 148, #queue-req: 0, 
[2025-10-28 11:29:08 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 116, #cached-token: 5899, token usage: 0.02, #running-req: 147, #queue-req: 0, 
[2025-10-28 11:29:08 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 98, #cached-token: 5695, token usage: 0.01, #running-req: 148, #queue-req: 0, 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP1 TP1] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP5 TP5] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP2 TP2] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP0 TP0] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP7 TP7] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP4 TP4] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP6 TP6] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP3 TP3] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 59, #cached-token: 5803, token usage: 0.02, #running-req: 155, #queue-req: 0, 
[2025-10-28 11:29:08 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6459, token usage: 0.02, #running-req: 155, #queue-req: 0, 
[2025-10-28 11:29:08 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 108, #cached-token: 5796, token usage: 0.02, #running-req: 155, #queue-req: 0, 
[2025-10-28 11:29:08 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 76, #cached-token: 5765, token usage: 0.01, #running-req: 156, #queue-req: 0, 
[2025-10-28 11:29:08 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 148, #cached-token: 5780, token usage: 0.02, #running-req: 155, #queue-req: 0, 
[2025-10-28 11:29:08 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5943, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[2025-10-28 11:29:08 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5785, token usage: 0.02, #running-req: 155, #queue-req: 0, 
[2025-10-28 11:29:08 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5989, token usage: 0.02, #running-req: 156, #queue-req: 0, 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP1 TP1] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP5 TP5] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP6 TP6] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP4 TP4] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP2 TP2] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP7 TP7] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP3 TP3] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP0 TP0] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP4 TP4] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 727, token usage: 0.02, #running-req: 164, #queue-req: 0, 
[2025-10-28 11:29:08 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 718, token usage: 0.02, #running-req: 164, #queue-req: 0, 
[2025-10-28 11:29:08 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1446, token usage: 0.02, #running-req: 163, #queue-req: 0, 
[2025-10-28 11:29:08 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1443, token usage: 0.02, #running-req: 163, #queue-req: 0, 
[2025-10-28 11:29:08 DP3 TP3] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 728, token usage: 0.02, #running-req: 164, #queue-req: 0, 
[2025-10-28 11:29:08 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 739, token usage: 0.02, #running-req: 163, #queue-req: 0, 
[2025-10-28 11:29:08 DP2 TP2] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 702, token usage: 0.02, #running-req: 164, #queue-req: 0, 
[2025-10-28 11:29:08 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1429, token usage: 0.02, #running-req: 163, #queue-req: 0, 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP3 TP3] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP5 TP5] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP4 TP4] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP1 TP1] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP7 TP7] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP0 TP0] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP2 TP2] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:08 DP6 TP6] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:10 DP3 TP3] Decode batch, #running-req: 165, #token: 13075, token usage: 0.02, cuda graph: True, gen throughput (token/s): 131.92, #queue-req: 0, 
[2025-10-28 11:29:10 DP6 TP6] Decode batch, #running-req: 165, #token: 13876, token usage: 0.02, cuda graph: True, gen throughput (token/s): 175.56, #queue-req: 0, 
[2025-10-28 11:29:10 DP1 TP1] Decode batch, #running-req: 165, #token: 13725, token usage: 0.02, cuda graph: True, gen throughput (token/s): 173.44, #queue-req: 0, 
[2025-10-28 11:29:10 DP7 TP7] Decode batch, #running-req: 165, #token: 14423, token usage: 0.02, cuda graph: True, gen throughput (token/s): 174.73, #queue-req: 0, 
[2025-10-28 11:29:11] INFO:     127.0.0.1:51562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:12 DP4 TP4] Decode batch, #running-req: 165, #token: 15542, token usage: 0.02, cuda graph: True, gen throughput (token/s): 257.86, #queue-req: 0, 
[2025-10-28 11:29:12] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:12 DP0 TP0] Decode batch, #running-req: 163, #token: 16436, token usage: 0.02, cuda graph: True, gen throughput (token/s): 249.25, #queue-req: 0, 
[2025-10-28 11:29:12] INFO:     127.0.0.1:51598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:12 DP5 TP5] Decode batch, #running-req: 164, #token: 16703, token usage: 0.02, cuda graph: True, gen throughput (token/s): 284.68, #queue-req: 0, 
[2025-10-28 11:29:12] INFO:     127.0.0.1:60060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13 DP2 TP2] Decode batch, #running-req: 165, #token: 17189, token usage: 0.03, cuda graph: True, gen throughput (token/s): 315.25, #queue-req: 0, 
[2025-10-28 11:29:13] INFO:     127.0.0.1:49426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:48802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:60244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:50268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:49970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:58878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:52304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:51436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:58348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:49388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:57054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:53432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:53540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:49168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:50370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:51948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:53636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:57168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:52440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:55402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:53804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:59180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:54594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:49338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:53312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:13] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:52088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:58802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:51946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:57248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:50968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:54506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:52276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:52950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:58212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:54876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:53700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:48792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:55696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:58856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:53166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:49982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:54930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:53124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:49194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:59424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:50508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:57744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:51254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:58504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:52794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:58364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:59636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:56378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:57860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:51740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:56448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:58438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:50872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:57808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:52726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:60086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:49840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:50874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:50402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:52168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:53482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:55378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:14] INFO:     127.0.0.1:57020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:49708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:50524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:54754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:48780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15 DP3 TP3] Decode batch, #running-req: 144, #token: 17167, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1338.46, #queue-req: 0, 
[2025-10-28 11:29:15] INFO:     127.0.0.1:51374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:49546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:58970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:51410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:50716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:54158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:51630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:54572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:50630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:49826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:51956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:49022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:54498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:53138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15 DP6 TP6] Decode batch, #running-req: 147, #token: 18171, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1339.71, #queue-req: 0, 
[2025-10-28 11:29:15] INFO:     127.0.0.1:53064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:52974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:57674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15 DP1 TP1] Decode batch, #running-req: 132, #token: 16792, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1307.62, #queue-req: 0, 
[2025-10-28 11:29:15] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15 DP7 TP7] Decode batch, #running-req: 146, #token: 18881, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1354.73, #queue-req: 0, 
[2025-10-28 11:29:15] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:58920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:60384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:54678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:56462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:56728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:53098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:53620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:50778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:49438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:48914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:58398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:54170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:54244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:49062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:54956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:49672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:59844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:56518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:58860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:51380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:49532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:51498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:52138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:54858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:56542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:15] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:51670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:56402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:51698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:56558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:51658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:53116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:60232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:56574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:56602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:60178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:48966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:51336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:53436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:55466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:56390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:53600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:55430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:55946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:53510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:60028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16 DP4 TP4] Decode batch, #running-req: 128, #token: 17263, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1331.11, #queue-req: 0, 
[2025-10-28 11:29:16] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:51966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:55628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:55090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:55440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:53204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16 DP0 TP0] Decode batch, #running-req: 131, #token: 18521, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1350.31, #queue-req: 0, 
[2025-10-28 11:29:16] INFO:     127.0.0.1:56966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:57526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:57940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:51772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:56416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:57114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:52188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:56642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:49354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:16] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:48968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:58576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:51858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:60044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:57152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:60386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:60462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:52908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:51138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:60348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17 DP5 TP5] Decode batch, #running-req: 122, #token: 18192, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1316.17, #queue-req: 0, 
[2025-10-28 11:29:17] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:54654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:54740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:50148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:54446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:50826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:50344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:60438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:51782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:48936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17 DP2 TP2] Decode batch, #running-req: 115, #token: 17150, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1316.00, #queue-req: 0, 
[2025-10-28 11:29:17] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:59512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:57292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:59396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:48982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:51914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:50096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:59366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:51614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:58232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:50942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:57004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:60416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:54186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:59878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:60256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:57966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:51316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:51518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:54694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:56114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:52996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:50984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:55558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:60316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:57992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:17] INFO:     127.0.0.1:52308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:49310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:51684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:54192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:54484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:49402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:57330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:51596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:53786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:49012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:60318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:50808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:57444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:59634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:50178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:51162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:60262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:54004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:49450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:53180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:53462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:50072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:49430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:51710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:48910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:59250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:50202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:59044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:59338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:49262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:49346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:50206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:48816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:59136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:49834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:60216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:60008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:57070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:59326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:53494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:53714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:56294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:53818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:59984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:49620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:18] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:50536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:54406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19 DP3 TP3] Decode batch, #running-req: 75, #token: 12006, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1137.85, #queue-req: 0, 
[2025-10-28 11:29:19] INFO:     127.0.0.1:59358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:48970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:54016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:60326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:49380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:50234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:50598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:49094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:54084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:50998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:51588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:51776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:59146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:49312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:55638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:58580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19 DP6 TP6] Decode batch, #running-req: 78, #token: 12916, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1097.67, #queue-req: 0, 
[2025-10-28 11:29:19] INFO:     127.0.0.1:51644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19 DP1 TP1] Decode batch, #running-req: 64, #token: 11430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1012.46, #queue-req: 0, 
[2025-10-28 11:29:19 DP7 TP7] Decode batch, #running-req: 73, #token: 12391, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1145.75, #queue-req: 0, 
[2025-10-28 11:29:19] INFO:     127.0.0.1:53248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:58006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:51804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:50764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:51750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:59576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:55608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:60284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:50552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:49830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:51866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:54276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:48888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:50582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:50932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:49026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:51288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:54288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:48902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:59060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:59524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:55572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:55120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:54554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:50680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:58936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:53110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:19] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:56524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:48954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:57374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:56926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:50440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:58024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:49072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20 DP4 TP4] Decode batch, #running-req: 61, #token: 11111, token usage: 0.02, cuda graph: True, gen throughput (token/s): 954.19, #queue-req: 0, 
[2025-10-28 11:29:20] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:58816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:50384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:57514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:56200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:55272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:58446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:58686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20 DP0 TP0] Decode batch, #running-req: 65, #token: 12838, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1035.35, #queue-req: 0, 
[2025-10-28 11:29:20] INFO:     127.0.0.1:60142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:55590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:49400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:58766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:55540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:49006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:50894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:49270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:56126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:49060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:56184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20 DP5 TP5] Decode batch, #running-req: 50, #token: 9231, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1032.89, #queue-req: 0, 
[2025-10-28 11:29:20] INFO:     127.0.0.1:49978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:49746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:57460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:48880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:55434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:56248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:55256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:49820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:57422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:56718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:48926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:52126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:50312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:54478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:57106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20 DP2 TP2] Decode batch, #running-req: 46, #token: 9412, token usage: 0.01, cuda graph: True, gen throughput (token/s): 887.70, #queue-req: 0, 
[2025-10-28 11:29:20] INFO:     127.0.0.1:55652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:56306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:53228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:20] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:52606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:51712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:54820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:53614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:57842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:54706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:51042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:59726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:51116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:52436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:52322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:51544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:54902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:55302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:52824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:54784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:56268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:49488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:51166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:48990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:54626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:55520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:59808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:58828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:56506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:49230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:50652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:58284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:56232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:56590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:56240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:52688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:52742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:59258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:58996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:57518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:57736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:54432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:48834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:48794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:51918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:59578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:53518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:59656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:49510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:51792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:57178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:57668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:58262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:53404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:21] INFO:     127.0.0.1:59264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:53052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:55344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:53838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:51484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:54152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:54776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:52368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:51524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:58076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:49848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22 DP3 TP3] Decode batch, #running-req: 27, #token: 6011, token usage: 0.01, cuda graph: True, gen throughput (token/s): 656.76, #queue-req: 0, 
[2025-10-28 11:29:22] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:57598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:60308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:49892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:56342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:52386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:57092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:59718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:49702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:49368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:51514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:50252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22 DP6 TP6] Decode batch, #running-req: 29, #token: 6478, token usage: 0.01, cuda graph: True, gen throughput (token/s): 637.72, #queue-req: 0, 
[2025-10-28 11:29:22] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:51056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22 DP7 TP7] Decode batch, #running-req: 32, #token: 7547, token usage: 0.01, cuda graph: True, gen throughput (token/s): 642.36, #queue-req: 0, 
[2025-10-28 11:29:22 DP1 TP1] Decode batch, #running-req: 19, #token: 4527, token usage: 0.01, cuda graph: True, gen throughput (token/s): 551.95, #queue-req: 0, 
[2025-10-28 11:29:22] INFO:     127.0.0.1:55998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:56354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:59506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:55186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:50820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:58114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:48950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:50856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:52834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:57026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:59162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:51330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:50326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:60422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:59098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:58466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:56894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:48818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:53218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:51986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:22] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23 DP4 TP4] Decode batch, #running-req: 17, #token: 4403, token usage: 0.01, cuda graph: True, gen throughput (token/s): 492.17, #queue-req: 0, 
[2025-10-28 11:29:23] INFO:     127.0.0.1:58140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:51206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:52738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:52486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:52426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23 DP0 TP0] Decode batch, #running-req: 29, #token: 7254, token usage: 0.01, cuda graph: True, gen throughput (token/s): 689.99, #queue-req: 0, 
[2025-10-28 11:29:23] INFO:     127.0.0.1:50846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:54866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:54984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:59112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:55096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:50610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:58750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:58408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23 DP5 TP5] Decode batch, #running-req: 19, #token: 4644, token usage: 0.01, cuda graph: True, gen throughput (token/s): 435.03, #queue-req: 0, 
[2025-10-28 11:29:23] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:57694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:50644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:59350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:59892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:50694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:51904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:54348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:49452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:55802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23 DP2 TP2] Decode batch, #running-req: 14, #token: 4006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 489.32, #queue-req: 0, 
[2025-10-28 11:29:23] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:53454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:59436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:52240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:55092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:54352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:49640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:56042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:52356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:57812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:50760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:49922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:52588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:51030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:52848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:49322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:23] INFO:     127.0.0.1:51610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:55766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:52562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:57544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:57930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:60202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:49154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:49378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24 DP3 TP3] Decode batch, #running-req: 10, #token: 3317, token usage: 0.00, cuda graph: True, gen throughput (token/s): 302.99, #queue-req: 0, 
[2025-10-28 11:29:24] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:59792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:53724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:57386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:49416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:59310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24 DP6 TP6] Decode batch, #running-req: 9, #token: 3070, token usage: 0.00, cuda graph: True, gen throughput (token/s): 295.05, #queue-req: 0, 
[2025-10-28 11:29:24] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24 DP7 TP7] Decode batch, #running-req: 16, #token: 4926, token usage: 0.01, cuda graph: True, gen throughput (token/s): 435.49, #queue-req: 0, 
[2025-10-28 11:29:24 DP1 TP1] Decode batch, #running-req: 6, #token: 2051, token usage: 0.00, cuda graph: True, gen throughput (token/s): 232.74, #queue-req: 0, 
[2025-10-28 11:29:24] INFO:     127.0.0.1:48848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:55594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:54032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:50040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:50504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:60324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24 DP4 TP4] Decode batch, #running-req: 11, #token: 3433, token usage: 0.01, cuda graph: True, gen throughput (token/s): 272.35, #queue-req: 0, 
[2025-10-28 11:29:24] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:53004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:58954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:58314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:24] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25 DP0 TP0] Decode batch, #running-req: 8, #token: 2358, token usage: 0.00, cuda graph: True, gen throughput (token/s): 350.75, #queue-req: 0, 
[2025-10-28 11:29:25] INFO:     127.0.0.1:51352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:50758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:53742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:54534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:54796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:51844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25 DP5 TP5] Decode batch, #running-req: 4, #token: 1812, token usage: 0.00, cuda graph: True, gen throughput (token/s): 194.05, #queue-req: 0, 
[2025-10-28 11:29:25] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:49882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25 DP2 TP2] Decode batch, #running-req: 7, #token: 2365, token usage: 0.00, cuda graph: True, gen throughput (token/s): 268.13, #queue-req: 0, 
[2025-10-28 11:29:25] INFO:     127.0.0.1:59946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:51268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:50870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:52638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:55850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:60364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:55980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:52468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:54998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:53852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:25] INFO:     127.0.0.1:50108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26] INFO:     127.0.0.1:53082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26] INFO:     127.0.0.1:49608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26] INFO:     127.0.0.1:58228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26 DP3 TP3] Decode batch, #running-req: 3, #token: 1607, token usage: 0.00, cuda graph: True, gen throughput (token/s): 138.99, #queue-req: 0, 
[2025-10-28 11:29:26] INFO:     127.0.0.1:49774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26 DP6 TP6] Decode batch, #running-req: 1, #token: 0, token usage: 0.00, cuda graph: True, gen throughput (token/s): 105.80, #queue-req: 0, 
[2025-10-28 11:29:26 DP1 TP1] Decode batch, #running-req: 3, #token: 1625, token usage: 0.00, cuda graph: True, gen throughput (token/s): 99.21, #queue-req: 0, 
[2025-10-28 11:29:26 DP7 TP7] Decode batch, #running-req: 8, #token: 3126, token usage: 0.00, cuda graph: True, gen throughput (token/s): 285.33, #queue-req: 0, 
[2025-10-28 11:29:26] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26] INFO:     127.0.0.1:53870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26] INFO:     127.0.0.1:59298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26] INFO:     127.0.0.1:55362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26] INFO:     127.0.0.1:50018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26] INFO:     127.0.0.1:55718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26 DP4 TP4] Decode batch, #running-req: 4, #token: 1943, token usage: 0.00, cuda graph: True, gen throughput (token/s): 155.47, #queue-req: 0, 
[2025-10-28 11:29:26] INFO:     127.0.0.1:53998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26 DP0 TP0] Decode batch, #running-req: 4, #token: 1963, token usage: 0.00, cuda graph: True, gen throughput (token/s): 124.51, #queue-req: 0, 
[2025-10-28 11:29:26] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26] INFO:     127.0.0.1:59030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:26 DP2 TP2] Decode batch, #running-req: 1, #token: 968, token usage: 0.00, cuda graph: True, gen throughput (token/s): 68.62, #queue-req: 0, 
[2025-10-28 11:29:27] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:27] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:27] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:27] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:27] INFO:     127.0.0.1:56818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:27] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:27 DP3 TP3] Decode batch, #running-req: 2, #token: 1369, token usage: 0.00, cuda graph: True, gen throughput (token/s): 60.29, #queue-req: 0, 
[2025-10-28 11:29:27] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:27] INFO:     127.0.0.1:51640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:27] INFO:     127.0.0.1:57886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:27 DP1 TP1] Decode batch, #running-req: 2, #token: 1413, token usage: 0.00, cuda graph: True, gen throughput (token/s): 78.32, #queue-req: 0, 
[2025-10-28 11:29:27 DP7 TP7] Decode batch, #running-req: 1, #token: 1024, token usage: 0.00, cuda graph: True, gen throughput (token/s): 141.26, #queue-req: 0, 
[2025-10-28 11:29:27] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:27 DP4 TP4] Decode batch, #running-req: 2, #token: 1040, token usage: 0.00, cuda graph: True, gen throughput (token/s): 92.49, #queue-req: 0, 
[2025-10-28 11:29:27 DP0 TP0] Decode batch, #running-req: 1, #token: 1028, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.50, #queue-req: 0, 
[2025-10-28 11:29:28] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:28] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:28] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:28 DP1 TP1] Decode batch, #running-req: 1, #token: 1052, token usage: 0.00, cuda graph: True, gen throughput (token/s): 61.09, #queue-req: 0, 
[2025-10-28 11:29:28 DP7 TP7] Decode batch, #running-req: 1, #token: 1064, token usage: 0.00, cuda graph: True, gen throughput (token/s): 35.41, #queue-req: 0, 
[2025-10-28 11:29:28] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:29 DP4 TP4] Decode batch, #running-req: 1, #token: 1080, token usage: 0.00, cuda graph: True, gen throughput (token/s): 36.54, #queue-req: 0, 
[2025-10-28 11:29:29 DP1 TP1] Decode batch, #running-req: 1, #token: 1092, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.51, #queue-req: 0, 
[2025-10-28 11:29:30 DP4 TP4] Decode batch, #running-req: 1, #token: 1120, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.52, #queue-req: 0, 
[2025-10-28 11:29:30 DP1 TP1] Decode batch, #running-req: 1, #token: 1132, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.54, #queue-req: 0, 
[2025-10-28 11:29:31 DP4 TP4] Decode batch, #running-req: 1, #token: 1160, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.55, #queue-req: 0, 
[2025-10-28 11:29:31] INFO:     127.0.0.1:49932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:32 DP4 TP4] Decode batch, #running-req: 1, #token: 1200, token usage: 0.00, cuda graph: True, gen throughput (token/s): 38.76, #queue-req: 0, 
[2025-10-28 11:29:32] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:45] INFO:     127.0.0.1:48292 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-28 11:29:45 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:45] INFO:     127.0.0.1:48294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:45 DP1 TP1] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:45 DP4 TP4] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1435, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:45 DP6 TP6] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1451, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:45 DP2 TP2] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1421, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:45 DP0 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 734, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:45 DP7 TP7] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1428, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:45 DP5 TP5] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1513, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-28 11:29:45 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 190, #cached-token: 3499, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-28 11:29:45 DP3 TP3] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1447, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:45 DP0 TP0] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:45 DP7 TP7] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:45 DP1 TP1] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:45 DP5 TP5] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:45 DP6 TP6] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:45 DP4 TP4] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:45 DP3 TP3] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:45 DP2 TP2] [fused_moe] using default for (203, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:46 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 495, #cached-token: 6823, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:29:46 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 268, #cached-token: 4792, token usage: 0.00, #running-req: 6, #queue-req: 0, 
[2025-10-28 11:29:46 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 513, #cached-token: 6867, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:29:46 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 412, #cached-token: 6945, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:29:46 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 318, #cached-token: 6890, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:29:46 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 285, #cached-token: 7021, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:29:46 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 266, #cached-token: 6940, token usage: 0.00, #running-req: 2, #queue-req: 0, 
[2025-10-28 11:29:46 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 380, #cached-token: 7001, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-28 11:29:46 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 110, #cached-token: 5693, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:46 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 236, #cached-token: 5605, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:46 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 131, #cached-token: 5756, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:46 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 264, #cached-token: 4839, token usage: 0.00, #running-req: 13, #queue-req: 0, 
[2025-10-28 11:29:46 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 299, #cached-token: 5594, token usage: 0.00, #running-req: 11, #queue-req: 0, 
[2025-10-28 11:29:46 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 385, #cached-token: 5538, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:46 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 227, #cached-token: 5544, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:46 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 217, #cached-token: 5524, token usage: 0.00, #running-req: 12, #queue-req: 0, 
[2025-10-28 11:29:46 DP4 TP4] Prefill batch, #new-seq: 14, #new-token: 456, #cached-token: 9762, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:29:46 DP2 TP2] Prefill batch, #new-seq: 14, #new-token: 443, #cached-token: 9720, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:29:46 DP6 TP6] Prefill batch, #new-seq: 14, #new-token: 377, #cached-token: 9841, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:29:46 DP7 TP7] Prefill batch, #new-seq: 14, #new-token: 512, #cached-token: 9652, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:29:46 DP0 TP0] Prefill batch, #new-seq: 15, #new-token: 440, #cached-token: 10528, token usage: 0.00, #running-req: 19, #queue-req: 0, 
[2025-10-28 11:29:46 DP3 TP3] Prefill batch, #new-seq: 14, #new-token: 362, #cached-token: 9812, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:29:46 DP5 TP5] Prefill batch, #new-seq: 14, #new-token: 359, #cached-token: 9817, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:29:46 DP1 TP1] Prefill batch, #new-seq: 15, #new-token: 417, #cached-token: 10454, token usage: 0.00, #running-req: 20, #queue-req: 0, 
[2025-10-28 11:29:46 DP3 TP3] Prefill batch, #new-seq: 11, #new-token: 353, #cached-token: 7663, token usage: 0.00, #running-req: 34, #queue-req: 0, 
[2025-10-28 11:29:46 DP1 TP1] Prefill batch, #new-seq: 10, #new-token: 155, #cached-token: 6984, token usage: 0.00, #running-req: 35, #queue-req: 0, 
[2025-10-28 11:29:46 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 199, #cached-token: 7109, token usage: 0.00, #running-req: 34, #queue-req: 0, 
[2025-10-28 11:29:46 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 172, #cached-token: 7177, token usage: 0.00, #running-req: 34, #queue-req: 0, 
[2025-10-28 11:29:46 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 72, #cached-token: 7255, token usage: 0.00, #running-req: 34, #queue-req: 0, 
[2025-10-28 11:29:46 DP2 TP2] Prefill batch, #new-seq: 11, #new-token: 334, #cached-token: 7729, token usage: 0.00, #running-req: 34, #queue-req: 0, 
[2025-10-28 11:29:46 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 242, #cached-token: 7077, token usage: 0.00, #running-req: 34, #queue-req: 0, 
[2025-10-28 11:29:46 DP4 TP4] Prefill batch, #new-seq: 11, #new-token: 213, #cached-token: 7796, token usage: 0.00, #running-req: 34, #queue-req: 0, 
[2025-10-28 11:29:46 DP3 TP3] Prefill batch, #new-seq: 14, #new-token: 200, #cached-token: 10011, token usage: 0.01, #running-req: 45, #queue-req: 0, 
[2025-10-28 11:29:46 DP7 TP7] Prefill batch, #new-seq: 15, #new-token: 57, #cached-token: 10844, token usage: 0.01, #running-req: 44, #queue-req: 0, 
[2025-10-28 11:29:46 DP1 TP1] Prefill batch, #new-seq: 15, #new-token: 262, #cached-token: 10680, token usage: 0.01, #running-req: 45, #queue-req: 0, 
[2025-10-28 11:29:46 DP0 TP0] Prefill batch, #new-seq: 15, #new-token: 155, #cached-token: 10897, token usage: 0.01, #running-req: 44, #queue-req: 0, 
[2025-10-28 11:29:46 DP5 TP5] Prefill batch, #new-seq: 15, #new-token: 167, #cached-token: 10648, token usage: 0.01, #running-req: 44, #queue-req: 0, 
[2025-10-28 11:29:46 DP4 TP4] Prefill batch, #new-seq: 15, #new-token: 226, #cached-token: 10897, token usage: 0.01, #running-req: 45, #queue-req: 0, 
[2025-10-28 11:29:46 DP2 TP2] Prefill batch, #new-seq: 15, #new-token: 200, #cached-token: 10794, token usage: 0.01, #running-req: 45, #queue-req: 0, 
[2025-10-28 11:29:46 DP6 TP6] Prefill batch, #new-seq: 15, #new-token: 164, #cached-token: 10681, token usage: 0.01, #running-req: 44, #queue-req: 0, 
[2025-10-28 11:29:46 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 111, #cached-token: 7173, token usage: 0.01, #running-req: 60, #queue-req: 0, 
[2025-10-28 11:29:46 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 372, #cached-token: 6887, token usage: 0.01, #running-req: 60, #queue-req: 0, 
[2025-10-28 11:29:46 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 187, #cached-token: 7096, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-28 11:29:46 DP1 TP1] Prefill batch, #new-seq: 10, #new-token: 310, #cached-token: 6914, token usage: 0.01, #running-req: 60, #queue-req: 0, 
[2025-10-28 11:29:46 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 295, #cached-token: 6995, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-28 11:29:46 DP0 TP0] Prefill batch, #new-seq: 10, #new-token: 277, #cached-token: 6964, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-28 11:29:46 DP3 TP3] Prefill batch, #new-seq: 11, #new-token: 248, #cached-token: 7663, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-28 11:29:46 DP5 TP5] Prefill batch, #new-seq: 10, #new-token: 385, #cached-token: 6893, token usage: 0.01, #running-req: 59, #queue-req: 0, 
[2025-10-28 11:29:47 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 59, #cached-token: 5680, token usage: 0.01, #running-req: 69, #queue-req: 0, 
[2025-10-28 11:29:47 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 74, #cached-token: 5711, token usage: 0.01, #running-req: 70, #queue-req: 0, 
[2025-10-28 11:29:47 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 203, #cached-token: 6390, token usage: 0.01, #running-req: 70, #queue-req: 0, 
[2025-10-28 11:29:47 DP3 TP3] Prefill batch, #new-seq: 8, #new-token: 72, #cached-token: 5750, token usage: 0.01, #running-req: 70, #queue-req: 0, 
[2025-10-28 11:29:47 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 69, #cached-token: 5801, token usage: 0.01, #running-req: 69, #queue-req: 0, 
[2025-10-28 11:29:47 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 212, #cached-token: 6346, token usage: 0.01, #running-req: 70, #queue-req: 0, 
[2025-10-28 11:29:47 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 88, #cached-token: 5699, token usage: 0.01, #running-req: 69, #queue-req: 0, 
[2025-10-28 11:29:47 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 51, #cached-token: 6600, token usage: 0.01, #running-req: 69, #queue-req: 0, 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP3 TP3] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP0 TP0] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP7 TP7] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP4 TP4] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP1 TP1] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP6 TP6] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP2 TP2] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP5 TP5] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP4 TP4] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7259, token usage: 0.01, #running-req: 78, #queue-req: 0, 
[2025-10-28 11:29:47 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 60, #cached-token: 6512, token usage: 0.01, #running-req: 79, #queue-req: 0, 
[2025-10-28 11:29:47 DP6 TP6] Prefill batch, #new-seq: 11, #new-token: 155, #cached-token: 7987, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-28 11:29:47 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6438, token usage: 0.01, #running-req: 78, #queue-req: 0, 
[2025-10-28 11:29:47 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 133, #cached-token: 6489, token usage: 0.01, #running-req: 79, #queue-req: 0, 
[2025-10-28 11:29:47 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 111, #cached-token: 7144, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[2025-10-28 11:29:47 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 94, #cached-token: 7250, token usage: 0.01, #running-req: 78, #queue-req: 0, 
[2025-10-28 11:29:47 DP5 TP5] Prefill batch, #new-seq: 11, #new-token: 55, #cached-token: 7913, token usage: 0.01, #running-req: 77, #queue-req: 0, 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP2 TP2] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP6 TP6] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP4 TP4] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP5 TP5] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP0 TP0] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP3 TP3] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP7 TP7] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP1 TP1] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5059, token usage: 0.01, #running-req: 88, #queue-req: 0, 
[2025-10-28 11:29:47 DP6 TP6] Prefill batch, #new-seq: 6, #new-token: 88, #cached-token: 4234, token usage: 0.01, #running-req: 88, #queue-req: 0, 
[2025-10-28 11:29:47 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 42, #cached-token: 5072, token usage: 0.01, #running-req: 88, #queue-req: 0, 
[2025-10-28 11:29:47 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 220, #cached-token: 4917, token usage: 0.01, #running-req: 87, #queue-req: 0, 
[2025-10-28 11:29:47 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 84, #cached-token: 4964, token usage: 0.01, #running-req: 88, #queue-req: 0, 
[2025-10-28 11:29:47 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5035, token usage: 0.01, #running-req: 88, #queue-req: 0, 
[2025-10-28 11:29:47 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5165, token usage: 0.01, #running-req: 88, #queue-req: 0, 
[2025-10-28 11:29:47 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 83, #cached-token: 5077, token usage: 0.01, #running-req: 87, #queue-req: 0, 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP4 TP4] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP6 TP6] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP5 TP5] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP2 TP2] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP7 TP7] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP0 TP0] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP3 TP3] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP1 TP1] [fused_moe] using default for (538, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP4 TP4] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4381, token usage: 0.01, #running-req: 95, #queue-req: 0, 
[2025-10-28 11:29:47 DP2 TP2] Prefill batch, #new-seq: 6, #new-token: 75, #cached-token: 4240, token usage: 0.01, #running-req: 95, #queue-req: 0, 
[2025-10-28 11:29:47 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5121, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[2025-10-28 11:29:47 DP3 TP3] Prefill batch, #new-seq: 6, #new-token: 112, #cached-token: 4317, token usage: 0.01, #running-req: 95, #queue-req: 0, 
[2025-10-28 11:29:47 DP5 TP5] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4401, token usage: 0.01, #running-req: 95, #queue-req: 0, 
[2025-10-28 11:29:47 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 157, #cached-token: 4919, token usage: 0.01, #running-req: 95, #queue-req: 0, 
[2025-10-28 11:29:47 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5104, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[2025-10-28 11:29:47 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5085, token usage: 0.01, #running-req: 94, #queue-req: 0, 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP3 TP3] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP2 TP2] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP5 TP5] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP4 TP4] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP6 TP6] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP0 TP0] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP7 TP7] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP1 TP1] [fused_moe] using default for (377, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 54, #cached-token: 5011, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-28 11:29:47 DP6 TP6] Prefill batch, #new-seq: 6, #new-token: 161, #cached-token: 4228, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-28 11:29:47 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 62, #cached-token: 5022, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-28 11:29:47 DP7 TP7] Prefill batch, #new-seq: 6, #new-token: 82, #cached-token: 4366, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-28 11:29:47 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4340, token usage: 0.01, #running-req: 102, #queue-req: 0, 
[2025-10-28 11:29:47 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 216, #cached-token: 4957, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-28 11:29:47 DP0 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4410, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[2025-10-28 11:29:47 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 91, #cached-token: 4994, token usage: 0.01, #running-req: 101, #queue-req: 0, 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP3 TP3] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP2 TP2] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP5 TP5] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP0 TP0] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP1 TP1] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP6 TP6] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP7 TP7] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP4 TP4] [fused_moe] using default for (678, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP5 TP5] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4387, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-28 11:29:47 DP3 TP3] Prefill batch, #new-seq: 6, #new-token: 49, #cached-token: 4334, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-28 11:29:47 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 102, #cached-token: 4928, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-28 11:29:47 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 131, #cached-token: 4962, token usage: 0.01, #running-req: 107, #queue-req: 0, 
[2025-10-28 11:29:47 DP4 TP4] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4341, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-28 11:29:47 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 71, #cached-token: 4982, token usage: 0.01, #running-req: 107, #queue-req: 0, 
[2025-10-28 11:29:47 DP2 TP2] Prefill batch, #new-seq: 6, #new-token: 40, #cached-token: 4246, token usage: 0.01, #running-req: 108, #queue-req: 0, 
[2025-10-28 11:29:47 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 127, #cached-token: 4942, token usage: 0.01, #running-req: 107, #queue-req: 0, 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP3 TP3] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP4 TP4] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP6 TP6] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP0 TP0] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP1 TP1] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP7 TP7] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP5 TP5] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP2 TP2] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:47 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 99, #cached-token: 5059, token usage: 0.01, #running-req: 114, #queue-req: 0, 
[2025-10-28 11:29:47 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 238, #cached-token: 4868, token usage: 0.01, #running-req: 114, #queue-req: 0, 
[2025-10-28 11:29:47 DP2 TP2] Prefill batch, #new-seq: 7, #new-token: 42, #cached-token: 5085, token usage: 0.01, #running-req: 114, #queue-req: 0, 
[2025-10-28 11:29:47 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 171, #cached-token: 5028, token usage: 0.01, #running-req: 114, #queue-req: 0, 
[2025-10-28 11:29:47 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 91, #cached-token: 4961, token usage: 0.01, #running-req: 114, #queue-req: 0, 
[2025-10-28 11:29:47 DP0 TP0] Prefill batch, #new-seq: 6, #new-token: 124, #cached-token: 4370, token usage: 0.01, #running-req: 114, #queue-req: 0, 
[2025-10-28 11:29:47 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 258, #cached-token: 4223, token usage: 0.01, #running-req: 115, #queue-req: 0, 
[2025-10-28 11:29:47 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5160, token usage: 0.01, #running-req: 114, #queue-req: 0, 
[2025-10-28 11:29:47 DP4 TP4] Prefill batch, #new-seq: 6, #new-token: 184, #cached-token: 4164, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-28 11:29:47 DP6 TP6] Prefill batch, #new-seq: 5, #new-token: 128, #cached-token: 3566, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-28 11:29:47 DP2 TP2] Prefill batch, #new-seq: 6, #new-token: 309, #cached-token: 4120, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-28 11:29:47 DP7 TP7] Prefill batch, #new-seq: 5, #new-token: 45, #cached-token: 3572, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-28 11:29:47 DP3 TP3] Prefill batch, #new-seq: 6, #new-token: 130, #cached-token: 4276, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-28 11:29:47 DP5 TP5] Prefill batch, #new-seq: 6, #new-token: 124, #cached-token: 4400, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-28 11:29:47 DP0 TP0] Prefill batch, #new-seq: 7, #new-token: 317, #cached-token: 4856, token usage: 0.01, #running-req: 120, #queue-req: 0, 
[2025-10-28 11:29:47 DP1 TP1] Prefill batch, #new-seq: 7, #new-token: 244, #cached-token: 4890, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-28 11:29:48 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 229, #cached-token: 6359, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-28 11:29:48 DP2 TP2] Prefill batch, #new-seq: 9, #new-token: 263, #cached-token: 6223, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-28 11:29:48 DP6 TP6] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7424, token usage: 0.01, #running-req: 126, #queue-req: 0, 
[2025-10-28 11:29:48 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 207, #cached-token: 6367, token usage: 0.01, #running-req: 128, #queue-req: 0, 
[2025-10-28 11:29:48 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 175, #cached-token: 6447, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-28 11:29:48 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 179, #cached-token: 6307, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-28 11:29:48 DP3 TP3] Prefill batch, #new-seq: 9, #new-token: 286, #cached-token: 6274, token usage: 0.01, #running-req: 127, #queue-req: 0, 
[2025-10-28 11:29:48 DP7 TP7] Prefill batch, #new-seq: 10, #new-token: 181, #cached-token: 7172, token usage: 0.01, #running-req: 126, #queue-req: 0, 
[2025-10-28 11:29:48 DP6 TP6] Prefill batch, #new-seq: 9, #new-token: 42, #cached-token: 6427, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-28 11:29:48 DP4 TP4] Prefill batch, #new-seq: 9, #new-token: 128, #cached-token: 6442, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-28 11:29:48 DP2 TP2] Prefill batch, #new-seq: 10, #new-token: 134, #cached-token: 7126, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-28 11:29:48 DP5 TP5] Prefill batch, #new-seq: 9, #new-token: 100, #cached-token: 6384, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-28 11:29:48 DP7 TP7] Prefill batch, #new-seq: 9, #new-token: 127, #cached-token: 6516, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-28 11:29:48 DP1 TP1] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6441, token usage: 0.01, #running-req: 137, #queue-req: 0, 
[2025-10-28 11:29:48 DP0 TP0] Prefill batch, #new-seq: 9, #new-token: 79, #cached-token: 6500, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[2025-10-28 11:29:48 DP3 TP3] Prefill batch, #new-seq: 10, #new-token: 49, #cached-token: 7201, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP4 TP4] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP3 TP3] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP2 TP2] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP5 TP5] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP0 TP0] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP1 TP1] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP6 TP6] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP7 TP7] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP4 TP4] Prefill batch, #new-seq: 8, #new-token: 152, #cached-token: 5693, token usage: 0.01, #running-req: 145, #queue-req: 0, 
[2025-10-28 11:29:48 DP6 TP6] Prefill batch, #new-seq: 8, #new-token: 71, #cached-token: 5744, token usage: 0.01, #running-req: 145, #queue-req: 0, 
[2025-10-28 11:29:48 DP7 TP7] Prefill batch, #new-seq: 8, #new-token: 100, #cached-token: 5868, token usage: 0.01, #running-req: 145, #queue-req: 0, 
[2025-10-28 11:29:48 DP1 TP1] Prefill batch, #new-seq: 8, #new-token: 166, #cached-token: 5727, token usage: 0.01, #running-req: 146, #queue-req: 0, 
[2025-10-28 11:29:48 DP5 TP5] Prefill batch, #new-seq: 8, #new-token: 177, #cached-token: 5628, token usage: 0.01, #running-req: 145, #queue-req: 0, 
[2025-10-28 11:29:48 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 95, #cached-token: 5116, token usage: 0.01, #running-req: 146, #queue-req: 0, 
[2025-10-28 11:29:48 DP0 TP0] Prefill batch, #new-seq: 8, #new-token: 46, #cached-token: 5888, token usage: 0.02, #running-req: 145, #queue-req: 0, 
[2025-10-28 11:29:48 DP2 TP2] Prefill batch, #new-seq: 8, #new-token: 62, #cached-token: 5737, token usage: 0.01, #running-req: 146, #queue-req: 0, 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP4 TP4] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP0 TP0] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP3 TP3] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP7 TP7] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP6 TP6] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP1 TP1] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP2 TP2] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP5 TP5] [fused_moe] using default for (869, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP4 TP4] Prefill batch, #new-seq: 7, #new-token: 124, #cached-token: 4948, token usage: 0.01, #running-req: 153, #queue-req: 0, 
[2025-10-28 11:29:48 DP2 TP2] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4367, token usage: 0.02, #running-req: 154, #queue-req: 0, 
[2025-10-28 11:29:48 DP6 TP6] Prefill batch, #new-seq: 7, #new-token: 162, #cached-token: 4945, token usage: 0.02, #running-req: 153, #queue-req: 0, 
[2025-10-28 11:29:48 DP0 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4394, token usage: 0.02, #running-req: 153, #queue-req: 0, 
[2025-10-28 11:29:48 DP1 TP1] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4549, token usage: 0.01, #running-req: 154, #queue-req: 0, 
[2025-10-28 11:29:48 DP5 TP5] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5118, token usage: 0.02, #running-req: 153, #queue-req: 0, 
[2025-10-28 11:29:48 DP3 TP3] Prefill batch, #new-seq: 7, #new-token: 60, #cached-token: 5021, token usage: 0.01, #running-req: 153, #queue-req: 0, 
[2025-10-28 11:29:48 DP7 TP7] Prefill batch, #new-seq: 7, #new-token: 87, #cached-token: 5037, token usage: 0.02, #running-req: 153, #queue-req: 0, 
[aiter] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP0 TP0] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP7 TP7] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP3 TP3] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP6 TP6] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP5 TP5] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP2 TP2] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP1 TP1] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP4 TP4] [fused_moe] using default for (458, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP4 TP4] Prefill batch, #new-seq: 5, #new-token: 49, #cached-token: 3544, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-28 11:29:48 DP2 TP2] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3653, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-28 11:29:48 DP6 TP6] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3610, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-28 11:29:48 DP1 TP1] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3624, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-28 11:29:48 DP5 TP5] Prefill batch, #new-seq: 5, #new-token: 41, #cached-token: 3616, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-28 11:29:48 DP3 TP3] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3678, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-28 11:29:48 DP7 TP7] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3669, token usage: 0.02, #running-req: 160, #queue-req: 0, 
[2025-10-28 11:29:48 DP0 TP0] Prefill batch, #new-seq: 5, #new-token: 46, #cached-token: 3613, token usage: 0.02, #running-req: 159, #queue-req: 0, 
[aiter] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP3 TP3] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP0 TP0] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP2 TP2] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP4 TP4] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP6 TP6] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP5 TP5] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP7 TP7] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:48 DP1 TP1] [fused_moe] using default for (161, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-28 11:29:49 DP5 TP5] Decode batch, #running-req: 165, #token: 11914, token usage: 0.02, cuda graph: True, gen throughput (token/s): 51.74, #queue-req: 0, 
[2025-10-28 11:29:49 DP4 TP4] Decode batch, #running-req: 165, #token: 12462, token usage: 0.02, cuda graph: True, gen throughput (token/s): 104.05, #queue-req: 0, 
[2025-10-28 11:29:50 DP1 TP1] Decode batch, #running-req: 165, #token: 13678, token usage: 0.02, cuda graph: True, gen throughput (token/s): 158.10, #queue-req: 0, 
[2025-10-28 11:29:50 DP3 TP3] Decode batch, #running-req: 165, #token: 13879, token usage: 0.02, cuda graph: True, gen throughput (token/s): 141.89, #queue-req: 0, 
[2025-10-28 11:29:51 DP0 TP0] Decode batch, #running-req: 164, #token: 15092, token usage: 0.02, cuda graph: True, gen throughput (token/s): 162.69, #queue-req: 0, 
[2025-10-28 11:29:51] INFO:     127.0.0.1:51334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:51 DP2 TP2] Decode batch, #running-req: 165, #token: 15101, token usage: 0.02, cuda graph: True, gen throughput (token/s): 179.91, #queue-req: 0, 
[2025-10-28 11:29:52] INFO:     127.0.0.1:51416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:52 DP7 TP7] Decode batch, #running-req: 164, #token: 16558, token usage: 0.02, cuda graph: True, gen throughput (token/s): 241.06, #queue-req: 0, 
[2025-10-28 11:29:52] INFO:     127.0.0.1:59754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53 DP6 TP6] Decode batch, #running-req: 164, #token: 17334, token usage: 0.03, cuda graph: True, gen throughput (token/s): 242.97, #queue-req: 0, 
[2025-10-28 11:29:53] INFO:     127.0.0.1:53116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:59914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:52838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:54242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:50794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:51186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:58706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:49304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:52092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:56974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:49330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:58222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:52796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:48786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:48848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:53256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:48656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:49460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:53036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:53192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:53422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:51786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:54032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:57106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:51674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:50906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:53] INFO:     127.0.0.1:60076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:59266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54 DP5 TP5] Decode batch, #running-req: 159, #token: 17981, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1379.42, #queue-req: 0, 
[2025-10-28 11:29:54] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:56006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:48790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:48922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:55694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:49950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:53178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:54334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:48312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:55798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:50542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:51318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:54798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:56336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:58688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:50746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:55562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:52574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:49210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:53428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:52688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:53208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:50978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54 DP4 TP4] Decode batch, #running-req: 150, #token: 17161, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1362.41, #queue-req: 0, 
[2025-10-28 11:29:54] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:56430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:58388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:50760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:57744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:50142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:51780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:58326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:49390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:49772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:50050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:51154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:52078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:54] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:49974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:49392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:51994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:59146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:50352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:48686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:58400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:57242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:50088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:58908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:50316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:51874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:48296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:50396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:51024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:49428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:51446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:59030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:51796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:52916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:55708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:59400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:50732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55 DP1 TP1] Decode batch, #running-req: 139, #token: 17246, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1341.77, #queue-req: 0, 
[2025-10-28 11:29:55] INFO:     127.0.0.1:55086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:50246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:59780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55 DP3 TP3] Decode batch, #running-req: 148, #token: 18396, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1357.49, #queue-req: 0, 
[2025-10-28 11:29:55] INFO:     127.0.0.1:54212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:52770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:58802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:56358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:51954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:53496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:55212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:55812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:49408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:52788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:50790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:59690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:51978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:48460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:48986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:58276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:48970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:55142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:55462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:59768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55 DP0 TP0] Decode batch, #running-req: 141, #token: 18728, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1341.75, #queue-req: 0, 
[2025-10-28 11:29:55] INFO:     127.0.0.1:48798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:51254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:56620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:56806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:55912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:53362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:54530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:55838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:58620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:51030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:55] INFO:     127.0.0.1:60036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:48832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:51552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:56294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:56788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:48774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:50690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:53014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:51850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:52226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:48628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:49820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:49596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:49578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:56510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:51080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56 DP2 TP2] Decode batch, #running-req: 136, #token: 17960, token usage: 0.03, cuda graph: True, gen throughput (token/s): 1369.23, #queue-req: 0, 
[2025-10-28 11:29:56] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:49088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:52014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:50286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:50354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:53400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:54832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:51514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:56472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:56280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:50018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:56220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:50768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:56276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:49862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:51886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:53266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:49196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:49296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:54484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:54120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:53854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:51812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:50868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:53680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:52034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:50096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:52534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:48722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:52436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:48614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:54684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:56770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:56324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:50424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:49344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:59292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:48910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:49826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:52668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:54612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:57312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:56] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:49716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:54318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:48508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:51700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:60112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:57176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57 DP7 TP7] Decode batch, #running-req: 113, #token: 16408, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1324.72, #queue-req: 0, 
[2025-10-28 11:29:57] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:60074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:50470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:49052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:52824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:52756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:54342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:58472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:50824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:54564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:58716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:49738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:49318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:48828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:48992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:53804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:51930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:53556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:49558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:58496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:51126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:53788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:50736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:52978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:48740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:49732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:59224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57 DP6 TP6] Decode batch, #running-req: 101, #token: 15139, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1250.06, #queue-req: 0, 
[2025-10-28 11:29:57] INFO:     127.0.0.1:53340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:49936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:57336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:49218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:51162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:50390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:50808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:48478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:49624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:57602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:57168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:51214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:51282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:56154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:54868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:48698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:53138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:50844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:52142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:57] INFO:     127.0.0.1:59702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:51956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:53536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:49256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:48890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:51526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:48356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:54400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:48564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:55822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:49002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:50858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:53816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:53274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:55610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58 DP5 TP5] Decode batch, #running-req: 99, #token: 15653, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1260.20, #queue-req: 0, 
[2025-10-28 11:29:58] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:48408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:49426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:48782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:48620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:50604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:51770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:49186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:48666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:55276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:51456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:53386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:54554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:51724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:55132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:55162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:51916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:49286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:50500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:51384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:49544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58 DP4 TP4] Decode batch, #running-req: 77, #token: 12091, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1159.36, #queue-req: 0, 
[2025-10-28 11:29:58] INFO:     127.0.0.1:58368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:48942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:49668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:53106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:51744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:48996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:49066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:54010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:49586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:54996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:50952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:55552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:48338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:54602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:48568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:52156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:53502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:58] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:53162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:59586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:53318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:53856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:60008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:59800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:59404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:49170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:50816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:50336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:53280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:48768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:49272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:50360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:55260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:51660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:48498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:48548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:51222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:56372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:58056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59 DP1 TP1] Decode batch, #running-req: 68, #token: 12233, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1050.08, #queue-req: 0, 
[2025-10-28 11:29:59] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:58024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:51228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:51466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59 DP3 TP3] Decode batch, #running-req: 81, #token: 13200, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1186.60, #queue-req: 0, 
[2025-10-28 11:29:59] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:58936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:53048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:55586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:51898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:55966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:49926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:59854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:48738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:59960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:49534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:48416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:59310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:51016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:58358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:48756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:49516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:50536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59 DP0 TP0] Decode batch, #running-req: 76, #token: 13491, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1136.93, #queue-req: 0, 
[2025-10-28 11:29:59] INFO:     127.0.0.1:53840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:55484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:56708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:58750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:49332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:56016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:48640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:56288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:54166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:29:59] INFO:     127.0.0.1:56172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:52210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:53996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:53874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:56456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00 DP2 TP2] Decode batch, #running-req: 70, #token: 12354, token usage: 0.02, cuda graph: True, gen throughput (token/s): 1058.48, #queue-req: 0, 
[2025-10-28 11:30:00] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:53030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:48496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:56816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:50650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:53700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:50722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:52586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:48906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:54516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:50782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:52062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:48778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:56990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:58892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:54016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:52392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:53968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:50920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:53904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:56084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:58976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:50558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:50432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:53870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00 DP7 TP7] Decode batch, #running-req: 53, #token: 10240, token usage: 0.02, cuda graph: True, gen throughput (token/s): 936.05, #queue-req: 0, 
[2025-10-28 11:30:00] INFO:     127.0.0.1:54442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:48954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:54382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:50086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:54430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:54036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:50012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:53132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:48574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:59656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:54376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:48430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:49268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:58540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:48732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:55526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:52744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:00] INFO:     127.0.0.1:57904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:54734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01 DP6 TP6] Decode batch, #running-req: 42, #token: 7919, token usage: 0.01, cuda graph: True, gen throughput (token/s): 821.81, #queue-req: 0, 
[2025-10-28 11:30:01] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:59196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:60078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:51306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:51582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:54908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:50154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:58848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:59372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:53070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:50936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:54056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:57272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:59424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:51192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:51546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:58876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:50874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:54538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:51824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:58638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:55894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:54338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:53770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01 DP5 TP5] Decode batch, #running-req: 29, #token: 6315, token usage: 0.01, cuda graph: True, gen throughput (token/s): 754.40, #queue-req: 0, 
[2025-10-28 11:30:01] INFO:     127.0.0.1:53416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:57992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:55054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:56150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:48700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:57308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:53462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:57948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:53354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:54694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:56142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:48326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:59286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01 DP4 TP4] Decode batch, #running-req: 24, #token: 5493, token usage: 0.01, cuda graph: True, gen throughput (token/s): 586.91, #queue-req: 0, 
[2025-10-28 11:30:01] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:53094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:49354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:51646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:53454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:49158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:49118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:54820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:56524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:58862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:53952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:54304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:51980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:54040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:01] INFO:     127.0.0.1:52088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:48552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:54150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:50374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:48926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:50686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:52150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:53722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:57374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:56252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:57940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:52266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:50124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:51106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:51266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02 DP1 TP1] Decode batch, #running-req: 26, #token: 5761, token usage: 0.01, cuda graph: True, gen throughput (token/s): 626.15, #queue-req: 0, 
[2025-10-28 11:30:02] INFO:     127.0.0.1:50946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:55628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:50580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02 DP3 TP3] Decode batch, #running-req: 36, #token: 7988, token usage: 0.01, cuda graph: True, gen throughput (token/s): 766.16, #queue-req: 0, 
[2025-10-28 11:30:02] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:50272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:49082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:53764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:59218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:49570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:48488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:59542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:50656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:52704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:60048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02 DP0 TP0] Decode batch, #running-req: 32, #token: 6990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 715.73, #queue-req: 0, 
[2025-10-28 11:30:02] INFO:     127.0.0.1:49048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:56852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:57814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:56256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:57208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:49502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:56304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02 DP2 TP2] Decode batch, #running-req: 21, #token: 5201, token usage: 0.01, cuda graph: True, gen throughput (token/s): 595.94, #queue-req: 0, 
[2025-10-28 11:30:02] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:51558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:50484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:57064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:02] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:48372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:55228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:54096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:52576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:59630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:50568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:54770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:56964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:50596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:58140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:50266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:59598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:55044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03 DP7 TP7] Decode batch, #running-req: 29, #token: 7594, token usage: 0.01, cuda graph: True, gen throughput (token/s): 617.13, #queue-req: 0, 
[2025-10-28 11:30:03] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:54364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:56866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:57522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:51406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:52066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:51736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:52242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:49500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:51296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:48784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:54226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:55670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:59006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03 DP6 TP6] Decode batch, #running-req: 12, #token: 3694, token usage: 0.01, cuda graph: True, gen throughput (token/s): 375.03, #queue-req: 0, 
[2025-10-28 11:30:03] INFO:     127.0.0.1:59150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:54278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:53694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:55706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:59084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:54696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:57768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:58178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:58166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03 DP5 TP5] Decode batch, #running-req: 11, #token: 3443, token usage: 0.01, cuda graph: True, gen throughput (token/s): 300.15, #queue-req: 0, 
[2025-10-28 11:30:03] INFO:     127.0.0.1:50970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:58412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:50842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:03] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:58902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04 DP4 TP4] Decode batch, #running-req: 8, #token: 2194, token usage: 0.00, cuda graph: True, gen throughput (token/s): 224.00, #queue-req: 0, 
[2025-10-28 11:30:04] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:54012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:59874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:53632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:59446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:58970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:60024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:59480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:53920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:59318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:48982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:59072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04 DP1 TP1] Decode batch, #running-req: 9, #token: 3034, token usage: 0.00, cuda graph: True, gen throughput (token/s): 269.73, #queue-req: 0, 
[2025-10-28 11:30:04] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04 DP3 TP3] Decode batch, #running-req: 11, #token: 3495, token usage: 0.01, cuda graph: True, gen throughput (token/s): 382.67, #queue-req: 0, 
[2025-10-28 11:30:04] INFO:     127.0.0.1:52716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:50306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04 DP0 TP0] Decode batch, #running-req: 9, #token: 3107, token usage: 0.00, cuda graph: True, gen throughput (token/s): 337.90, #queue-req: 0, 
[2025-10-28 11:30:04] INFO:     127.0.0.1:48818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:54776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:51438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04 DP2 TP2] Decode batch, #running-req: 8, #token: 2810, token usage: 0.00, cuda graph: True, gen throughput (token/s): 273.99, #queue-req: 0, 
[2025-10-28 11:30:04] INFO:     127.0.0.1:48600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:49634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:55194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:04] INFO:     127.0.0.1:53598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:54294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:52492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:51090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:48442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:48702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05 DP7 TP7] Decode batch, #running-req: 13, #token: 4424, token usage: 0.01, cuda graph: True, gen throughput (token/s): 422.02, #queue-req: 0, 
[2025-10-28 11:30:05] INFO:     127.0.0.1:52942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:58858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:51140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:57608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05 DP6 TP6] Decode batch, #running-req: 5, #token: 1961, token usage: 0.00, cuda graph: True, gen throughput (token/s): 173.75, #queue-req: 0, 
[2025-10-28 11:30:05] INFO:     127.0.0.1:49246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:50984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:53452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:50000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:55256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:55028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:49792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:52522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05 DP5 TP5] Decode batch, #running-req: 2, #token: 1345, token usage: 0.00, cuda graph: True, gen throughput (token/s): 127.43, #queue-req: 0, 
[2025-10-28 11:30:05] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:58112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05 DP4 TP4] Decode batch, #running-req: 4, #token: 1860, token usage: 0.00, cuda graph: True, gen throughput (token/s): 101.94, #queue-req: 0, 
[2025-10-28 11:30:05] INFO:     127.0.0.1:52320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:05] INFO:     127.0.0.1:49366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06] INFO:     127.0.0.1:58252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06] INFO:     127.0.0.1:49874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06] INFO:     127.0.0.1:54876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06 DP1 TP1] Decode batch, #running-req: 5, #token: 2176, token usage: 0.00, cuda graph: True, gen throughput (token/s): 152.50, #queue-req: 0, 
[2025-10-28 11:30:06 DP3 TP3] Decode batch, #running-req: 3, #token: 1530, token usage: 0.00, cuda graph: True, gen throughput (token/s): 154.05, #queue-req: 0, 
[2025-10-28 11:30:06] INFO:     127.0.0.1:55608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06 DP0 TP0] Decode batch, #running-req: 1, #token: 939, token usage: 0.00, cuda graph: True, gen throughput (token/s): 111.28, #queue-req: 0, 
[2025-10-28 11:30:06 DP2 TP2] Decode batch, #running-req: 3, #token: 1592, token usage: 0.00, cuda graph: True, gen throughput (token/s): 105.74, #queue-req: 0, 
[2025-10-28 11:30:06] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06 DP7 TP7] Decode batch, #running-req: 6, #token: 1950, token usage: 0.00, cuda graph: True, gen throughput (token/s): 226.65, #queue-req: 0, 
[2025-10-28 11:30:06] INFO:     127.0.0.1:55874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06] INFO:     127.0.0.1:49128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:06] INFO:     127.0.0.1:58828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07 DP6 TP6] Decode batch, #running-req: 2, #token: 1367, token usage: 0.00, cuda graph: True, gen throughput (token/s): 73.68, #queue-req: 0, 
[2025-10-28 11:30:07] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07 DP5 TP5] Decode batch, #running-req: 1, #token: 1057, token usage: 0.00, cuda graph: True, gen throughput (token/s): 28.11, #queue-req: 0, 
[2025-10-28 11:30:07] INFO:     127.0.0.1:49376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07] INFO:     127.0.0.1:53754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07] INFO:     127.0.0.1:54990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07] INFO:     127.0.0.1:58878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07] INFO:     127.0.0.1:49528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07 DP1 TP1] Decode batch, #running-req: 2, #token: 1365, token usage: 0.00, cuda graph: True, gen throughput (token/s): 123.82, #queue-req: 0, 
[2025-10-28 11:30:07 DP3 TP3] Decode batch, #running-req: 1, #token: 1001, token usage: 0.00, cuda graph: True, gen throughput (token/s): 56.34, #queue-req: 0, 
[2025-10-28 11:30:07] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:07] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:08] INFO:     127.0.0.1:55408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:08 DP5 TP5] Decode batch, #running-req: 1, #token: 1097, token usage: 0.00, cuda graph: True, gen throughput (token/s): 33.92, #queue-req: 0, 
[2025-10-28 11:30:08] INFO:     127.0.0.1:57364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:08] INFO:     127.0.0.1:57328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-28 11:30:13] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-28 11:30:18] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
