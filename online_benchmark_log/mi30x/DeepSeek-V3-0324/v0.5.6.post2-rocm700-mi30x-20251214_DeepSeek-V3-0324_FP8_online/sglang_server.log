[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-14 10:00:39] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
INFO 12-14 10:00:40 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-14 10:00:41] WARNING server_args.py:1428: Attention backend not explicitly specified. Use aiter backend by default.
[2025-12-14 10:00:41] server_args=ServerArgs(model_path='/data/models/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/data/models/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.765, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=777529231, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key=None, served_model_name='/data/models/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', enable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2025-12-14 10:00:41] Using default HuggingFace chat template with detected content format: string
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-14 10:00:48] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-14 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-14 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-14 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-14 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-14 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-14 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-14 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[aiter] import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
[2025-12-14 10:00:49] INFO core.py:477: import [module_aiter_enum] under /sgl-workspace/aiter/aiter/jit/module_aiter_enum.so
INFO 12-14 10:00:49 [__init__.py:241] Automatically detected platform rocm.
INFO 12-14 10:00:49 [__init__.py:241] Automatically detected platform rocm.
INFO 12-14 10:00:49 [__init__.py:241] Automatically detected platform rocm.
INFO 12-14 10:00:50 [__init__.py:241] Automatically detected platform rocm.
INFO 12-14 10:00:50 [__init__.py:241] Automatically detected platform rocm.
INFO 12-14 10:00:50 [__init__.py:241] Automatically detected platform rocm.
INFO 12-14 10:00:50 [__init__.py:241] Automatically detected platform rocm.
INFO 12-14 10:00:50 [__init__.py:241] Automatically detected platform rocm.
INFO 12-14 10:00:50 [__init__.py:241] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-14 10:00:51 TP7] Process 224 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-12-14 10:00:51 TP6] Process 223 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-14 10:00:51 TP1] Process 218 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-14 10:00:51 TP2] Process 219 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
[2025-12-14 10:00:51 TP7] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:71: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-12-14 10:00:51 TP4] Process 221 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-14 10:00:51 TP5] Process 222 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-12-14 10:00:51 TP3] Process 220 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-12-14 10:00:51 TP6] Init torch distributed begin.
[2025-12-14 10:00:51 TP0] Process 217 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2025-12-14 10:00:51 TP1] Init torch distributed begin.
[2025-12-14 10:00:51 TP2] Init torch distributed begin.
[2025-12-14 10:00:51 TP4] Init torch distributed begin.
[2025-12-14 10:00:51 TP5] Init torch distributed begin.
[2025-12-14 10:00:51 TP3] Init torch distributed begin.
[2025-12-14 10:00:51 TP0] Init torch distributed begin.
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-12-14 10:00:52 TP0] sglang is using nccl==2.26.6
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-14 10:00:59 TP0] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-14 10:00:59 TP3] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-14 10:00:59 TP2] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-14 10:00:59 TP1] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-14 10:00:59 TP7] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-14 10:00:59 TP4] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-14 10:00:59 TP6] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[aiter] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-14 10:00:59 TP5] import [module_custom_all_reduce] under /sgl-workspace/aiter/aiter/jit/module_custom_all_reduce.so
[2025-12-14 10:00:59 TP0] Using AiterCustomAllreduce for ROCm.
[2025-12-14 10:00:59 TP2] Using AiterCustomAllreduce for ROCm.
[2025-12-14 10:00:59 TP3] Using AiterCustomAllreduce for ROCm.
[2025-12-14 10:00:59 TP7] Using AiterCustomAllreduce for ROCm.
[2025-12-14 10:00:59 TP1] Using AiterCustomAllreduce for ROCm.
[2025-12-14 10:00:59 TP6] Using AiterCustomAllreduce for ROCm.
[2025-12-14 10:00:59 TP4] Using AiterCustomAllreduce for ROCm.
[2025-12-14 10:00:59 TP5] Using AiterCustomAllreduce for ROCm.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7
[2025-12-14 10:01:00 TP7] Init torch distributed ends. mem usage=3.23 GB
[2025-12-14 10:01:00 TP0] Init torch distributed ends. mem usage=2.95 GB
[2025-12-14 10:01:00 TP6] Init torch distributed ends. mem usage=3.24 GB
[2025-12-14 10:01:00 TP5] Init torch distributed ends. mem usage=3.22 GB
[2025-12-14 10:01:00 TP4] Init torch distributed ends. mem usage=3.31 GB
[2025-12-14 10:01:00 TP3] Init torch distributed ends. mem usage=3.36 GB
[2025-12-14 10:01:00 TP2] Init torch distributed ends. mem usage=3.37 GB
[2025-12-14 10:01:00 TP1] Init torch distributed ends. mem usage=3.37 GB
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
/opt/venv/lib/python3.10/site-packages/apex/transformer/functional/fused_rope.py:49: UserWarning: Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0
  warnings.warn("Aiter backend is selected for fused RoPE. This has lower precision. To disable aiter, export USE_ROCM_AITER_ROPE_BACKEND=0", UserWarning)
[2025-12-14 10:01:00 TP3] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-14 10:01:00 TP6] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-14 10:01:00 TP2] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-14 10:01:00 TP7] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-14 10:01:00 TP1] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-14 10:01:00 TP4] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-14 10:01:00 TP5] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-14 10:01:00 TP0] Ignore import error when loading sglang.srt.models.mindspore: name 'ms' is not defined
[2025-12-14 10:01:01 TP7] Load weight begin. avail mem=188.20 GB
[2025-12-14 10:01:01 TP6] Load weight begin. avail mem=188.19 GB
[2025-12-14 10:01:01 TP3] Load weight begin. avail mem=188.07 GB
[2025-12-14 10:01:01 TP0] Load weight begin. avail mem=188.48 GB
[2025-12-14 10:01:01 TP0] Detected fp8 checkpoint.
[2025-12-14 10:01:01 TP2] Load weight begin. avail mem=188.06 GB
[2025-12-14 10:01:01 TP1] Load weight begin. avail mem=188.06 GB
[2025-12-14 10:01:01 TP4] Load weight begin. avail mem=188.12 GB
[2025-12-14 10:01:01 TP5] Load weight begin. avail mem=188.21 GB
[2025-12-14 10:01:01 TP0] Shared experts fusion optimization enabled.
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:17,  9.40it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:11, 13.37it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:10, 15.69it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:00<00:08, 17.90it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:00<00:10, 14.50it/s]
Loading safetensors checkpoint shards:   7% Completed | 12/163 [00:00<00:10, 14.26it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:00<00:09, 15.69it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:01<00:09, 15.82it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:01<00:10, 13.65it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:01<00:10, 13.69it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:01<00:18,  7.62it/s]
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:02<00:16,  8.22it/s]
Loading safetensors checkpoint shards:  17% Completed | 27/163 [00:02<00:14,  9.24it/s]
Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:02<00:13,  9.77it/s]
Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:02<00:14,  9.27it/s]
Loading safetensors checkpoint shards:  20% Completed | 33/163 [00:02<00:12, 10.04it/s]
Loading safetensors checkpoint shards:  21% Completed | 35/163 [00:03<00:12,  9.91it/s]
Loading safetensors checkpoint shards:  23% Completed | 37/163 [00:03<00:12, 10.20it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:03<00:11, 10.45it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:03<00:11, 10.21it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:03<00:11, 10.75it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:04<00:11, 10.06it/s]
Loading safetensors checkpoint shards:  29% Completed | 47/163 [00:04<00:10, 11.23it/s]
Loading safetensors checkpoint shards:  30% Completed | 49/163 [00:04<00:09, 12.36it/s]
Loading safetensors checkpoint shards:  31% Completed | 51/163 [00:04<00:16,  6.76it/s]
Loading safetensors checkpoint shards:  33% Completed | 53/163 [00:05<00:13,  8.19it/s]
Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:05<00:11,  9.47it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:05<00:09, 10.62it/s]
Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:05<00:09, 11.32it/s]
Loading safetensors checkpoint shards:  37% Completed | 61/163 [00:05<00:09, 10.35it/s]
Loading safetensors checkpoint shards:  39% Completed | 63/163 [00:05<00:09, 10.91it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:06<00:08, 12.21it/s]
Loading safetensors checkpoint shards:  41% Completed | 67/163 [00:06<00:08, 11.48it/s]
Loading safetensors checkpoint shards:  42% Completed | 69/163 [00:06<00:08, 11.63it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:06<00:07, 12.57it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:06<00:07, 11.38it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:06<00:07, 12.25it/s]
Loading safetensors checkpoint shards:  47% Completed | 77/163 [00:06<00:06, 13.25it/s]
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:07<00:05, 15.34it/s]
Loading safetensors checkpoint shards:  50% Completed | 82/163 [00:07<00:05, 15.34it/s]
Loading safetensors checkpoint shards:  52% Completed | 84/163 [00:07<00:05, 15.78it/s]
Loading safetensors checkpoint shards:  53% Completed | 86/163 [00:08<00:11,  6.50it/s]
Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:08<00:09,  7.93it/s]
Loading safetensors checkpoint shards:  56% Completed | 91/163 [00:08<00:07, 10.26it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:08<00:06, 11.11it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:08<00:05, 12.22it/s]
Loading safetensors checkpoint shards:  61% Completed | 99/163 [00:08<00:04, 14.24it/s]
Loading safetensors checkpoint shards:  62% Completed | 101/163 [00:08<00:04, 15.21it/s]
Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:09<00:03, 15.07it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:09<00:03, 14.76it/s]
Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:09<00:03, 15.18it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:09<00:04, 13.01it/s]
Loading safetensors checkpoint shards:  68% Completed | 111/163 [00:09<00:03, 13.84it/s]
Loading safetensors checkpoint shards:  69% Completed | 113/163 [00:09<00:03, 13.88it/s]
Loading safetensors checkpoint shards:  71% Completed | 115/163 [00:10<00:03, 13.00it/s]
Loading safetensors checkpoint shards:  72% Completed | 118/163 [00:10<00:02, 15.20it/s]
Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:10<00:02, 15.15it/s]
Loading safetensors checkpoint shards:  75% Completed | 122/163 [00:10<00:02, 16.16it/s]
Loading safetensors checkpoint shards:  76% Completed | 124/163 [00:10<00:02, 15.67it/s]
Loading safetensors checkpoint shards:  77% Completed | 126/163 [00:10<00:02, 16.45it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:10<00:01, 19.51it/s]
Loading safetensors checkpoint shards:  81% Completed | 132/163 [00:11<00:04,  7.60it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:11<00:02,  9.82it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:11<00:02, 11.80it/s]
Loading safetensors checkpoint shards:  86% Completed | 140/163 [00:12<00:01, 12.23it/s]
Loading safetensors checkpoint shards:  88% Completed | 143/163 [00:12<00:01, 14.52it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:12<00:01, 14.57it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:12<00:01, 14.35it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:12<00:01, 12.62it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:12<00:00, 13.31it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:12<00:00, 12.25it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:13<00:00, 12.00it/s]
Loading safetensors checkpoint shards:  96% Completed | 157/163 [00:13<00:00, 13.21it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:13<00:00, 13.47it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:13<00:00, 14.72it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:13<00:00, 13.08it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:13<00:00, 11.89it/s]

[2025-12-14 10:02:41 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.85 GB, mem usage=79.63 GB.
[2025-12-14 10:02:41 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.44 GB, mem usage=79.63 GB.
[2025-12-14 10:02:41 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.44 GB, mem usage=79.63 GB.
[2025-12-14 10:02:41 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.57 GB, mem usage=79.63 GB.
[2025-12-14 10:02:41 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.56 GB, mem usage=79.63 GB.
[2025-12-14 10:02:41 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.49 GB, mem usage=79.63 GB.
[2025-12-14 10:02:41 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.58 GB, mem usage=79.63 GB.
[2025-12-14 10:02:41 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.43 GB, mem usage=79.63 GB.
[2025-12-14 10:02:41 TP0] Using KV cache dtype: torch.bfloat16
[2025-12-14 10:02:41 TP7] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-14 10:02:41 TP7] Memory pool end. avail mem=43.61 GB
[2025-12-14 10:02:41 TP0] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-14 10:02:41 TP3] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-14 10:02:41 TP0] Memory pool end. avail mem=43.89 GB
[2025-12-14 10:02:41 TP3] Memory pool end. avail mem=43.48 GB
[2025-12-14 10:02:41 TP1] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-14 10:02:41 TP1] Memory pool end. avail mem=43.47 GB
[2025-12-14 10:02:41 TP2] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-14 10:02:41 TP2] Memory pool end. avail mem=43.47 GB
[2025-12-14 10:02:41 TP5] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-14 10:02:41 TP5] Memory pool end. avail mem=43.61 GB
[2025-12-14 10:02:41 TP4] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-14 10:02:41 TP6] KV Cache is allocated. #tokens: 981350, KV size: 64.23 GB
[2025-12-14 10:02:41 TP4] Memory pool end. avail mem=43.53 GB
[2025-12-14 10:02:41 TP6] Memory pool end. avail mem=43.60 GB
[2025-12-14 10:02:44 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-12-14 10:02:45 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.15 GB
[2025-12-14 10:02:45 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.21 GB
[2025-12-14 10:02:45 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-12-14 10:02:45 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.15 GB
[2025-12-14 10:02:45 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.28 GB
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-14 10:02:46 TP5] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-14 10:02:46 TP5] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-14 10:02:46 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.57 GB
[2025-12-14 10:02:46 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-12-14 10:02:46 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-14 10:02:46 TP4] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-14 10:02:46 TP4] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-14 10:02:46 TP1] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-14 10:02:46 TP1] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.91 GB):   0%|          | 0/52 [00:00<?, ?it/s][aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-14 10:02:46 TP7] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-14 10:02:46 TP7] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-14 10:02:46 TP2] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-14 10:02:46 TP2] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-14 10:02:46 TP6] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-14 10:02:46 TP6] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-14 10:02:47 TP3] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-14 10:02:47 TP3] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[2025-12-14 10:02:47 TP0] import [module_mla_metadata] under /sgl-workspace/aiter/aiter/jit/module_mla_metadata.so
[aiter] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[2025-12-14 10:02:47 TP0] type hints mismatch, override to --> get_mla_metadata_v1(seqlens_qo_indptr: torch.Tensor, seqlens_kv_indptr: torch.Tensor, num_heads_per_head_k: int, num_heads_k: int, is_causal: bool, work_metadata_ptrs: torch.Tensor, work_info_set: torch.Tensor, work_indptr: torch.Tensor, reduce_indptr: torch.Tensor, reduce_final_map: torch.Tensor, reduce_partial_map: torch.Tensor, kv_granularity: int = 16, max_seqlen_qo: int = -1, uni_seqlen_qo: int = -1, fast_mode: bool = True, topk: int = -1, max_split_per_batch: int = -1, intra_batch_mode: bool = False, dtype_q: Optional[torch.dtype] = None, dtype_kv: Optional[torch.dtype] = None) -> None
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-14 10:02:52 TP5] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-14 10:02:52 TP5] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-14 10:02:54 TP2] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-14 10:02:54 TP2] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-14 10:02:55 TP4] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-14 10:02:55 TP4] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-14 10:02:57 TP7] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-14 10:02:57 TP7] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-14 10:02:57 TP5] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-14 10:02:57 TP5] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-14 10:02:57 TP5] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-14 10:02:57 TP5] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-14 10:02:58 TP3] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-14 10:02:58 TP3] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-14 10:02:58 TP5] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-14 10:02:59 TP5] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:02:59 TP5] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-14 10:02:59 TP5] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-14 10:02:59 TP5] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-14 10:02:59 TP2] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-14 10:02:59 TP6] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-14 10:02:59 TP6] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-14 10:03:00 TP2] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-14 10:03:00 TP2] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-14 10:03:00 TP2] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-14 10:03:00 TP4] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-14 10:03:00 TP4] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-14 10:03:00 TP4] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-14 10:03:00 TP4] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-14 10:03:01 TP2] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-14 10:03:01 TP0] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-14 10:03:01 TP0] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-14 10:03:01 TP2] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:01 TP2] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-14 10:03:01 TP2] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-14 10:03:01 TP2] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-14 10:03:01 TP4] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-14 10:03:02 TP4] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:02 TP4] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-14 10:03:02 TP4] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-14 10:03:02 TP4] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-14 10:03:02 TP7] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[2025-12-14 10:03:02 TP1] import [module_rmsnorm] under /sgl-workspace/aiter/aiter/jit/module_rmsnorm.so
[aiter] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[2025-12-14 10:03:02 TP1] import [module_quant] under /sgl-workspace/aiter/aiter/jit/module_quant.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-14 10:03:03 TP7] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-14 10:03:03 TP7] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-14 10:03:03 TP7] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-14 10:03:04 TP3] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-14 10:03:04 TP7] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-14 10:03:04 TP3] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-14 10:03:04 TP3] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-14 10:03:04 TP3] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-14 10:03:04 TP6] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-14 10:03:04 TP7] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:04 TP7] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-14 10:03:04 TP7] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-14 10:03:04 TP7] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-14 10:03:04 TP6] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-14 10:03:04 TP6] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-14 10:03:04 TP6] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-14 10:03:05 TP3] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-14 10:03:05 TP3] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:05 TP3] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-14 10:03:05 TP3] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-14 10:03:05 TP3] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-14 10:03:05 TP6] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-14 10:03:05 TP0] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-14 10:03:06 TP6] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:06 TP6] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-14 10:03:06 TP6] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-14 10:03:06 TP6] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-14 10:03:06 TP0] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-14 10:03:06 TP0] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-14 10:03:06 TP0] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-14 10:03:07 TP0] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-14 10:03:07 TP0] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:07 TP0] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-14 10:03:07 TP0] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-14 10:03:07 TP0] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
[aiter] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[2025-12-14 10:03:08 TP1] import [module_rope_pos_fwd] under /sgl-workspace/aiter/aiter/jit/module_rope_pos_fwd.so
[aiter] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[2025-12-14 10:03:08 TP1] import [module_mla_asm] under /sgl-workspace/aiter/aiter/jit/module_mla_asm.so
[aiter] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[2025-12-14 10:03:08 TP1] type hints mismatch, override to --> mla_decode_stage1_asm_fwd(Q: torch.Tensor, KV: torch.Tensor, qo_indptr: torch.Tensor, kv_indptr: torch.Tensor, kv_page_indices: torch.Tensor, kv_last_page_lens: torch.Tensor, num_kv_splits_indptr: Optional[torch.Tensor], work_meta_data: Optional[torch.Tensor], work_indptr: Optional[torch.Tensor], work_info_set: Optional[torch.Tensor], max_seqlen_q: int, softmax_scale: float, splitData: torch.Tensor, splitLse: torch.Tensor, output: torch.Tensor, q_scale: Optional[torch.Tensor] = None, kv_scale: Optional[torch.Tensor] = None) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_ps.co GetFunction: _ZN5aiter42mla_a16w16_qh16_m16x4_n16x1_coex0_mask1_psE Success
[aiter] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[2025-12-14 10:03:08 TP1] import [module_mla_reduce] under /sgl-workspace/aiter/aiter/jit/module_mla_reduce.so
[aiter] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[2025-12-14 10:03:09 TP1] import [module_moe_asm] under /sgl-workspace/aiter/aiter/jit/module_moe_asm.so
[aiter] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[2025-12-14 10:03:09 TP1] merge tuned file under model_configs/ and configs/ /sgl-workspace/aiter/aiter/configs/tuned_fmoe.csv:/sgl-workspace/aiter/aiter/configs/model_configs/a8w8_blockscale_tuned_fmoe_qwen3_235b.csv
[aiter] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:09 TP1] [fused_moe] using 1stage default for (304, 512, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[2025-12-14 10:03:09 TP1] import [module_moe_sorting] under /sgl-workspace/aiter/aiter/jit/module_moe_sorting.so
[aiter] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[2025-12-14 10:03:09 TP1] type hints mismatch, override to --> fmoe_fp8_blockscale_g1u1(out: torch.Tensor, input: torch.Tensor, gate: torch.Tensor, down: torch.Tensor, sorted_token_ids: torch.Tensor, sorted_weights: torch.Tensor, sorted_expert_ids: torch.Tensor, num_valid_ids: torch.Tensor, topk: int, input_scale: torch.Tensor, fc1_scale: torch.Tensor, fc2_scale: torch.Tensor, kernel_name: str, fc_scale_blkn: int = 128, fc_scale_blkk: int = 128, fc2_smooth_scale: Optional[torch.Tensor] = None, activation: ActivationType = ActivationType.Silu) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942/fmoe/silu/fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256.co GetFunction: _ZN5aiter50fmoe_bf16_blockscaleFp8_g1u1_vs_silu_1tg_ps_32x256E Success
Capturing batches (bs=512 avail_mem=42.91 GB):   2%|         | 1/52 [00:23<20:05, 23.64s/it]Capturing batches (bs=496 avail_mem=40.27 GB):   2%|         | 1/52 [00:23<20:05, 23.64s/it]Capturing batches (bs=496 avail_mem=40.27 GB):   4%|         | 2/52 [00:24<08:22, 10.04s/it]Capturing batches (bs=480 avail_mem=40.27 GB):   4%|         | 2/52 [00:24<08:22, 10.04s/it]Capturing batches (bs=480 avail_mem=40.27 GB):   6%|         | 3/52 [00:24<04:35,  5.63s/it]Capturing batches (bs=464 avail_mem=40.27 GB):   6%|         | 3/52 [00:24<04:35,  5.63s/it]Capturing batches (bs=464 avail_mem=40.27 GB):   8%|         | 4/52 [00:24<02:50,  3.56s/it]Capturing batches (bs=448 avail_mem=40.27 GB):   8%|         | 4/52 [00:24<02:50,  3.56s/it]Capturing batches (bs=448 avail_mem=40.27 GB):  10%|         | 5/52 [00:25<01:53,  2.41s/it]Capturing batches (bs=432 avail_mem=40.27 GB):  10%|         | 5/52 [00:25<01:53,  2.41s/it]Capturing batches (bs=432 avail_mem=40.27 GB):  12%|        | 6/52 [00:25<01:19,  1.72s/it]Capturing batches (bs=416 avail_mem=40.26 GB):  12%|        | 6/52 [00:25<01:19,  1.72s/it]Capturing batches (bs=416 avail_mem=40.26 GB):  13%|        | 7/52 [00:26<00:57,  1.28s/it]Capturing batches (bs=400 avail_mem=40.26 GB):  13%|        | 7/52 [00:26<00:57,  1.28s/it]Capturing batches (bs=400 avail_mem=40.26 GB):  15%|        | 8/52 [00:26<00:43,  1.01it/s]Capturing batches (bs=384 avail_mem=40.26 GB):  15%|        | 8/52 [00:26<00:43,  1.01it/s]Capturing batches (bs=384 avail_mem=40.26 GB):  17%|        | 9/52 [00:28<00:56,  1.32s/it]Capturing batches (bs=368 avail_mem=40.25 GB):  17%|        | 9/52 [00:28<00:56,  1.32s/it]Capturing batches (bs=368 avail_mem=40.25 GB):  19%|        | 10/52 [00:28<00:43,  1.03s/it]Capturing batches (bs=352 avail_mem=40.25 GB):  19%|        | 10/52 [00:28<00:43,  1.03s/it]Capturing batches (bs=352 avail_mem=40.25 GB):  21%|        | 11/52 [00:29<00:34,  1.18it/s]Capturing batches (bs=336 avail_mem=40.24 GB):  21%|        | 11/52 [00:29<00:34,  1.18it/s]Capturing batches (bs=336 avail_mem=40.24 GB):  23%|       | 12/52 [00:29<00:28,  1.43it/s]Capturing batches (bs=320 avail_mem=40.24 GB):  23%|       | 12/52 [00:29<00:28,  1.43it/s]Capturing batches (bs=320 avail_mem=40.24 GB):  25%|       | 13/52 [00:30<00:23,  1.66it/s]Capturing batches (bs=304 avail_mem=40.24 GB):  25%|       | 13/52 [00:30<00:23,  1.66it/s]Capturing batches (bs=304 avail_mem=40.24 GB):  27%|       | 14/52 [00:30<00:20,  1.88it/s]Capturing batches (bs=288 avail_mem=40.24 GB):  27%|       | 14/52 [00:30<00:20,  1.88it/s]Capturing batches (bs=288 avail_mem=40.24 GB):  29%|       | 15/52 [00:30<00:17,  2.08it/s]Capturing batches (bs=272 avail_mem=40.24 GB):  29%|       | 15/52 [00:30<00:17,  2.08it/s]Capturing batches (bs=272 avail_mem=40.24 GB):  31%|       | 16/52 [00:31<00:16,  2.23it/s]Capturing batches (bs=256 avail_mem=40.23 GB):  31%|       | 16/52 [00:31<00:16,  2.23it/s][aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:19 TP2] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:19 TP4] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:19 TP1] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:19 TP0] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:19 TP3] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:19 TP7] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:19 TP5] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:19 TP6] [fused_moe] using 1stage default for (304, 256, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=256 avail_mem=40.23 GB):  33%|      | 17/52 [00:33<00:32,  1.07it/s]Capturing batches (bs=248 avail_mem=40.22 GB):  33%|      | 17/52 [00:33<00:32,  1.07it/s]Capturing batches (bs=248 avail_mem=40.22 GB):  35%|      | 18/52 [00:35<00:42,  1.25s/it]Capturing batches (bs=240 avail_mem=40.21 GB):  35%|      | 18/52 [00:35<00:42,  1.25s/it]Capturing batches (bs=240 avail_mem=40.21 GB):  37%|      | 19/52 [00:35<00:32,  1.02it/s]Capturing batches (bs=232 avail_mem=40.21 GB):  37%|      | 19/52 [00:35<00:32,  1.02it/s]Capturing batches (bs=232 avail_mem=40.21 GB):  38%|      | 20/52 [00:35<00:25,  1.25it/s]Capturing batches (bs=224 avail_mem=40.21 GB):  38%|      | 20/52 [00:35<00:25,  1.25it/s]Capturing batches (bs=224 avail_mem=40.21 GB):  40%|      | 21/52 [00:36<00:20,  1.49it/s]Capturing batches (bs=216 avail_mem=40.20 GB):  40%|      | 21/52 [00:36<00:20,  1.49it/s]Capturing batches (bs=216 avail_mem=40.20 GB):  42%|     | 22/52 [00:36<00:17,  1.72it/s]Capturing batches (bs=208 avail_mem=40.20 GB):  42%|     | 22/52 [00:36<00:17,  1.72it/s]Capturing batches (bs=208 avail_mem=40.20 GB):  44%|     | 23/52 [00:37<00:15,  1.89it/s]Capturing batches (bs=200 avail_mem=40.20 GB):  44%|     | 23/52 [00:37<00:15,  1.89it/s]Capturing batches (bs=200 avail_mem=40.20 GB):  46%|     | 24/52 [00:37<00:13,  2.08it/s]Capturing batches (bs=192 avail_mem=40.20 GB):  46%|     | 24/52 [00:37<00:13,  2.08it/s]Capturing batches (bs=192 avail_mem=40.20 GB):  48%|     | 25/52 [00:37<00:12,  2.23it/s]Capturing batches (bs=184 avail_mem=40.20 GB):  48%|     | 25/52 [00:37<00:12,  2.23it/s]Capturing batches (bs=184 avail_mem=40.20 GB):  50%|     | 26/52 [00:38<00:11,  2.36it/s]Capturing batches (bs=176 avail_mem=40.19 GB):  50%|     | 26/52 [00:38<00:11,  2.36it/s]Capturing batches (bs=176 avail_mem=40.19 GB):  52%|    | 27/52 [00:38<00:10,  2.45it/s]Capturing batches (bs=168 avail_mem=40.19 GB):  52%|    | 27/52 [00:38<00:10,  2.45it/s]Capturing batches (bs=168 avail_mem=40.19 GB):  54%|    | 28/52 [00:38<00:09,  2.52it/s]Capturing batches (bs=160 avail_mem=40.19 GB):  54%|    | 28/52 [00:38<00:09,  2.52it/s]Capturing batches (bs=160 avail_mem=40.19 GB):  56%|    | 29/52 [00:39<00:08,  2.58it/s]Capturing batches (bs=152 avail_mem=40.19 GB):  56%|    | 29/52 [00:39<00:08,  2.58it/s]Capturing batches (bs=152 avail_mem=40.19 GB):  58%|    | 30/52 [00:39<00:08,  2.63it/s]Capturing batches (bs=144 avail_mem=40.19 GB):  58%|    | 30/52 [00:39<00:08,  2.63it/s]Capturing batches (bs=144 avail_mem=40.19 GB):  60%|    | 31/52 [00:40<00:07,  2.65it/s]Capturing batches (bs=136 avail_mem=40.18 GB):  60%|    | 31/52 [00:40<00:07,  2.65it/s]Capturing batches (bs=136 avail_mem=40.18 GB):  62%|   | 32/52 [00:40<00:07,  2.67it/s]Capturing batches (bs=128 avail_mem=40.18 GB):  62%|   | 32/52 [00:40<00:07,  2.67it/s][aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:28 TP4] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:28 TP2] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:28 TP5] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:28 TP7] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:28 TP6] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:28 TP1] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:28 TP3] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:28 TP0] [fused_moe] using 1stage default for (304, 128, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=128 avail_mem=40.18 GB):  63%|   | 33/52 [00:42<00:16,  1.16it/s]Capturing batches (bs=120 avail_mem=40.17 GB):  63%|   | 33/52 [00:42<00:16,  1.16it/s]Capturing batches (bs=120 avail_mem=40.17 GB):  65%|   | 34/52 [00:44<00:21,  1.20s/it]Capturing batches (bs=112 avail_mem=40.16 GB):  65%|   | 34/52 [00:44<00:21,  1.20s/it]Capturing batches (bs=112 avail_mem=40.16 GB):  67%|   | 35/52 [00:44<00:16,  1.05it/s]Capturing batches (bs=104 avail_mem=40.16 GB):  67%|   | 35/52 [00:44<00:16,  1.05it/s]Capturing batches (bs=104 avail_mem=40.16 GB):  69%|   | 36/52 [00:45<00:12,  1.29it/s]Capturing batches (bs=96 avail_mem=40.15 GB):  69%|   | 36/52 [00:45<00:12,  1.29it/s] Capturing batches (bs=96 avail_mem=40.15 GB):  71%|   | 37/52 [00:45<00:09,  1.53it/s]Capturing batches (bs=88 avail_mem=40.15 GB):  71%|   | 37/52 [00:45<00:09,  1.53it/s]Capturing batches (bs=88 avail_mem=40.15 GB):  73%|  | 38/52 [00:45<00:07,  1.76it/s]Capturing batches (bs=80 avail_mem=40.15 GB):  73%|  | 38/52 [00:45<00:07,  1.76it/s]Capturing batches (bs=80 avail_mem=40.15 GB):  75%|  | 39/52 [00:46<00:06,  1.96it/s]Capturing batches (bs=72 avail_mem=40.15 GB):  75%|  | 39/52 [00:46<00:06,  1.96it/s]Capturing batches (bs=72 avail_mem=40.15 GB):  77%|  | 40/52 [00:46<00:05,  2.03it/s]Capturing batches (bs=64 avail_mem=40.14 GB):  77%|  | 40/52 [00:46<00:05,  2.03it/s][aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:33 TP1] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:33 TP0] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:33 TP3] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:33 TP7] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:33 TP2] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:33 TP4] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:33 TP5] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:33 TP6] [fused_moe] using 1stage default for (304, 64, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=64 avail_mem=40.14 GB):  79%|  | 41/52 [00:47<00:05,  2.18it/s]Capturing batches (bs=56 avail_mem=40.14 GB):  79%|  | 41/52 [00:47<00:05,  2.18it/s]Capturing batches (bs=56 avail_mem=40.14 GB):  81%|  | 42/52 [00:47<00:04,  2.32it/s]Capturing batches (bs=48 avail_mem=40.14 GB):  81%|  | 42/52 [00:47<00:04,  2.32it/s]Capturing batches (bs=48 avail_mem=40.14 GB):  83%| | 43/52 [00:47<00:03,  2.43it/s]Capturing batches (bs=40 avail_mem=40.14 GB):  83%| | 43/52 [00:47<00:03,  2.43it/s]Capturing batches (bs=40 avail_mem=40.14 GB):  85%| | 44/52 [00:48<00:03,  2.51it/s]Capturing batches (bs=32 avail_mem=40.13 GB):  85%| | 44/52 [00:48<00:03,  2.51it/s][aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:34 TP4] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:34 TP0] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:34 TP3] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:34 TP7] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:34 TP2] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:34 TP1] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:34 TP5] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:34 TP6] [fused_moe] using 1stage default for (304, 32, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=32 avail_mem=40.13 GB):  87%| | 45/52 [00:48<00:02,  2.57it/s]Capturing batches (bs=24 avail_mem=40.13 GB):  87%| | 45/52 [00:48<00:02,  2.57it/s]Capturing batches (bs=24 avail_mem=40.13 GB):  88%| | 46/52 [00:48<00:02,  2.62it/s]Capturing batches (bs=16 avail_mem=40.13 GB):  88%| | 46/52 [00:48<00:02,  2.62it/s][aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:35 TP2] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:35 TP1] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:35 TP0] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:35 TP7] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:35 TP3] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:35 TP4] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:35 TP5] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:03:35 TP6] [fused_moe] using 1stage default for (304, 16, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
Capturing batches (bs=16 avail_mem=40.13 GB):  90%| | 47/52 [00:49<00:01,  2.65it/s]Capturing batches (bs=12 avail_mem=40.13 GB):  90%| | 47/52 [00:49<00:01,  2.65it/s]Capturing batches (bs=12 avail_mem=40.13 GB):  92%|| 48/52 [00:49<00:01,  2.68it/s]Capturing batches (bs=8 avail_mem=40.13 GB):  92%|| 48/52 [00:49<00:01,  2.68it/s] Capturing batches (bs=8 avail_mem=40.13 GB):  94%|| 49/52 [00:49<00:01,  2.70it/s]Capturing batches (bs=4 avail_mem=40.12 GB):  94%|| 49/52 [00:49<00:01,  2.70it/s]Capturing batches (bs=4 avail_mem=40.12 GB):  96%|| 50/52 [00:50<00:00,  2.71it/s]Capturing batches (bs=2 avail_mem=40.12 GB):  96%|| 50/52 [00:50<00:00,  2.71it/s]Capturing batches (bs=2 avail_mem=40.12 GB):  98%|| 51/52 [00:50<00:00,  2.72it/s]Capturing batches (bs=1 avail_mem=40.12 GB):  98%|| 51/52 [00:50<00:00,  2.72it/s]Capturing batches (bs=1 avail_mem=40.12 GB): 100%|| 52/52 [00:52<00:00,  1.09it/s]Capturing batches (bs=1 avail_mem=40.12 GB): 100%|| 52/52 [00:52<00:00,  1.02s/it]
[aiter] Registering 6396 cuda graph addresses
[2025-12-14 10:03:39 TP7] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-14 10:03:39 TP3] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-14 10:03:39 TP1] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-14 10:03:39 TP5] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-14 10:03:39 TP0] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-14 10:03:39 TP2] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-14 10:03:39 TP6] Registering 6396 cuda graph addresses
[aiter] Registering 6396 cuda graph addresses
[2025-12-14 10:03:39 TP4] Registering 6396 cuda graph addresses
[2025-12-14 10:03:39 TP4] Capture cuda graph end. Time elapsed: 54.59 s. mem usage=3.46 GB. avail mem=39.74 GB.
[2025-12-14 10:03:39 TP2] Capture cuda graph end. Time elapsed: 53.81 s. mem usage=3.46 GB. avail mem=39.68 GB.
[2025-12-14 10:03:39 TP5] Capture cuda graph end. Time elapsed: 54.82 s. mem usage=3.46 GB. avail mem=39.83 GB.
[2025-12-14 10:03:39 TP3] Capture cuda graph end. Time elapsed: 53.62 s. mem usage=3.46 GB. avail mem=39.70 GB.
[2025-12-14 10:03:39 TP7] Capture cuda graph end. Time elapsed: 53.88 s. mem usage=3.46 GB. avail mem=39.82 GB.
[2025-12-14 10:03:39 TP6] Capture cuda graph end. Time elapsed: 53.81 s. mem usage=3.46 GB. avail mem=39.81 GB.
[2025-12-14 10:03:39 TP1] Capture cuda graph end. Time elapsed: 54.76 s. mem usage=3.46 GB. avail mem=39.69 GB.
[2025-12-14 10:03:39 TP0] Capture cuda graph end. Time elapsed: 53.63 s. mem usage=3.46 GB. avail mem=40.11 GB.
[2025-12-14 10:03:40 TP0] max_total_num_tokens=981350, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=40.11 GB
[2025-12-14 10:03:40] INFO:     Started server process [53]
[2025-12-14 10:03:40] INFO:     Waiting for application startup.
[2025-12-14 10:03:40] INFO:     Application startup complete.
[2025-12-14 10:03:40] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-12-14 10:03:41] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-14 10:03:41] INFO:     127.0.0.1:56116 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-12-14 10:03:41 TP0] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:03:42 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:03:42 TP6] start build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:03:42 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:03:42 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:03:42 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:03:42 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:03:42 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:03:42 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:03:43] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-14 10:03:43] INFO:     127.0.0.1:56280 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-12-14 10:03:48] Endpoint '/get_model_info' is deprecated and will be removed in a future version. Please use '/model_info' instead.
[2025-12-14 10:03:48] INFO:     127.0.0.1:56292 - "GET /get_model_info HTTP/1.1" 200 OK
[aiter] [32mfinish build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale], cost 42.6s [0m
[2025-12-14 10:04:24 TP6] [32mfinish build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale], cost 42.6s [0m
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:04:24 TP6] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:04:24 TP0] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:04:24 TP2] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:04:24 TP7] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:04:24 TP4] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:04:24 TP5] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:04:24 TP3] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:04:24 TP1] import [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:04:28 TP0] Prefill batch, #new-seq: 1, #new-token: 667, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:04:30 TP0] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:04:30 TP3] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:04:30 TP1] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:04:30 TP7] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:04:30 TP2] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:04:30 TP4] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:04:30 TP6] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[aiter] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:04:30 TP5] [fused_moe] using 1stage default for (304, 1024, 7168, 256, 257, 9, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_1x128', True, False) 
[2025-12-14 10:04:30] INFO:     127.0.0.1:56306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:31 TP0] Prefill batch, #new-seq: 106, #new-token: 6440, #cached-token: 70702, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-12-14 10:04:34 TP0] Prefill batch, #new-seq: 261, #new-token: 16359, #cached-token: 174087, token usage: 0.01, #running-req: 107, #queue-req: 952, 
[2025-12-14 10:04:37 TP0] Prefill batch, #new-seq: 279, #new-token: 16367, #cached-token: 186747, token usage: 0.02, #running-req: 368, #queue-req: 673, 
[2025-12-14 10:04:38 TP0] Prefill batch, #new-seq: 277, #new-token: 16320, #cached-token: 185525, token usage: 0.04, #running-req: 647, #queue-req: 396, 
[2025-12-14 10:04:42 TP0] Prefill batch, #new-seq: 100, #new-token: 6637, #cached-token: 66981, token usage: 0.06, #running-req: 924, #queue-req: 296, 
[2025-12-14 10:04:48] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:48] The server is fired up and ready to roll!
[2025-12-14 10:04:50 TP0] Prefill batch, #new-seq: 1, #new-token: 121, #cached-token: 670, token usage: 0.07, #running-req: 1023, #queue-req: 295, 
[2025-12-14 10:04:54] INFO:     127.0.0.1:34834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:54] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:54 TP0] Prefill batch, #new-seq: 1, #new-token: 74, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 294, 
[2025-12-14 10:04:54 TP0] Prefill batch, #new-seq: 1, #new-token: 63, #cached-token: 671, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-12-14 10:04:54] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:55 TP0] Prefill batch, #new-seq: 1, #new-token: 55, #cached-token: 669, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-12-14 10:04:55 TP0] Decode batch, #running-req: 1024, #token: 101619, token usage: 0.10, cuda graph: False, gen throughput (token/s): 513.84, #queue-req: 292, 
[2025-12-14 10:04:55] INFO:     127.0.0.1:54608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:55] INFO:     127.0.0.1:55252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:55] INFO:     127.0.0.1:58122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:56] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:56] INFO:     127.0.0.1:59088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:56] INFO:     127.0.0.1:59200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:56] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:56] INFO:     127.0.0.1:34824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:56 TP0] Prefill batch, #new-seq: 3, #new-token: 271, #cached-token: 2010, token usage: 0.10, #running-req: 1021, #queue-req: 289, 
[2025-12-14 10:04:58] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:58] INFO:     127.0.0.1:56916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:58] INFO:     127.0.0.1:57312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:58] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:04:58 TP0] Prefill batch, #new-seq: 9, #new-token: 566, #cached-token: 6030, token usage: 0.11, #running-req: 1015, #queue-req: 280, 
[2025-12-14 10:05:00] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:00] INFO:     127.0.0.1:57446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:01 TP0] Prefill batch, #new-seq: 2, #new-token: 136, #cached-token: 1340, token usage: 0.11, #running-req: 1022, #queue-req: 278, 
[2025-12-14 10:05:01] INFO:     127.0.0.1:54830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:01] INFO:     127.0.0.1:55064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:01] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:01] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:01] INFO:     127.0.0.1:56494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:01] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:01] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:01] INFO:     127.0.0.1:59464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:01] INFO:     127.0.0.1:34958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:02 TP0] Prefill batch, #new-seq: 9, #new-token: 455, #cached-token: 6036, token usage: 0.11, #running-req: 1015, #queue-req: 269, 
[2025-12-14 10:05:04] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:04] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:04] INFO:     127.0.0.1:60030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:04] INFO:     127.0.0.1:34948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:04] INFO:     127.0.0.1:35082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:04 TP0] Prefill batch, #new-seq: 5, #new-token: 293, #cached-token: 3347, token usage: 0.11, #running-req: 1019, #queue-req: 264, 
[2025-12-14 10:05:04] INFO:     127.0.0.1:54774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:04] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:04] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:04] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:04] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:04] INFO:     127.0.0.1:34092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:04] INFO:     127.0.0.1:34310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:04 TP0] Prefill batch, #new-seq: 7, #new-token: 333, #cached-token: 4688, token usage: 0.11, #running-req: 1017, #queue-req: 257, 
[2025-12-14 10:05:05] INFO:     127.0.0.1:58816 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:05] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:05] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:05 TP0] Prefill batch, #new-seq: 3, #new-token: 176, #cached-token: 2012, token usage: 0.11, #running-req: 1021, #queue-req: 254, 
[2025-12-14 10:05:06] INFO:     127.0.0.1:54912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:55426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:33914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:34140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:35018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06 TP0] Prefill batch, #new-seq: 8, #new-token: 491, #cached-token: 5358, token usage: 0.11, #running-req: 1016, #queue-req: 246, 
[2025-12-14 10:05:06] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:56228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:60818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:33696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:33984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:06 TP0] Prefill batch, #new-seq: 10, #new-token: 685, #cached-token: 6702, token usage: 0.11, #running-req: 1014, #queue-req: 236, 
[2025-12-14 10:05:07] INFO:     127.0.0.1:54928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:07] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:07] INFO:     127.0.0.1:33484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:07] INFO:     127.0.0.1:33968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:07 TP0] Prefill batch, #new-seq: 4, #new-token: 239, #cached-token: 2679, token usage: 0.11, #running-req: 1020, #queue-req: 232, 
[2025-12-14 10:05:08] INFO:     127.0.0.1:55608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:59016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:35144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08 TP0] Prefill batch, #new-seq: 5, #new-token: 318, #cached-token: 3353, token usage: 0.11, #running-req: 1019, #queue-req: 227, 
[2025-12-14 10:05:08] INFO:     127.0.0.1:57096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:58670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:60008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:60764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:34184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:34326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:34456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:35440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08] INFO:     127.0.0.1:35500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:08 TP0] Prefill batch, #new-seq: 13, #new-token: 798, #cached-token: 8707, token usage: 0.11, #running-req: 1011, #queue-req: 214, 
[2025-12-14 10:05:10] INFO:     127.0.0.1:55642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:10] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:10] INFO:     127.0.0.1:56296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:10] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:10] INFO:     127.0.0.1:57618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:10] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:10] INFO:     127.0.0.1:58454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:10] INFO:     127.0.0.1:60960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:10] INFO:     127.0.0.1:34280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:10] INFO:     127.0.0.1:34404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:10] INFO:     127.0.0.1:34802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:10 TP0] Prefill batch, #new-seq: 11, #new-token: 520, #cached-token: 7366, token usage: 0.11, #running-req: 1013, #queue-req: 203, 
[2025-12-14 10:05:11] INFO:     127.0.0.1:55026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11] INFO:     127.0.0.1:55618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11] INFO:     127.0.0.1:57830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11] INFO:     127.0.0.1:58096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11] INFO:     127.0.0.1:59742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11] INFO:     127.0.0.1:60240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11] INFO:     127.0.0.1:60382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11 TP0] Prefill batch, #new-seq: 8, #new-token: 499, #cached-token: 5360, token usage: 0.12, #running-req: 1016, #queue-req: 195, 
[2025-12-14 10:05:11] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11] INFO:     127.0.0.1:58848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11] INFO:     127.0.0.1:60056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11] INFO:     127.0.0.1:33106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:11 TP0] Prefill batch, #new-seq: 6, #new-token: 331, #cached-token: 4019, token usage: 0.12, #running-req: 1018, #queue-req: 189, 
[2025-12-14 10:05:12] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:55484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:57020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:60886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:32784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:33228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:34606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:35124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12 TP0] Prefill batch, #new-seq: 10, #new-token: 618, #cached-token: 6698, token usage: 0.12, #running-req: 1014, #queue-req: 179, 
[2025-12-14 10:05:12] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:56428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:58862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:59218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:59550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:60570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:32782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:34154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12 TP0] Prefill batch, #new-seq: 11, #new-token: 561, #cached-token: 7369, token usage: 0.12, #running-req: 1013, #queue-req: 168, 
[2025-12-14 10:05:12] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:12] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13 TP0] Prefill batch, #new-seq: 7, #new-token: 348, #cached-token: 4692, token usage: 0.12, #running-req: 1017, #queue-req: 161, 
[2025-12-14 10:05:13] INFO:     127.0.0.1:55106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:56718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:59348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:60872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:33666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:33942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:34918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13 TP0] Prefill batch, #new-seq: 9, #new-token: 559, #cached-token: 6026, token usage: 0.12, #running-req: 1015, #queue-req: 152, 
[2025-12-14 10:05:13] INFO:     127.0.0.1:56078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:56370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:32920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:32954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:34468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:13] INFO:     127.0.0.1:35358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14 TP0] Prefill batch, #new-seq: 8, #new-token: 474, #cached-token: 5362, token usage: 0.12, #running-req: 1016, #queue-req: 144, 
[2025-12-14 10:05:14] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:56266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:56662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:58880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:58958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:59004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:34288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14 TP0] Prefill batch, #new-seq: 9, #new-token: 601, #cached-token: 6032, token usage: 0.12, #running-req: 1015, #queue-req: 135, 
[2025-12-14 10:05:14] INFO:     127.0.0.1:54714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:55008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:56014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:56654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:59368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:60064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:60122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14] INFO:     127.0.0.1:33710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:14 TP0] Prefill batch, #new-seq: 12, #new-token: 735, #cached-token: 8038, token usage: 0.12, #running-req: 1012, #queue-req: 123, 
[2025-12-14 10:05:15] INFO:     127.0.0.1:55726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:15] INFO:     127.0.0.1:55974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:15] INFO:     127.0.0.1:56258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:15] INFO:     127.0.0.1:59670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:15] INFO:     127.0.0.1:59854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:15] INFO:     127.0.0.1:60894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:15] INFO:     127.0.0.1:33308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:15] INFO:     127.0.0.1:33374 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:15] INFO:     127.0.0.1:34344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:15] INFO:     127.0.0.1:34624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:15 TP0] Prefill batch, #new-seq: 10, #new-token: 660, #cached-token: 6701, token usage: 0.12, #running-req: 1014, #queue-req: 113, 
[2025-12-14 10:05:15] INFO:     127.0.0.1:33350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:15] INFO:     127.0.0.1:34372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:15 TP0] Prefill batch, #new-seq: 8, #new-token: 544, #cached-token: 5363, token usage: 0.12, #running-req: 1016, #queue-req: 105, 
[2025-12-14 10:05:16] INFO:     127.0.0.1:57968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:16] INFO:     127.0.0.1:56534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:16] INFO:     127.0.0.1:57248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:16] INFO:     127.0.0.1:59970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:16] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:16] INFO:     127.0.0.1:60512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:58562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:59848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:34248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:34388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:34758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18 TP0] Prefill batch, #new-seq: 11, #new-token: 706, #cached-token: 7373, token usage: 0.12, #running-req: 1013, #queue-req: 94, 
[2025-12-14 10:05:18] INFO:     127.0.0.1:55140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:55430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:56476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:58268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:60692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:34674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18] INFO:     127.0.0.1:35406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:18 TP0] Prefill batch, #new-seq: 12, #new-token: 627, #cached-token: 8038, token usage: 0.12, #running-req: 1012, #queue-req: 82, 
[2025-12-14 10:05:19] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:19] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:19] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:19] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:19] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:19] INFO:     127.0.0.1:57188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:19] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:19] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:19] INFO:     127.0.0.1:57878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:19] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:19 TP0] Prefill batch, #new-seq: 10, #new-token: 598, #cached-token: 6697, token usage: 0.12, #running-req: 1014, #queue-req: 72, 
[2025-12-14 10:05:20] INFO:     127.0.0.1:54758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:56884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:59454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:60174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:60282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:34370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20 TP0] Prefill batch, #new-seq: 11, #new-token: 829, #cached-token: 7366, token usage: 0.12, #running-req: 1013, #queue-req: 61, 
[2025-12-14 10:05:20] INFO:     127.0.0.1:55160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:56510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:56866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:60116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:60230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:60458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:33174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:33196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:33508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:33620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:34124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:34174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:34198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:34514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20] INFO:     127.0.0.1:35296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:20 TP0] Prefill batch, #new-seq: 18, #new-token: 1154, #cached-token: 12058, token usage: 0.12, #running-req: 1006, #queue-req: 43, 
[2025-12-14 10:05:23] INFO:     127.0.0.1:55364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:56404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:58436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:59410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:60538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:32978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:33742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:34768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23 TP0] Prefill batch, #new-seq: 12, #new-token: 712, #cached-token: 8041, token usage: 0.12, #running-req: 1012, #queue-req: 31, 
[2025-12-14 10:05:23] INFO:     127.0.0.1:56314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:56728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:58256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:59896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:33436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:33690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:34506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23] INFO:     127.0.0.1:34866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:23 TP0] Prefill batch, #new-seq: 14, #new-token: 848, #cached-token: 9380, token usage: 0.12, #running-req: 1010, #queue-req: 17, 
[2025-12-14 10:05:26] INFO:     127.0.0.1:55174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:55704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:56242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:58558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:59288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:60856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:32838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:33222 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26 TP0] Prefill batch, #new-seq: 12, #new-token: 715, #cached-token: 8038, token usage: 0.12, #running-req: 1012, #queue-req: 5, 
[2025-12-14 10:05:26] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:55348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:56830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:57636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:60656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:33072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:33580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26] INFO:     127.0.0.1:35362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:26 TP0] Prefill batch, #new-seq: 5, #new-token: 269, #cached-token: 3354, token usage: 0.12, #running-req: 1013, #queue-req: 0, 
[2025-12-14 10:05:27] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:54852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:60580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:35424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:57744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:60310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:32926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:33506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:35074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:56122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:56954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:58020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:58364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:32898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:32928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:34258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:34498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:34590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:35096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:55050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:56934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:58746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:60324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:33608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:33754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:55798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:56366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:59174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:60624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:33014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:34810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27 TP0] Decode batch, #running-req: 982, #token: 121220, token usage: 0.12, cuda graph: False, gen throughput (token/s): 1265.79, #queue-req: 0, 
[2025-12-14 10:05:27] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:59512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:34212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:27] INFO:     127.0.0.1:34448 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:54964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:55694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:57030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:59138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:59832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:33946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:34088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:35060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:35542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:54844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:55360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:56608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:59396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:34460 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:55410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:60672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:33246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:34736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:35516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:57916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:58586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:60086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:60742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:33528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:33762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:34024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:59204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:60910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:33336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:55010 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:55120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:60082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:33476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:34500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:34750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:58342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:59066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:59262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:59954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:32822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:33846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:33990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:34314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:28] INFO:     127.0.0.1:35638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:58410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:59722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:59978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:35280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:54916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:55548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:58306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:59238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:32936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:34580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:34774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:35092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:59578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:59590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:59932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:60604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:33870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:34722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:57056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:57356 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:57596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:58006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:33162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:34162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:34402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:34440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:35584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:60284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:60964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:34650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:56642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:32960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:34290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:34888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:55602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:57066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:58712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:59648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:29] INFO:     127.0.0.1:34062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:33206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:34114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:34334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:35376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:36040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:55086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:33298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:35112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:36278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:55126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:55764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:57462 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:33864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:34100 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:35398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:35674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:60472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:33786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:34638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:34862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:35020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:35050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:35756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:55436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:56732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:57006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:60010 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:60160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:33428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:33722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:35106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:57548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:58080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:32768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:34428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:35326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:33104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:33904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:34232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:35034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:35414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:30] INFO:     127.0.0.1:36754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:54740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:55518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:55830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:56616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:57690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:59304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:60968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:33406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:55468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:55504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:56628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:34298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:35488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:57114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:58312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:58320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:32910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:35202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:55276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:57784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:59108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:33882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:34482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:36224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:36302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:36464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:36682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:36740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:54968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:55866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:56674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:58044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:58786 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:58864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:34878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:35932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:60838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:33938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:34050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:34116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:36376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:56144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:58250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:59918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:35836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:36444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:36718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:36934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:31] INFO:     127.0.0.1:37998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:54756 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:56070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:58058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:33126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:33674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:34012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:34584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:34680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:34690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:34858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:35306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:35420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:37116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:57298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:59738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:32998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:34222 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:35226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:55406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:56540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:58822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:60500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:32992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:33886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:34780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:34856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:35246 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:36470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:55190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:57348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:58642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:59354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:60016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:60470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:60776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:33944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:34894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:35160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:35656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:36880 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:55728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:34988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:36148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:38132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:55302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:57974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:60408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:60644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:36642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:37336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:57150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:59214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:33954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:34330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:35264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:32] INFO:     127.0.0.1:36346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:55216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:58172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:58764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:33306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:34416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:35242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:35908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:55000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:57814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:60342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:60368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:33834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:33894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:36270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:37352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:58696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:32922 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:33676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:33850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:34934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:37042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:38034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:60040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:34558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:35850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:35990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:37600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33 TP0] Decode batch, #running-req: 558, #token: 86583, token usage: 0.09, cuda graph: False, gen throughput (token/s): 5377.34, #queue-req: 0, 
[2025-12-14 10:05:33] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:55260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:34642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:35736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:36452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:36498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:36814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:37510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:56180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:56646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:57504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:57798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:60908 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:33450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:33626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:36530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:37204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:56358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:58606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:59788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:60566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:32860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:34042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:34526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:35962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:36494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:36664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:56708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:58612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:59538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:60754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:60946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:33704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:59058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:33276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:33776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:34980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:35002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:36724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:33] INFO:     127.0.0.1:37990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:59314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:59474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:36318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:58570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:33358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:34078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:36282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:59344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:60612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:34582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:34706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:35342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:36542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:37056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:55168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:36746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:37654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:57282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:34008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:35458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:36554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:36794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:37160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:37566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:55736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:33288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:36502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:37466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:54594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:59510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:35446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:35556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:35752 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:36836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:34026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:35322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:36202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:37812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:37918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:54644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:58542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:35892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:37144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:37364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:38280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:59352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:33812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:35878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:37236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:38274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:55970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:37312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:37478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:60052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:33766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:35626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:35942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:36778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:37124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:34] INFO:     127.0.0.1:59430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:34006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:34210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:38232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:33088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:34964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:38064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:38120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:57396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:60708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:34172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:34850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:35380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37640 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:38220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:58158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:59636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:32890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:33136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:38134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:34466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:35822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:56386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:59808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:33802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:35040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:56276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:56742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:35706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:35870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:54754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:58716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:34804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:35548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:35714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37298 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:34682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:34902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:35914 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:35772 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:36766 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:38162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:38258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:38330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:59442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:60094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:35782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:37842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:35] INFO:     127.0.0.1:38360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:55900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:56754 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:54896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:36236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:38010 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:36974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:38338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:59424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:38042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:38082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:38308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:56390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:60788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:35954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:36052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:54646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:56224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:58300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:33252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37366 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:38304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:56520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:58526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:35098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:35620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:36402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:38182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:56526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:56782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:33058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:35662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:38192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36 TP0] Decode batch, #running-req: 250, #token: 49289, token usage: 0.05, cuda graph: True, gen throughput (token/s): 5088.41, #queue-req: 0, 
[2025-12-14 10:05:36] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:36826 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:38294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:56056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:32850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:33592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:36020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:60942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:35218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:36960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:54790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:59322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:35944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:36646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:59980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:36] INFO:     127.0.0.1:37958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:60416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:35066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:35174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:56860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:33398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:38038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:36690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:38104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:38202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:32876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:33570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:38144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:33826 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:36130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:38084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:60028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:36706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:38152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:60728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:33558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:35728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:36658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:59656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:38018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:56464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:36044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:54692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:60356 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:36012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:36228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:37890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:38072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:56012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:36806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:37] INFO:     127.0.0.1:36956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:32814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:37178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:37006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:35978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:37394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:37430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:37686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:38158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:38292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:57520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:37884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:34794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:35806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:54956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:33034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:37586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:37716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:35866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:37784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:34268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:57208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:33116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:35648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:60262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:34842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:38200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:56166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:37552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38 TP0] Decode batch, #running-req: 102, #token: 24661, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3055.93, #queue-req: 0, 
[2025-12-14 10:05:38] INFO:     127.0.0.1:57232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:35162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:55842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:57080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:38054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:60414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:35598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:38] INFO:     127.0.0.1:36080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:35576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:58540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:37004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:38172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:34356 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:37414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:36358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:33662 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:37806 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:37814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:57410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:37736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:33496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:33522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:36910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:54954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:55718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:35608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:37680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:38242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:37524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:38314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:36096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:37726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:37860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:60926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:36482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:54698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:35134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:37196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:39] INFO:     127.0.0.1:37844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:37350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:34704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:37782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:38346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:58990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:37248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:37936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:33780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:37190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:38178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:59790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:56092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:37268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40 TP0] Decode batch, #running-req: 37, #token: 10942, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1452.46, #queue-req: 0, 
[2025-12-14 10:05:40] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:33678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:34634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:36258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:58538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:38098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:55592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:38204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:40] INFO:     127.0.0.1:32788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:36508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:36862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:37022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:36942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:35798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:36162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:36330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:35570 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:36066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:35390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:33322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:58188 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:37874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41] INFO:     127.0.0.1:36588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:41 TP0] Decode batch, #running-req: 11, #token: 3944, token usage: 0.00, cuda graph: True, gen throughput (token/s): 645.73, #queue-req: 0, 
[2025-12-14 10:05:42] INFO:     127.0.0.1:35804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:42] INFO:     127.0.0.1:36924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:42] INFO:     127.0.0.1:37234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:42] INFO:     127.0.0.1:35416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:42] INFO:     127.0.0.1:34612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:42] INFO:     127.0.0.1:35528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:42 TP0] Decode batch, #running-req: 4, #token: 1751, token usage: 0.00, cuda graph: True, gen throughput (token/s): 201.78, #queue-req: 0, 
[2025-12-14 10:05:42] INFO:     127.0.0.1:37082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:43] INFO:     127.0.0.1:36564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:43] INFO:     127.0.0.1:35690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:43 TP0] Decode batch, #running-req: 1, #token: 1110, token usage: 0.00, cuda graph: True, gen throughput (token/s): 51.53, #queue-req: 0, 
[2025-12-14 10:05:44 TP0] Decode batch, #running-req: 1, #token: 1150, token usage: 0.00, cuda graph: True, gen throughput (token/s): 41.52, #queue-req: 0, 
[2025-12-14 10:05:45 TP0] Decode batch, #running-req: 1, #token: 1190, token usage: 0.00, cuda graph: True, gen throughput (token/s): 41.50, #queue-req: 0, 
[2025-12-14 10:05:46 TP0] Decode batch, #running-req: 1, #token: 1230, token usage: 0.00, cuda graph: True, gen throughput (token/s): 41.47, #queue-req: 0, 
[2025-12-14 10:05:47] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:05:52] INFO:     127.0.0.1:45216 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-14 10:06:04] INFO:     127.0.0.1:58580 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:04 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:06:07 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1.92, #queue-req: 0, 
[2025-12-14 10:06:08] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:06:09] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58656 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58930 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:58992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59002 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59016 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:06:09 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:06:09 TP2] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:06:09 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:06:09 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:06:09 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:06:09] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59126 - "POST /generate HTTP/1.1" 200 OK
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:06:09 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:06:09 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:06:09 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale
[2025-12-14 10:06:09] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59286 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59328 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59362 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59386 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59436 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59622 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:06:09] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[aiter] [32mfinish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale], cost 42.8s [0m
[2025-12-14 10:06:52 TP2] [32mfinish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale], cost 42.8s [0m
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:06:52 TP2] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:06:52 TP0] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:06:52 TP7] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:06:52 TP6] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:06:52 TP3] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:06:52 TP5] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:06:52 TP4] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[aiter] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:06:52 TP1] import [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale] under /sgl-workspace/aiter/aiter/jit/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip_nqscale.so
[2025-12-14 10:06:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 122, 
[2025-12-14 10:06:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.02, #running-req: 6, #queue-req: 117, 
[2025-12-14 10:06:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-12-14 10:06:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-12-14 10:06:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-12-14 10:06:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.08, #running-req: 26, #queue-req: 97, 
[2025-12-14 10:06:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-12-14 10:07:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-12-14 10:07:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-12-14 10:07:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-12-14 10:07:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-12-14 10:07:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-12-14 10:07:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.20, #running-req: 61, #queue-req: 62, 
[2025-12-14 10:07:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.22, #running-req: 66, #queue-req: 57, 
[2025-12-14 10:07:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.23, #running-req: 71, #queue-req: 52, 
[2025-12-14 10:07:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.25, #running-req: 76, #queue-req: 47, 
[2025-12-14 10:07:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.26, #running-req: 81, #queue-req: 42, 
[2025-12-14 10:07:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.28, #running-req: 86, #queue-req: 37, 
[2025-12-14 10:07:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.30, #running-req: 91, #queue-req: 32, 
[2025-12-14 10:07:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.31, #running-req: 96, #queue-req: 27, 
[2025-12-14 10:07:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.33, #running-req: 101, #queue-req: 22, 
[2025-12-14 10:07:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.35, #running-req: 106, #queue-req: 17, 
[2025-12-14 10:07:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.36, #running-req: 111, #queue-req: 12, 
[2025-12-14 10:07:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.38, #running-req: 116, #queue-req: 7, 
[2025-12-14 10:07:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.39, #running-req: 121, #queue-req: 2, 
[2025-12-14 10:07:17 TP0] Prefill batch, #new-seq: 2, #new-token: 6394, #cached-token: 8, token usage: 0.41, #running-req: 126, #queue-req: 0, 
[2025-12-14 10:07:22 TP0] Decode batch, #running-req: 128, #token: 412959, token usage: 0.42, cuda graph: True, gen throughput (token/s): 46.31, #queue-req: 0, 
[2025-12-14 10:07:25 TP0] Decode batch, #running-req: 128, #token: 418079, token usage: 0.43, cuda graph: True, gen throughput (token/s): 1979.71, #queue-req: 0, 
[2025-12-14 10:07:27 TP0] Decode batch, #running-req: 128, #token: 423199, token usage: 0.43, cuda graph: True, gen throughput (token/s): 1963.05, #queue-req: 0, 
[2025-12-14 10:07:30 TP0] Decode batch, #running-req: 128, #token: 428319, token usage: 0.44, cuda graph: True, gen throughput (token/s): 1942.38, #queue-req: 0, 
[2025-12-14 10:07:33 TP0] Decode batch, #running-req: 128, #token: 433439, token usage: 0.44, cuda graph: True, gen throughput (token/s): 1933.50, #queue-req: 0, 
[2025-12-14 10:07:35 TP0] Decode batch, #running-req: 128, #token: 438559, token usage: 0.45, cuda graph: True, gen throughput (token/s): 1933.27, #queue-req: 0, 
[2025-12-14 10:07:38 TP0] Decode batch, #running-req: 128, #token: 443679, token usage: 0.45, cuda graph: True, gen throughput (token/s): 1928.75, #queue-req: 0, 
[2025-12-14 10:07:41 TP0] Decode batch, #running-req: 128, #token: 448799, token usage: 0.46, cuda graph: True, gen throughput (token/s): 1904.53, #queue-req: 0, 
[2025-12-14 10:07:43 TP0] Decode batch, #running-req: 128, #token: 453919, token usage: 0.46, cuda graph: True, gen throughput (token/s): 1916.90, #queue-req: 0, 
[2025-12-14 10:07:46 TP0] Decode batch, #running-req: 128, #token: 459039, token usage: 0.47, cuda graph: True, gen throughput (token/s): 1904.77, #queue-req: 0, 
[2025-12-14 10:07:49 TP0] Decode batch, #running-req: 128, #token: 464159, token usage: 0.47, cuda graph: True, gen throughput (token/s): 1895.46, #queue-req: 0, 
[2025-12-14 10:07:51 TP0] Decode batch, #running-req: 128, #token: 469279, token usage: 0.48, cuda graph: True, gen throughput (token/s): 1873.53, #queue-req: 0, 
[2025-12-14 10:07:54 TP0] Decode batch, #running-req: 128, #token: 474399, token usage: 0.48, cuda graph: True, gen throughput (token/s): 1781.42, #queue-req: 0, 
[2025-12-14 10:07:57 TP0] Decode batch, #running-req: 128, #token: 479519, token usage: 0.49, cuda graph: True, gen throughput (token/s): 1768.88, #queue-req: 0, 
[2025-12-14 10:08:00 TP0] Decode batch, #running-req: 128, #token: 484639, token usage: 0.49, cuda graph: True, gen throughput (token/s): 1871.98, #queue-req: 0, 
[2025-12-14 10:08:03 TP0] Decode batch, #running-req: 128, #token: 489759, token usage: 0.50, cuda graph: True, gen throughput (token/s): 1857.60, #queue-req: 0, 
[2025-12-14 10:08:05 TP0] Decode batch, #running-req: 128, #token: 494879, token usage: 0.50, cuda graph: True, gen throughput (token/s): 1855.96, #queue-req: 0, 
[2025-12-14 10:08:08 TP0] Decode batch, #running-req: 128, #token: 499999, token usage: 0.51, cuda graph: True, gen throughput (token/s): 1849.76, #queue-req: 0, 
[2025-12-14 10:08:11 TP0] Decode batch, #running-req: 128, #token: 505119, token usage: 0.51, cuda graph: True, gen throughput (token/s): 1679.52, #queue-req: 0, 
[2025-12-14 10:08:17 TP0] Decode batch, #running-req: 128, #token: 510239, token usage: 0.52, cuda graph: True, gen throughput (token/s): 847.24, #queue-req: 0, 
[2025-12-14 10:08:19] INFO:     127.0.0.1:47096 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19 TP0] Prefill batch, #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:08:19] INFO:     127.0.0.1:47104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47232 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47248 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47268 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47466 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47482 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47496 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47576 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:19] INFO:     127.0.0.1:47602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47762 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47876 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47902 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:47984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48146 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48286 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:20] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:08:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 122, 
[2025-12-14 10:08:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 6, #queue-req: 117, 
[2025-12-14 10:08:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-12-14 10:08:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-12-14 10:08:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-12-14 10:08:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.08, #running-req: 26, #queue-req: 97, 
[2025-12-14 10:08:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-12-14 10:08:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-12-14 10:08:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-12-14 10:08:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-12-14 10:08:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-12-14 10:08:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-12-14 10:08:39 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.20, #running-req: 61, #queue-req: 61, 
[2025-12-14 10:08:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.22, #running-req: 67, #queue-req: 56, 
[2025-12-14 10:08:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15977, #cached-token: 28, token usage: 0.23, #running-req: 72, #queue-req: 51, 
[2025-12-14 10:08:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.25, #running-req: 77, #queue-req: 46, 
[2025-12-14 10:08:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.26, #running-req: 82, #queue-req: 41, 
[2025-12-14 10:08:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.28, #running-req: 87, #queue-req: 36, 
[2025-12-14 10:08:48 TP0] Prefill batch, #new-seq: 6, #new-token: 15994, #cached-token: 3212, token usage: 0.30, #running-req: 92, #queue-req: 30, 
[2025-12-14 10:08:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.32, #running-req: 98, #queue-req: 25, 
[2025-12-14 10:08:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.33, #running-req: 103, #queue-req: 20, 
[2025-12-14 10:08:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.35, #running-req: 108, #queue-req: 15, 
[2025-12-14 10:08:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.37, #running-req: 113, #queue-req: 10, 
[2025-12-14 10:08:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.38, #running-req: 118, #queue-req: 5, 
[2025-12-14 10:08:59 TP0] Prefill batch, #new-seq: 5, #new-token: 12792, #cached-token: 3213, token usage: 0.40, #running-req: 123, #queue-req: 0, 
[2025-12-14 10:09:08 TP0] Decode batch, #running-req: 128, #token: 406557, token usage: 0.41, cuda graph: True, gen throughput (token/s): 100.25, #queue-req: 0, 
[2025-12-14 10:09:14 TP0] Decode batch, #running-req: 128, #token: 411677, token usage: 0.42, cuda graph: True, gen throughput (token/s): 880.67, #queue-req: 0, 
[2025-12-14 10:09:20 TP0] Decode batch, #running-req: 128, #token: 416797, token usage: 0.42, cuda graph: True, gen throughput (token/s): 878.64, #queue-req: 0, 
[2025-12-14 10:09:26 TP0] Decode batch, #running-req: 128, #token: 421917, token usage: 0.43, cuda graph: True, gen throughput (token/s): 876.20, #queue-req: 0, 
[2025-12-14 10:09:32 TP0] Decode batch, #running-req: 128, #token: 427037, token usage: 0.44, cuda graph: True, gen throughput (token/s): 869.39, #queue-req: 0, 
[2025-12-14 10:09:38 TP0] Decode batch, #running-req: 128, #token: 432157, token usage: 0.44, cuda graph: True, gen throughput (token/s): 864.86, #queue-req: 0, 
[2025-12-14 10:09:43 TP0] Decode batch, #running-req: 128, #token: 437277, token usage: 0.45, cuda graph: True, gen throughput (token/s): 864.57, #queue-req: 0, 
[2025-12-14 10:09:49 TP0] Decode batch, #running-req: 128, #token: 442397, token usage: 0.45, cuda graph: True, gen throughput (token/s): 864.10, #queue-req: 0, 
[2025-12-14 10:09:55 TP0] Decode batch, #running-req: 128, #token: 447517, token usage: 0.46, cuda graph: True, gen throughput (token/s): 864.15, #queue-req: 0, 
[2025-12-14 10:10:01 TP0] Decode batch, #running-req: 128, #token: 452637, token usage: 0.46, cuda graph: True, gen throughput (token/s): 863.70, #queue-req: 0, 
[2025-12-14 10:10:07 TP0] Decode batch, #running-req: 128, #token: 457757, token usage: 0.47, cuda graph: True, gen throughput (token/s): 856.45, #queue-req: 0, 
[2025-12-14 10:10:13 TP0] Decode batch, #running-req: 128, #token: 462877, token usage: 0.47, cuda graph: True, gen throughput (token/s): 851.82, #queue-req: 0, 
[2025-12-14 10:10:19 TP0] Decode batch, #running-req: 128, #token: 467997, token usage: 0.48, cuda graph: True, gen throughput (token/s): 851.48, #queue-req: 0, 
[2025-12-14 10:10:25 TP0] Decode batch, #running-req: 128, #token: 473117, token usage: 0.48, cuda graph: True, gen throughput (token/s): 851.66, #queue-req: 0, 
[2025-12-14 10:10:31 TP0] Decode batch, #running-req: 128, #token: 478237, token usage: 0.49, cuda graph: True, gen throughput (token/s): 851.47, #queue-req: 0, 
[2025-12-14 10:10:37 TP0] Decode batch, #running-req: 128, #token: 483357, token usage: 0.49, cuda graph: True, gen throughput (token/s): 851.37, #queue-req: 0, 
[2025-12-14 10:10:43 TP0] Decode batch, #running-req: 128, #token: 488477, token usage: 0.50, cuda graph: True, gen throughput (token/s): 844.63, #queue-req: 0, 
[2025-12-14 10:10:49 TP0] Decode batch, #running-req: 128, #token: 493597, token usage: 0.50, cuda graph: True, gen throughput (token/s): 839.64, #queue-req: 0, 
[2025-12-14 10:10:56 TP0] Decode batch, #running-req: 128, #token: 498717, token usage: 0.51, cuda graph: True, gen throughput (token/s): 839.81, #queue-req: 0, 
[2025-12-14 10:11:02 TP0] Decode batch, #running-req: 128, #token: 503837, token usage: 0.51, cuda graph: True, gen throughput (token/s): 839.92, #queue-req: 0, 
[2025-12-14 10:11:04] INFO:     127.0.0.1:49192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49206 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49214 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04 TP0] Prefill batch, #new-seq: 2, #new-token: 6396, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:11:04] INFO:     127.0.0.1:49222 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49442 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49592 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.01, #running-req: 2, #queue-req: 39, 
[2025-12-14 10:11:04] INFO:     127.0.0.1:49650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49652 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49810 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49826 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49916 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49980 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50010 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50032 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50040 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50134 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50194 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50220 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50282 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50316 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:04] INFO:     127.0.0.1:50324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:11:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.02, #running-req: 7, #queue-req: 116, 
[2025-12-14 10:11:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.04, #running-req: 12, #queue-req: 111, 
[2025-12-14 10:11:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.06, #running-req: 17, #queue-req: 106, 
[2025-12-14 10:11:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.07, #running-req: 22, #queue-req: 101, 
[2025-12-14 10:11:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.09, #running-req: 27, #queue-req: 96, 
[2025-12-14 10:11:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.10, #running-req: 32, #queue-req: 91, 
[2025-12-14 10:11:14 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.12, #running-req: 37, #queue-req: 85, 
[2025-12-14 10:11:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.14, #running-req: 43, #queue-req: 80, 
[2025-12-14 10:11:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.16, #running-req: 48, #queue-req: 75, 
[2025-12-14 10:11:19 TP0] Prefill batch, #new-seq: 5, #new-token: 15978, #cached-token: 27, token usage: 0.17, #running-req: 53, #queue-req: 70, 
[2025-12-14 10:11:20 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.19, #running-req: 58, #queue-req: 64, 
[2025-12-14 10:11:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.21, #running-req: 64, #queue-req: 59, 
[2025-12-14 10:11:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.22, #running-req: 69, #queue-req: 54, 
[2025-12-14 10:11:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.24, #running-req: 74, #queue-req: 49, 
[2025-12-14 10:11:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.26, #running-req: 79, #queue-req: 44, 
[2025-12-14 10:11:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.27, #running-req: 84, #queue-req: 39, 
[2025-12-14 10:11:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.29, #running-req: 89, #queue-req: 34, 
[2025-12-14 10:11:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.31, #running-req: 94, #queue-req: 29, 
[2025-12-14 10:11:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.32, #running-req: 99, #queue-req: 24, 
[2025-12-14 10:11:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.34, #running-req: 104, #queue-req: 19, 
[2025-12-14 10:11:35 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.36, #running-req: 109, #queue-req: 14, 
[2025-12-14 10:11:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.37, #running-req: 114, #queue-req: 9, 
[2025-12-14 10:11:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.39, #running-req: 119, #queue-req: 4, 
[2025-12-14 10:11:40 TP0] Prefill batch, #new-seq: 4, #new-token: 12795, #cached-token: 9, token usage: 0.40, #running-req: 124, #queue-req: 0, 
[2025-12-14 10:11:46 TP0] Decode batch, #running-req: 128, #token: 412956, token usage: 0.42, cuda graph: True, gen throughput (token/s): 114.46, #queue-req: 0, 
[2025-12-14 10:11:52 TP0] Decode batch, #running-req: 128, #token: 418076, token usage: 0.43, cuda graph: True, gen throughput (token/s): 890.06, #queue-req: 0, 
[2025-12-14 10:11:58 TP0] Decode batch, #running-req: 128, #token: 423196, token usage: 0.43, cuda graph: True, gen throughput (token/s): 886.80, #queue-req: 0, 
[2025-12-14 10:12:04 TP0] Decode batch, #running-req: 128, #token: 428316, token usage: 0.44, cuda graph: True, gen throughput (token/s): 881.60, #queue-req: 0, 
[2025-12-14 10:12:10 TP0] Decode batch, #running-req: 128, #token: 433436, token usage: 0.44, cuda graph: True, gen throughput (token/s): 875.56, #queue-req: 0, 
[2025-12-14 10:12:15 TP0] Decode batch, #running-req: 128, #token: 438556, token usage: 0.45, cuda graph: True, gen throughput (token/s): 874.82, #queue-req: 0, 
[2025-12-14 10:12:21 TP0] Decode batch, #running-req: 128, #token: 443676, token usage: 0.45, cuda graph: True, gen throughput (token/s): 875.30, #queue-req: 0, 
[2025-12-14 10:12:27 TP0] Decode batch, #running-req: 128, #token: 448796, token usage: 0.46, cuda graph: True, gen throughput (token/s): 875.31, #queue-req: 0, 
[2025-12-14 10:12:33 TP0] Decode batch, #running-req: 128, #token: 453916, token usage: 0.46, cuda graph: True, gen throughput (token/s): 875.18, #queue-req: 0, 
[2025-12-14 10:12:39 TP0] Decode batch, #running-req: 128, #token: 459036, token usage: 0.47, cuda graph: True, gen throughput (token/s): 873.54, #queue-req: 0, 
[2025-12-14 10:12:45 TP0] Decode batch, #running-req: 128, #token: 464156, token usage: 0.47, cuda graph: True, gen throughput (token/s): 863.28, #queue-req: 0, 
[2025-12-14 10:12:51 TP0] Decode batch, #running-req: 128, #token: 469276, token usage: 0.48, cuda graph: True, gen throughput (token/s): 863.83, #queue-req: 0, 
[2025-12-14 10:12:57 TP0] Decode batch, #running-req: 128, #token: 474396, token usage: 0.48, cuda graph: True, gen throughput (token/s): 863.46, #queue-req: 0, 
[2025-12-14 10:13:03 TP0] Decode batch, #running-req: 128, #token: 479516, token usage: 0.49, cuda graph: True, gen throughput (token/s): 863.24, #queue-req: 0, 
[2025-12-14 10:13:08 TP0] Decode batch, #running-req: 128, #token: 484636, token usage: 0.49, cuda graph: True, gen throughput (token/s): 863.37, #queue-req: 0, 
[2025-12-14 10:13:14 TP0] Decode batch, #running-req: 128, #token: 489756, token usage: 0.50, cuda graph: True, gen throughput (token/s): 863.38, #queue-req: 0, 
[2025-12-14 10:13:20 TP0] Decode batch, #running-req: 128, #token: 494876, token usage: 0.50, cuda graph: True, gen throughput (token/s): 855.09, #queue-req: 0, 
[2025-12-14 10:13:26 TP0] Decode batch, #running-req: 128, #token: 499996, token usage: 0.51, cuda graph: True, gen throughput (token/s): 851.97, #queue-req: 0, 
[2025-12-14 10:13:32 TP0] Decode batch, #running-req: 128, #token: 505116, token usage: 0.51, cuda graph: True, gen throughput (token/s): 850.80, #queue-req: 0, 
[2025-12-14 10:13:38 TP0] Decode batch, #running-req: 128, #token: 510236, token usage: 0.52, cuda graph: True, gen throughput (token/s): 851.08, #queue-req: 0, 
[2025-12-14 10:13:40] INFO:     127.0.0.1:34390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40 TP0] Prefill batch, #new-seq: 2, #new-token: 6394, #cached-token: 8, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:13:40] INFO:     127.0.0.1:34408 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34470 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:40] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34544 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34566 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34600 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34602 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34660 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34666 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.01, #running-req: 2, #queue-req: 34, 
[2025-12-14 10:13:41] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34746 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34750 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34844 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34892 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34910 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34922 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34930 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34934 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:34998 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35006 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35078 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35116 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35162 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35228 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35286 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:41] INFO:     127.0.0.1:35412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:13:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 7, #queue-req: 104, 
[2025-12-14 10:13:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 12, #queue-req: 99, 
[2025-12-14 10:13:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.06, #running-req: 17, #queue-req: 94, 
[2025-12-14 10:13:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.07, #running-req: 22, #queue-req: 89, 
[2025-12-14 10:13:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 27, #queue-req: 84, 
[2025-12-14 10:13:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.10, #running-req: 32, #queue-req: 79, 
[2025-12-14 10:13:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.12, #running-req: 37, #queue-req: 74, 
[2025-12-14 10:13:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.14, #running-req: 42, #queue-req: 69, 
[2025-12-14 10:13:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 47, #queue-req: 64, 
[2025-12-14 10:13:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.17, #running-req: 52, #queue-req: 59, 
[2025-12-14 10:13:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.19, #running-req: 57, #queue-req: 54, 
[2025-12-14 10:13:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.20, #running-req: 62, #queue-req: 49, 
[2025-12-14 10:14:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.22, #running-req: 67, #queue-req: 44, 
[2025-12-14 10:14:01 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.24, #running-req: 72, #queue-req: 38, 
[2025-12-14 10:14:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.25, #running-req: 78, #queue-req: 33, 
[2025-12-14 10:14:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.27, #running-req: 83, #queue-req: 28, 
[2025-12-14 10:14:06 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.29, #running-req: 88, #queue-req: 23, 
[2025-12-14 10:14:07 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.30, #running-req: 93, #queue-req: 18, 
[2025-12-14 10:14:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.32, #running-req: 98, #queue-req: 13, 
[2025-12-14 10:14:10 TP0] Prefill batch, #new-seq: 6, #new-token: 15992, #cached-token: 3214, token usage: 0.34, #running-req: 103, #queue-req: 7, 
[2025-12-14 10:14:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.36, #running-req: 109, #queue-req: 2, 
[2025-12-14 10:14:14 TP0] Prefill batch, #new-seq: 2, #new-token: 6395, #cached-token: 7, token usage: 0.37, #running-req: 114, #queue-req: 0, 
[2025-12-14 10:14:19 TP0] Decode batch, #running-req: 116, #token: 374246, token usage: 0.38, cuda graph: True, gen throughput (token/s): 117.82, #queue-req: 0, 
[2025-12-14 10:14:25 TP0] Decode batch, #running-req: 116, #token: 378886, token usage: 0.39, cuda graph: True, gen throughput (token/s): 809.03, #queue-req: 0, 
[2025-12-14 10:14:31 TP0] Decode batch, #running-req: 116, #token: 383526, token usage: 0.39, cuda graph: True, gen throughput (token/s): 807.64, #queue-req: 0, 
[2025-12-14 10:14:36 TP0] Decode batch, #running-req: 116, #token: 388166, token usage: 0.40, cuda graph: True, gen throughput (token/s): 816.40, #queue-req: 0, 
[2025-12-14 10:14:42 TP0] Decode batch, #running-req: 116, #token: 392806, token usage: 0.40, cuda graph: True, gen throughput (token/s): 806.70, #queue-req: 0, 
[2025-12-14 10:14:48 TP0] Decode batch, #running-req: 116, #token: 397446, token usage: 0.40, cuda graph: True, gen throughput (token/s): 793.92, #queue-req: 0, 
[2025-12-14 10:14:54 TP0] Decode batch, #running-req: 116, #token: 402086, token usage: 0.41, cuda graph: True, gen throughput (token/s): 795.07, #queue-req: 0, 
[2025-12-14 10:15:00 TP0] Decode batch, #running-req: 116, #token: 406726, token usage: 0.41, cuda graph: True, gen throughput (token/s): 795.25, #queue-req: 0, 
[2025-12-14 10:15:05 TP0] Decode batch, #running-req: 116, #token: 411366, token usage: 0.42, cuda graph: True, gen throughput (token/s): 795.23, #queue-req: 0, 
[2025-12-14 10:15:11 TP0] Decode batch, #running-req: 116, #token: 416006, token usage: 0.42, cuda graph: True, gen throughput (token/s): 797.41, #queue-req: 0, 
[2025-12-14 10:15:17 TP0] Decode batch, #running-req: 116, #token: 420646, token usage: 0.43, cuda graph: True, gen throughput (token/s): 796.97, #queue-req: 0, 
[2025-12-14 10:15:23 TP0] Decode batch, #running-req: 116, #token: 425286, token usage: 0.43, cuda graph: True, gen throughput (token/s): 784.69, #queue-req: 0, 
[2025-12-14 10:15:29 TP0] Decode batch, #running-req: 116, #token: 429926, token usage: 0.44, cuda graph: True, gen throughput (token/s): 784.82, #queue-req: 0, 
[2025-12-14 10:15:35 TP0] Decode batch, #running-req: 116, #token: 434566, token usage: 0.44, cuda graph: True, gen throughput (token/s): 784.70, #queue-req: 0, 
[2025-12-14 10:15:41 TP0] Decode batch, #running-req: 116, #token: 439206, token usage: 0.45, cuda graph: True, gen throughput (token/s): 785.11, #queue-req: 0, 
[2025-12-14 10:15:47 TP0] Decode batch, #running-req: 116, #token: 443846, token usage: 0.45, cuda graph: True, gen throughput (token/s): 784.95, #queue-req: 0, 
[2025-12-14 10:15:53 TP0] Decode batch, #running-req: 116, #token: 448486, token usage: 0.46, cuda graph: True, gen throughput (token/s): 780.13, #queue-req: 0, 
[2025-12-14 10:15:59 TP0] Decode batch, #running-req: 116, #token: 453126, token usage: 0.46, cuda graph: True, gen throughput (token/s): 773.52, #queue-req: 0, 
[2025-12-14 10:16:05 TP0] Decode batch, #running-req: 116, #token: 457766, token usage: 0.47, cuda graph: True, gen throughput (token/s): 773.37, #queue-req: 0, 
[2025-12-14 10:16:11 TP0] Decode batch, #running-req: 116, #token: 462406, token usage: 0.47, cuda graph: True, gen throughput (token/s): 773.58, #queue-req: 0, 
[2025-12-14 10:16:12] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-14 10:16:13] INFO:     127.0.0.1:42842 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-14 10:16:14] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-14 10:16:14] INFO:     127.0.0.1:43268 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-14 10:16:24] INFO:     127.0.0.1:60874 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-14 10:16:31] INFO:     127.0.0.1:60890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:31 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:16:32 TP0] Decode batch, #running-req: 1, #token: 3229, token usage: 0.00, cuda graph: True, gen throughput (token/s): 72.08, #queue-req: 0, 
[2025-12-14 10:16:33] INFO:     127.0.0.1:35820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:16:33] INFO:     127.0.0.1:35834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35886 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35940 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35976 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35990 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:35994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36044 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 23, 
[2025-12-14 10:16:33] INFO:     127.0.0.1:36092 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36130 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36182 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36222 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36280 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36290 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33] INFO:     127.0.0.1:36326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:16:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.02, #running-req: 6, #queue-req: 52, 
[2025-12-14 10:16:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-12-14 10:16:35 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-12-14 10:16:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-12-14 10:16:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.08, #running-req: 26, #queue-req: 33, 
[2025-12-14 10:16:38 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.10, #running-req: 31, #queue-req: 27, 
[2025-12-14 10:16:38 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.12, #running-req: 37, #queue-req: 21, 
[2025-12-14 10:16:39 TP0] Prefill batch, #new-seq: 6, #new-token: 15986, #cached-token: 3220, token usage: 0.14, #running-req: 43, #queue-req: 15, 
[2025-12-14 10:16:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.16, #running-req: 49, #queue-req: 10, 
[2025-12-14 10:16:41 TP0] Prefill batch, #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.18, #running-req: 54, #queue-req: 4, 
[2025-12-14 10:16:42 TP0] Prefill batch, #new-seq: 4, #new-token: 12793, #cached-token: 11, token usage: 0.20, #running-req: 60, #queue-req: 0, 
[2025-12-14 10:16:45 TP0] Decode batch, #running-req: 64, #token: 207016, token usage: 0.21, cuda graph: True, gen throughput (token/s): 170.52, #queue-req: 0, 
[2025-12-14 10:16:47 TP0] Decode batch, #running-req: 64, #token: 209576, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1217.82, #queue-req: 0, 
[2025-12-14 10:16:49 TP0] Decode batch, #running-req: 64, #token: 212136, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1211.23, #queue-req: 0, 
[2025-12-14 10:16:51 TP0] Decode batch, #running-req: 64, #token: 214696, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1197.54, #queue-req: 0, 
[2025-12-14 10:16:54 TP0] Decode batch, #running-req: 64, #token: 217256, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1193.67, #queue-req: 0, 
[2025-12-14 10:16:56 TP0] Decode batch, #running-req: 64, #token: 219816, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1189.78, #queue-req: 0, 
[2025-12-14 10:16:58 TP0] Decode batch, #running-req: 64, #token: 222376, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1187.90, #queue-req: 0, 
[2025-12-14 10:17:00 TP0] Decode batch, #running-req: 64, #token: 224936, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1185.41, #queue-req: 0, 
[2025-12-14 10:17:02 TP0] Decode batch, #running-req: 64, #token: 227496, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1181.04, #queue-req: 0, 
[2025-12-14 10:17:04 TP0] Decode batch, #running-req: 64, #token: 230056, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1176.02, #queue-req: 0, 
[2025-12-14 10:17:07 TP0] Decode batch, #running-req: 64, #token: 232616, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1172.00, #queue-req: 0, 
[2025-12-14 10:17:09 TP0] Decode batch, #running-req: 64, #token: 235176, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1172.85, #queue-req: 0, 
[2025-12-14 10:17:11 TP0] Decode batch, #running-req: 64, #token: 237736, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1170.17, #queue-req: 0, 
[2025-12-14 10:17:13 TP0] Decode batch, #running-req: 64, #token: 240296, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1169.49, #queue-req: 0, 
[2025-12-14 10:17:15 TP0] Decode batch, #running-req: 64, #token: 242856, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1167.74, #queue-req: 0, 
[2025-12-14 10:17:18 TP0] Decode batch, #running-req: 64, #token: 245416, token usage: 0.25, cuda graph: True, gen throughput (token/s): 812.49, #queue-req: 0, 
[2025-12-14 10:17:23 TP0] Decode batch, #running-req: 64, #token: 247976, token usage: 0.25, cuda graph: True, gen throughput (token/s): 512.96, #queue-req: 0, 
[2025-12-14 10:17:28 TP0] Decode batch, #running-req: 64, #token: 250536, token usage: 0.26, cuda graph: True, gen throughput (token/s): 512.92, #queue-req: 0, 
[2025-12-14 10:17:33 TP0] Decode batch, #running-req: 64, #token: 253096, token usage: 0.26, cuda graph: True, gen throughput (token/s): 512.76, #queue-req: 0, 
[2025-12-14 10:17:38 TP0] Decode batch, #running-req: 64, #token: 255656, token usage: 0.26, cuda graph: True, gen throughput (token/s): 512.99, #queue-req: 0, 
[2025-12-14 10:17:39] INFO:     127.0.0.1:40798 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40824 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 3, 
[2025-12-14 10:17:39] INFO:     127.0.0.1:40912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40938 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40950 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40960 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:40996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41036 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41176 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41286 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:39] INFO:     127.0.0.1:41372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:17:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 5, #queue-req: 54, 
[2025-12-14 10:17:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.03, #running-req: 10, #queue-req: 49, 
[2025-12-14 10:17:43 TP0] Prefill batch, #new-seq: 6, #new-token: 15984, #cached-token: 3222, token usage: 0.05, #running-req: 15, #queue-req: 43, 
[2025-12-14 10:17:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-12-14 10:17:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.08, #running-req: 26, #queue-req: 33, 
[2025-12-14 10:17:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-12-14 10:17:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-12-14 10:17:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-12-14 10:17:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-12-14 10:17:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-12-14 10:17:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-12-14 10:17:56 TP0] Prefill batch, #new-seq: 3, #new-token: 9591, #cached-token: 12, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-12-14 10:18:05 TP0] Decode batch, #running-req: 64, #token: 206999, token usage: 0.21, cuda graph: True, gen throughput (token/s): 97.81, #queue-req: 0, 
[2025-12-14 10:18:09 TP0] Decode batch, #running-req: 64, #token: 209559, token usage: 0.21, cuda graph: True, gen throughput (token/s): 532.92, #queue-req: 0, 
[2025-12-14 10:18:14 TP0] Decode batch, #running-req: 64, #token: 212119, token usage: 0.22, cuda graph: True, gen throughput (token/s): 532.30, #queue-req: 0, 
[2025-12-14 10:18:19 TP0] Decode batch, #running-req: 64, #token: 214679, token usage: 0.22, cuda graph: True, gen throughput (token/s): 529.24, #queue-req: 0, 
[2025-12-14 10:18:24 TP0] Decode batch, #running-req: 64, #token: 217239, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.89, #queue-req: 0, 
[2025-12-14 10:18:29 TP0] Decode batch, #running-req: 64, #token: 219799, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.98, #queue-req: 0, 
[2025-12-14 10:18:34 TP0] Decode batch, #running-req: 64, #token: 222359, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.54, #queue-req: 0, 
[2025-12-14 10:18:38 TP0] Decode batch, #running-req: 64, #token: 224919, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.95, #queue-req: 0, 
[2025-12-14 10:18:43 TP0] Decode batch, #running-req: 64, #token: 227479, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.90, #queue-req: 0, 
[2025-12-14 10:18:48 TP0] Decode batch, #running-req: 64, #token: 230039, token usage: 0.23, cuda graph: True, gen throughput (token/s): 525.28, #queue-req: 0, 
[2025-12-14 10:18:53 TP0] Decode batch, #running-req: 64, #token: 232599, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.21, #queue-req: 0, 
[2025-12-14 10:18:58 TP0] Decode batch, #running-req: 64, #token: 235159, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.09, #queue-req: 0, 
[2025-12-14 10:19:03 TP0] Decode batch, #running-req: 64, #token: 237719, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.59, #queue-req: 0, 
[2025-12-14 10:19:08 TP0] Decode batch, #running-req: 64, #token: 240279, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.78, #queue-req: 0, 
[2025-12-14 10:19:13 TP0] Decode batch, #running-req: 64, #token: 242839, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.73, #queue-req: 0, 
[2025-12-14 10:19:18 TP0] Decode batch, #running-req: 64, #token: 245399, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.60, #queue-req: 0, 
[2025-12-14 10:19:23 TP0] Decode batch, #running-req: 64, #token: 247959, token usage: 0.25, cuda graph: True, gen throughput (token/s): 517.08, #queue-req: 0, 
[2025-12-14 10:19:28 TP0] Decode batch, #running-req: 64, #token: 250519, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.06, #queue-req: 0, 
[2025-12-14 10:19:32 TP0] Decode batch, #running-req: 64, #token: 253079, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.73, #queue-req: 0, 
[2025-12-14 10:19:37 TP0] Decode batch, #running-req: 64, #token: 255639, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.04, #queue-req: 0, 
[2025-12-14 10:19:38] INFO:     127.0.0.1:57308 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 2, 
[2025-12-14 10:19:38] INFO:     127.0.0.1:57364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57372 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57540 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57556 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57800 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57894 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:38] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:19:39 TP0] Prefill batch, #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.02, #running-req: 5, #queue-req: 53, 
[2025-12-14 10:19:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-12-14 10:19:42 TP0] Prefill batch, #new-seq: 7, #new-token: 15992, #cached-token: 6415, token usage: 0.06, #running-req: 16, #queue-req: 41, 
[2025-12-14 10:19:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 23, #queue-req: 36, 
[2025-12-14 10:19:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.09, #running-req: 28, #queue-req: 31, 
[2025-12-14 10:19:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.11, #running-req: 33, #queue-req: 26, 
[2025-12-14 10:19:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.12, #running-req: 38, #queue-req: 21, 
[2025-12-14 10:19:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.14, #running-req: 43, #queue-req: 16, 
[2025-12-14 10:19:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.16, #running-req: 48, #queue-req: 11, 
[2025-12-14 10:19:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.17, #running-req: 53, #queue-req: 6, 
[2025-12-14 10:19:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.19, #running-req: 58, #queue-req: 1, 
[2025-12-14 10:19:56 TP0] Prefill batch, #new-seq: 1, #new-token: 3198, #cached-token: 3, token usage: 0.21, #running-req: 63, #queue-req: 0, 
[2025-12-14 10:20:01 TP0] Decode batch, #running-req: 64, #token: 207013, token usage: 0.21, cuda graph: True, gen throughput (token/s): 107.47, #queue-req: 0, 
[2025-12-14 10:20:06 TP0] Decode batch, #running-req: 64, #token: 209573, token usage: 0.21, cuda graph: True, gen throughput (token/s): 534.23, #queue-req: 0, 
[2025-12-14 10:20:11 TP0] Decode batch, #running-req: 64, #token: 212133, token usage: 0.22, cuda graph: True, gen throughput (token/s): 533.42, #queue-req: 0, 
[2025-12-14 10:20:16 TP0] Decode batch, #running-req: 64, #token: 214693, token usage: 0.22, cuda graph: True, gen throughput (token/s): 529.67, #queue-req: 0, 
[2025-12-14 10:20:21 TP0] Decode batch, #running-req: 64, #token: 217253, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.05, #queue-req: 0, 
[2025-12-14 10:20:25 TP0] Decode batch, #running-req: 64, #token: 219813, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.48, #queue-req: 0, 
[2025-12-14 10:20:30 TP0] Decode batch, #running-req: 64, #token: 222373, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.30, #queue-req: 0, 
[2025-12-14 10:20:35 TP0] Decode batch, #running-req: 64, #token: 224933, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.25, #queue-req: 0, 
[2025-12-14 10:20:40 TP0] Decode batch, #running-req: 64, #token: 227493, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.43, #queue-req: 0, 
[2025-12-14 10:20:45 TP0] Decode batch, #running-req: 64, #token: 230053, token usage: 0.23, cuda graph: True, gen throughput (token/s): 525.00, #queue-req: 0, 
[2025-12-14 10:20:50 TP0] Decode batch, #running-req: 64, #token: 232613, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.44, #queue-req: 0, 
[2025-12-14 10:20:55 TP0] Decode batch, #running-req: 64, #token: 235173, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.43, #queue-req: 0, 
[2025-12-14 10:21:00 TP0] Decode batch, #running-req: 64, #token: 237733, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.36, #queue-req: 0, 
[2025-12-14 10:21:04 TP0] Decode batch, #running-req: 64, #token: 240293, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.32, #queue-req: 0, 
[2025-12-14 10:21:09 TP0] Decode batch, #running-req: 64, #token: 242853, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.67, #queue-req: 0, 
[2025-12-14 10:21:14 TP0] Decode batch, #running-req: 64, #token: 245413, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.51, #queue-req: 0, 
[2025-12-14 10:21:19 TP0] Decode batch, #running-req: 64, #token: 247973, token usage: 0.25, cuda graph: True, gen throughput (token/s): 517.11, #queue-req: 0, 
[2025-12-14 10:21:24 TP0] Decode batch, #running-req: 64, #token: 250533, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.60, #queue-req: 0, 
[2025-12-14 10:21:29 TP0] Decode batch, #running-req: 64, #token: 253093, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.59, #queue-req: 0, 
[2025-12-14 10:21:34 TP0] Decode batch, #running-req: 64, #token: 255653, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.85, #queue-req: 0, 
[2025-12-14 10:21:35] INFO:     127.0.0.1:40552 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40564 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40574 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40622 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40634 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.00, #running-req: 0, #queue-req: 1, 
[2025-12-14 10:21:35] INFO:     127.0.0.1:40650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40716 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40794 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40854 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40890 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40920 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40988 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:40992 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41054 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41136 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41142 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41170 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35] INFO:     127.0.0.1:41180 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:21:35 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.02, #running-req: 6, #queue-req: 53, 
[2025-12-14 10:21:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-12-14 10:21:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-12-14 10:21:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-12-14 10:21:41 TP0] Prefill batch, #new-seq: 7, #new-token: 15992, #cached-token: 6415, token usage: 0.09, #running-req: 26, #queue-req: 31, 
[2025-12-14 10:21:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.11, #running-req: 33, #queue-req: 26, 
[2025-12-14 10:21:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 38, #queue-req: 21, 
[2025-12-14 10:21:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.14, #running-req: 43, #queue-req: 16, 
[2025-12-14 10:21:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.16, #running-req: 48, #queue-req: 11, 
[2025-12-14 10:21:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.17, #running-req: 53, #queue-req: 6, 
[2025-12-14 10:21:50 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.19, #running-req: 58, #queue-req: 0, 
[2025-12-14 10:21:57 TP0] Decode batch, #running-req: 64, #token: 203802, token usage: 0.21, cuda graph: True, gen throughput (token/s): 110.72, #queue-req: 0, 
[2025-12-14 10:22:02 TP0] Decode batch, #running-req: 64, #token: 206362, token usage: 0.21, cuda graph: True, gen throughput (token/s): 533.38, #queue-req: 0, 
[2025-12-14 10:22:07 TP0] Decode batch, #running-req: 64, #token: 208922, token usage: 0.21, cuda graph: True, gen throughput (token/s): 532.36, #queue-req: 0, 
[2025-12-14 10:22:12 TP0] Decode batch, #running-req: 64, #token: 211482, token usage: 0.22, cuda graph: True, gen throughput (token/s): 529.47, #queue-req: 0, 
[2025-12-14 10:22:17 TP0] Decode batch, #running-req: 64, #token: 214042, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.07, #queue-req: 0, 
[2025-12-14 10:22:21 TP0] Decode batch, #running-req: 64, #token: 216602, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.86, #queue-req: 0, 
[2025-12-14 10:22:26 TP0] Decode batch, #running-req: 64, #token: 219162, token usage: 0.22, cuda graph: True, gen throughput (token/s): 526.75, #queue-req: 0, 
[2025-12-14 10:22:31 TP0] Decode batch, #running-req: 64, #token: 221722, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.91, #queue-req: 0, 
[2025-12-14 10:22:36 TP0] Decode batch, #running-req: 64, #token: 224282, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.88, #queue-req: 0, 
[2025-12-14 10:22:41 TP0] Decode batch, #running-req: 64, #token: 226842, token usage: 0.23, cuda graph: True, gen throughput (token/s): 525.43, #queue-req: 0, 
[2025-12-14 10:22:46 TP0] Decode batch, #running-req: 64, #token: 229402, token usage: 0.23, cuda graph: True, gen throughput (token/s): 521.53, #queue-req: 0, 
[2025-12-14 10:22:51 TP0] Decode batch, #running-req: 64, #token: 231962, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.83, #queue-req: 0, 
[2025-12-14 10:22:56 TP0] Decode batch, #running-req: 64, #token: 234522, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.45, #queue-req: 0, 
[2025-12-14 10:23:00 TP0] Decode batch, #running-req: 64, #token: 237082, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.71, #queue-req: 0, 
[2025-12-14 10:23:05 TP0] Decode batch, #running-req: 64, #token: 239642, token usage: 0.24, cuda graph: True, gen throughput (token/s): 521.79, #queue-req: 0, 
[2025-12-14 10:23:10 TP0] Decode batch, #running-req: 64, #token: 242202, token usage: 0.25, cuda graph: True, gen throughput (token/s): 521.74, #queue-req: 0, 
[2025-12-14 10:23:15 TP0] Decode batch, #running-req: 64, #token: 244762, token usage: 0.25, cuda graph: True, gen throughput (token/s): 517.47, #queue-req: 0, 
[2025-12-14 10:23:20 TP0] Decode batch, #running-req: 64, #token: 247322, token usage: 0.25, cuda graph: True, gen throughput (token/s): 516.76, #queue-req: 0, 
[2025-12-14 10:23:25 TP0] Decode batch, #running-req: 64, #token: 249882, token usage: 0.25, cuda graph: True, gen throughput (token/s): 517.04, #queue-req: 0, 
[2025-12-14 10:23:30 TP0] Decode batch, #running-req: 64, #token: 252442, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.17, #queue-req: 0, 
[2025-12-14 10:23:31] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.00, #running-req: 0, #queue-req: 4, 
[2025-12-14 10:23:31] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56506 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56740 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56742 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56808 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56900 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:23:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.02, #running-req: 5, #queue-req: 54, 
[2025-12-14 10:23:33 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.03, #running-req: 10, #queue-req: 49, 
[2025-12-14 10:23:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.05, #running-req: 15, #queue-req: 44, 
[2025-12-14 10:23:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 20, #queue-req: 39, 
[2025-12-14 10:23:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.08, #running-req: 25, #queue-req: 34, 
[2025-12-14 10:23:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.10, #running-req: 30, #queue-req: 29, 
[2025-12-14 10:23:40 TP0] Prefill batch, #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.12, #running-req: 35, #queue-req: 23, 
[2025-12-14 10:23:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-12-14 10:23:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-12-14 10:23:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-12-14 10:23:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-12-14 10:23:48 TP0] Prefill batch, #new-seq: 3, #new-token: 6391, #cached-token: 3212, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-12-14 10:23:54 TP0] Decode batch, #running-req: 64, #token: 207002, token usage: 0.21, cuda graph: True, gen throughput (token/s): 107.44, #queue-req: 0, 
[2025-12-14 10:23:59 TP0] Decode batch, #running-req: 64, #token: 209562, token usage: 0.21, cuda graph: True, gen throughput (token/s): 533.95, #queue-req: 0, 
[2025-12-14 10:24:04 TP0] Decode batch, #running-req: 64, #token: 212122, token usage: 0.22, cuda graph: True, gen throughput (token/s): 533.40, #queue-req: 0, 
[2025-12-14 10:24:08 TP0] Decode batch, #running-req: 64, #token: 214682, token usage: 0.22, cuda graph: True, gen throughput (token/s): 529.78, #queue-req: 0, 
[2025-12-14 10:24:13 TP0] Decode batch, #running-req: 64, #token: 217242, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.24, #queue-req: 0, 
[2025-12-14 10:24:18 TP0] Decode batch, #running-req: 64, #token: 219802, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.08, #queue-req: 0, 
[2025-12-14 10:24:23 TP0] Decode batch, #running-req: 64, #token: 222362, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.71, #queue-req: 0, 
[2025-12-14 10:24:28 TP0] Decode batch, #running-req: 64, #token: 224922, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.79, #queue-req: 0, 
[2025-12-14 10:24:33 TP0] Decode batch, #running-req: 64, #token: 227482, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.93, #queue-req: 0, 
[2025-12-14 10:24:38 TP0] Decode batch, #running-req: 64, #token: 230042, token usage: 0.23, cuda graph: True, gen throughput (token/s): 525.53, #queue-req: 0, 
[2025-12-14 10:24:42 TP0] Decode batch, #running-req: 64, #token: 232602, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.14, #queue-req: 0, 
[2025-12-14 10:24:47 TP0] Decode batch, #running-req: 64, #token: 235162, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.46, #queue-req: 0, 
[2025-12-14 10:24:52 TP0] Decode batch, #running-req: 64, #token: 237722, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.61, #queue-req: 0, 
[2025-12-14 10:24:57 TP0] Decode batch, #running-req: 64, #token: 240282, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.90, #queue-req: 0, 
[2025-12-14 10:25:02 TP0] Decode batch, #running-req: 64, #token: 242842, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.92, #queue-req: 0, 
[2025-12-14 10:25:07 TP0] Decode batch, #running-req: 64, #token: 245402, token usage: 0.25, cuda graph: True, gen throughput (token/s): 523.19, #queue-req: 0, 
[2025-12-14 10:25:12 TP0] Decode batch, #running-req: 64, #token: 247962, token usage: 0.25, cuda graph: True, gen throughput (token/s): 518.36, #queue-req: 0, 
[2025-12-14 10:25:17 TP0] Decode batch, #running-req: 64, #token: 250522, token usage: 0.26, cuda graph: True, gen throughput (token/s): 518.09, #queue-req: 0, 
[2025-12-14 10:25:22 TP0] Decode batch, #running-req: 64, #token: 253082, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.77, #queue-req: 0, 
[2025-12-14 10:25:27 TP0] Decode batch, #running-req: 64, #token: 255642, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.81, #queue-req: 0, 
[2025-12-14 10:25:27] INFO:     127.0.0.1:48664 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.00, #running-req: 0, #queue-req: 3, 
[2025-12-14 10:25:27] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48814 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48828 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48838 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48904 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48932 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48986 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:48994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:49004 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:49022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:49030 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:49042 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:49046 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:49056 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:49072 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:49088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:49106 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:27] INFO:     127.0.0.1:49118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49128 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49178 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49212 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28] INFO:     127.0.0.1:49296 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:25:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 5, #queue-req: 54, 
[2025-12-14 10:25:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.03, #running-req: 10, #queue-req: 49, 
[2025-12-14 10:25:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 15, #queue-req: 44, 
[2025-12-14 10:25:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.07, #running-req: 20, #queue-req: 39, 
[2025-12-14 10:25:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.08, #running-req: 25, #queue-req: 34, 
[2025-12-14 10:25:35 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.10, #running-req: 30, #queue-req: 29, 
[2025-12-14 10:25:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.11, #running-req: 35, #queue-req: 24, 
[2025-12-14 10:25:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.13, #running-req: 40, #queue-req: 19, 
[2025-12-14 10:25:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.15, #running-req: 45, #queue-req: 14, 
[2025-12-14 10:25:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.16, #running-req: 50, #queue-req: 9, 
[2025-12-14 10:25:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.18, #running-req: 55, #queue-req: 4, 
[2025-12-14 10:25:45 TP0] Prefill batch, #new-seq: 4, #new-token: 12795, #cached-token: 9, token usage: 0.20, #running-req: 60, #queue-req: 0, 
[2025-12-14 10:25:51 TP0] Decode batch, #running-req: 64, #token: 207016, token usage: 0.21, cuda graph: True, gen throughput (token/s): 105.42, #queue-req: 0, 
[2025-12-14 10:25:56 TP0] Decode batch, #running-req: 64, #token: 209576, token usage: 0.21, cuda graph: True, gen throughput (token/s): 534.40, #queue-req: 0, 
[2025-12-14 10:26:01 TP0] Decode batch, #running-req: 64, #token: 212136, token usage: 0.22, cuda graph: True, gen throughput (token/s): 533.33, #queue-req: 0, 
[2025-12-14 10:26:05 TP0] Decode batch, #running-req: 64, #token: 214696, token usage: 0.22, cuda graph: True, gen throughput (token/s): 529.38, #queue-req: 0, 
[2025-12-14 10:26:10 TP0] Decode batch, #running-req: 64, #token: 217256, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.01, #queue-req: 0, 
[2025-12-14 10:26:15 TP0] Decode batch, #running-req: 64, #token: 219816, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.07, #queue-req: 0, 
[2025-12-14 10:26:20 TP0] Decode batch, #running-req: 64, #token: 222376, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.84, #queue-req: 0, 
[2025-12-14 10:26:25 TP0] Decode batch, #running-req: 64, #token: 224936, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.91, #queue-req: 0, 
[2025-12-14 10:26:30 TP0] Decode batch, #running-req: 64, #token: 227496, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.96, #queue-req: 0, 
[2025-12-14 10:26:35 TP0] Decode batch, #running-req: 64, #token: 230056, token usage: 0.23, cuda graph: True, gen throughput (token/s): 525.54, #queue-req: 0, 
[2025-12-14 10:26:39 TP0] Decode batch, #running-req: 64, #token: 232616, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.21, #queue-req: 0, 
[2025-12-14 10:26:44 TP0] Decode batch, #running-req: 64, #token: 235176, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.06, #queue-req: 0, 
[2025-12-14 10:26:49 TP0] Decode batch, #running-req: 64, #token: 237736, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.54, #queue-req: 0, 
[2025-12-14 10:26:54 TP0] Decode batch, #running-req: 64, #token: 240296, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.62, #queue-req: 0, 
[2025-12-14 10:26:59 TP0] Decode batch, #running-req: 64, #token: 242856, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.21, #queue-req: 0, 
[2025-12-14 10:27:04 TP0] Decode batch, #running-req: 64, #token: 245416, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.46, #queue-req: 0, 
[2025-12-14 10:27:09 TP0] Decode batch, #running-req: 64, #token: 247976, token usage: 0.25, cuda graph: True, gen throughput (token/s): 517.53, #queue-req: 0, 
[2025-12-14 10:27:14 TP0] Decode batch, #running-req: 64, #token: 250536, token usage: 0.26, cuda graph: True, gen throughput (token/s): 516.99, #queue-req: 0, 
[2025-12-14 10:27:19 TP0] Decode batch, #running-req: 64, #token: 253096, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.13, #queue-req: 0, 
[2025-12-14 10:27:24 TP0] Decode batch, #running-req: 64, #token: 255656, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.05, #queue-req: 0, 
[2025-12-14 10:27:24] INFO:     127.0.0.1:60076 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60098 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.00, #running-req: 0, #queue-req: 3, 
[2025-12-14 10:27:24] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60204 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60218 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60272 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60284 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:24] INFO:     127.0.0.1:60342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60382 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60402 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60418 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60428 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60636 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25] INFO:     127.0.0.1:60672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:27:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.02, #running-req: 5, #queue-req: 54, 
[2025-12-14 10:27:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.03, #running-req: 10, #queue-req: 49, 
[2025-12-14 10:27:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 15, #queue-req: 44, 
[2025-12-14 10:27:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 20, #queue-req: 39, 
[2025-12-14 10:27:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.08, #running-req: 25, #queue-req: 34, 
[2025-12-14 10:27:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.10, #running-req: 30, #queue-req: 29, 
[2025-12-14 10:27:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.11, #running-req: 35, #queue-req: 24, 
[2025-12-14 10:27:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.13, #running-req: 40, #queue-req: 19, 
[2025-12-14 10:27:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.15, #running-req: 45, #queue-req: 14, 
[2025-12-14 10:27:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.16, #running-req: 50, #queue-req: 9, 
[2025-12-14 10:27:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.18, #running-req: 55, #queue-req: 4, 
[2025-12-14 10:27:42 TP0] Prefill batch, #new-seq: 4, #new-token: 12791, #cached-token: 13, token usage: 0.20, #running-req: 60, #queue-req: 0, 
[2025-12-14 10:27:48 TP0] Decode batch, #running-req: 64, #token: 207006, token usage: 0.21, cuda graph: True, gen throughput (token/s): 105.37, #queue-req: 0, 
[2025-12-14 10:27:53 TP0] Decode batch, #running-req: 64, #token: 209566, token usage: 0.21, cuda graph: True, gen throughput (token/s): 534.75, #queue-req: 0, 
[2025-12-14 10:27:58 TP0] Decode batch, #running-req: 64, #token: 212126, token usage: 0.22, cuda graph: True, gen throughput (token/s): 534.46, #queue-req: 0, 
[2025-12-14 10:28:02 TP0] Decode batch, #running-req: 64, #token: 214686, token usage: 0.22, cuda graph: True, gen throughput (token/s): 530.72, #queue-req: 0, 
[2025-12-14 10:28:07 TP0] Decode batch, #running-req: 64, #token: 217246, token usage: 0.22, cuda graph: True, gen throughput (token/s): 528.10, #queue-req: 0, 
[2025-12-14 10:28:12 TP0] Decode batch, #running-req: 64, #token: 219806, token usage: 0.22, cuda graph: True, gen throughput (token/s): 527.54, #queue-req: 0, 
[2025-12-14 10:28:17 TP0] Decode batch, #running-req: 64, #token: 222366, token usage: 0.23, cuda graph: True, gen throughput (token/s): 527.37, #queue-req: 0, 
[2025-12-14 10:28:22 TP0] Decode batch, #running-req: 64, #token: 224926, token usage: 0.23, cuda graph: True, gen throughput (token/s): 527.29, #queue-req: 0, 
[2025-12-14 10:28:27 TP0] Decode batch, #running-req: 64, #token: 227486, token usage: 0.23, cuda graph: True, gen throughput (token/s): 527.56, #queue-req: 0, 
[2025-12-14 10:28:32 TP0] Decode batch, #running-req: 64, #token: 230046, token usage: 0.23, cuda graph: True, gen throughput (token/s): 526.02, #queue-req: 0, 
[2025-12-14 10:28:36 TP0] Decode batch, #running-req: 64, #token: 232606, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.85, #queue-req: 0, 
[2025-12-14 10:28:41 TP0] Decode batch, #running-req: 64, #token: 235166, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.89, #queue-req: 0, 
[2025-12-14 10:28:46 TP0] Decode batch, #running-req: 64, #token: 237726, token usage: 0.24, cuda graph: True, gen throughput (token/s): 523.01, #queue-req: 0, 
[2025-12-14 10:28:51 TP0] Decode batch, #running-req: 64, #token: 240286, token usage: 0.24, cuda graph: True, gen throughput (token/s): 522.95, #queue-req: 0, 
[2025-12-14 10:28:56 TP0] Decode batch, #running-req: 64, #token: 242846, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.58, #queue-req: 0, 
[2025-12-14 10:29:01 TP0] Decode batch, #running-req: 64, #token: 245406, token usage: 0.25, cuda graph: True, gen throughput (token/s): 522.92, #queue-req: 0, 
[2025-12-14 10:29:06 TP0] Decode batch, #running-req: 64, #token: 247966, token usage: 0.25, cuda graph: True, gen throughput (token/s): 518.06, #queue-req: 0, 
[2025-12-14 10:29:11 TP0] Decode batch, #running-req: 64, #token: 250526, token usage: 0.26, cuda graph: True, gen throughput (token/s): 517.96, #queue-req: 0, 
[2025-12-14 10:29:16 TP0] Decode batch, #running-req: 64, #token: 253086, token usage: 0.26, cuda graph: True, gen throughput (token/s): 518.05, #queue-req: 0, 
[2025-12-14 10:29:21 TP0] Decode batch, #running-req: 64, #token: 255646, token usage: 0.26, cuda graph: True, gen throughput (token/s): 518.21, #queue-req: 0, 
[2025-12-14 10:29:21] INFO:     127.0.0.1:41416 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41434 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41454 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41464 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41480 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.00, #running-req: 0, #queue-req: 3, 
[2025-12-14 10:29:21] INFO:     127.0.0.1:41484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41490 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41502 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41518 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41532 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41548 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41550 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41558 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41598 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41622 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41668 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41674 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41682 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:21] INFO:     127.0.0.1:41692 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41720 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41744 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41758 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41788 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41820 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41832 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41858 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22] INFO:     127.0.0.1:41870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:29:22 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 5, #queue-req: 42, 
[2025-12-14 10:29:23 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.04, #running-req: 10, #queue-req: 36, 
[2025-12-14 10:29:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.05, #running-req: 16, #queue-req: 31, 
[2025-12-14 10:29:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 21, #queue-req: 26, 
[2025-12-14 10:29:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.08, #running-req: 26, #queue-req: 21, 
[2025-12-14 10:29:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.10, #running-req: 31, #queue-req: 16, 
[2025-12-14 10:29:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 36, #queue-req: 11, 
[2025-12-14 10:29:32 TP0] Prefill batch, #new-seq: 6, #new-token: 15992, #cached-token: 3214, token usage: 0.14, #running-req: 41, #queue-req: 5, 
[2025-12-14 10:29:34 TP0] Prefill batch, #new-seq: 5, #new-token: 15979, #cached-token: 26, token usage: 0.15, #running-req: 47, #queue-req: 0, 
[2025-12-14 10:29:41 TP0] Decode batch, #running-req: 52, #token: 168194, token usage: 0.17, cuda graph: True, gen throughput (token/s): 107.95, #queue-req: 0, 
[2025-12-14 10:29:45 TP0] Decode batch, #running-req: 52, #token: 170274, token usage: 0.17, cuda graph: True, gen throughput (token/s): 456.65, #queue-req: 0, 
[2025-12-14 10:29:50 TP0] Decode batch, #running-req: 52, #token: 172354, token usage: 0.18, cuda graph: True, gen throughput (token/s): 456.14, #queue-req: 0, 
[2025-12-14 10:29:54 TP0] Decode batch, #running-req: 52, #token: 174434, token usage: 0.18, cuda graph: True, gen throughput (token/s): 453.41, #queue-req: 0, 
[2025-12-14 10:29:59 TP0] Decode batch, #running-req: 52, #token: 176514, token usage: 0.18, cuda graph: True, gen throughput (token/s): 452.12, #queue-req: 0, 
[2025-12-14 10:30:03 TP0] Decode batch, #running-req: 52, #token: 178594, token usage: 0.18, cuda graph: True, gen throughput (token/s): 451.81, #queue-req: 0, 
[2025-12-14 10:30:08 TP0] Decode batch, #running-req: 52, #token: 180674, token usage: 0.18, cuda graph: True, gen throughput (token/s): 452.07, #queue-req: 0, 
[2025-12-14 10:30:13 TP0] Decode batch, #running-req: 52, #token: 182754, token usage: 0.19, cuda graph: True, gen throughput (token/s): 451.98, #queue-req: 0, 
[2025-12-14 10:30:17 TP0] Decode batch, #running-req: 52, #token: 184834, token usage: 0.19, cuda graph: True, gen throughput (token/s): 451.68, #queue-req: 0, 
[2025-12-14 10:30:22 TP0] Decode batch, #running-req: 52, #token: 186914, token usage: 0.19, cuda graph: True, gen throughput (token/s): 450.72, #queue-req: 0, 
[2025-12-14 10:30:27 TP0] Decode batch, #running-req: 52, #token: 188994, token usage: 0.19, cuda graph: True, gen throughput (token/s): 448.42, #queue-req: 0, 
[2025-12-14 10:30:31 TP0] Decode batch, #running-req: 52, #token: 191074, token usage: 0.19, cuda graph: True, gen throughput (token/s): 448.44, #queue-req: 0, 
[2025-12-14 10:30:36 TP0] Decode batch, #running-req: 52, #token: 193154, token usage: 0.20, cuda graph: True, gen throughput (token/s): 448.44, #queue-req: 0, 
[2025-12-14 10:30:40 TP0] Decode batch, #running-req: 52, #token: 195234, token usage: 0.20, cuda graph: True, gen throughput (token/s): 448.52, #queue-req: 0, 
[2025-12-14 10:30:45 TP0] Decode batch, #running-req: 52, #token: 197314, token usage: 0.20, cuda graph: True, gen throughput (token/s): 448.13, #queue-req: 0, 
[2025-12-14 10:30:50 TP0] Decode batch, #running-req: 52, #token: 199394, token usage: 0.20, cuda graph: True, gen throughput (token/s): 448.30, #queue-req: 0, 
[2025-12-14 10:30:54 TP0] Decode batch, #running-req: 52, #token: 201474, token usage: 0.21, cuda graph: True, gen throughput (token/s): 444.98, #queue-req: 0, 
[2025-12-14 10:30:59 TP0] Decode batch, #running-req: 52, #token: 203554, token usage: 0.21, cuda graph: True, gen throughput (token/s): 444.69, #queue-req: 0, 
[2025-12-14 10:31:04 TP0] Decode batch, #running-req: 52, #token: 205634, token usage: 0.21, cuda graph: True, gen throughput (token/s): 444.66, #queue-req: 0, 
[2025-12-14 10:31:08 TP0] Decode batch, #running-req: 52, #token: 207714, token usage: 0.21, cuda graph: True, gen throughput (token/s): 444.80, #queue-req: 0, 
[2025-12-14 10:31:09] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-14 10:31:09] INFO:     127.0.0.1:37192 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-14 10:31:10] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-14 10:31:10] INFO:     127.0.0.1:37200 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-14 10:31:20] INFO:     127.0.0.1:48796 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-14 10:31:27] INFO:     127.0.0.1:44964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:27 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:31:29] INFO:     127.0.0.1:44978 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:31:29] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:44996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:45008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:45014 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:45024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:45038 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:45052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:45064 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:45080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:45082 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:45094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:45110 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:45112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29] INFO:     127.0.0.1:45114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-12-14 10:31:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-12-14 10:31:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-12-14 10:31:32 TP0] Decode batch, #running-req: 16, #token: 51250, token usage: 0.05, cuda graph: True, gen throughput (token/s): 14.71, #queue-req: 0, 
[2025-12-14 10:31:33 TP0] Decode batch, #running-req: 16, #token: 51890, token usage: 0.05, cuda graph: True, gen throughput (token/s): 498.80, #queue-req: 0, 
[2025-12-14 10:31:34 TP0] Decode batch, #running-req: 16, #token: 52530, token usage: 0.05, cuda graph: True, gen throughput (token/s): 497.52, #queue-req: 0, 
[2025-12-14 10:31:35 TP0] Decode batch, #running-req: 16, #token: 53170, token usage: 0.05, cuda graph: True, gen throughput (token/s): 496.60, #queue-req: 0, 
[2025-12-14 10:31:37 TP0] Decode batch, #running-req: 16, #token: 53810, token usage: 0.05, cuda graph: True, gen throughput (token/s): 495.67, #queue-req: 0, 
[2025-12-14 10:31:40 TP0] Decode batch, #running-req: 16, #token: 54450, token usage: 0.06, cuda graph: True, gen throughput (token/s): 207.54, #queue-req: 0, 
[2025-12-14 10:31:41 TP0] Decode batch, #running-req: 16, #token: 55090, token usage: 0.06, cuda graph: True, gen throughput (token/s): 443.58, #queue-req: 0, 
[2025-12-14 10:31:42 TP0] Decode batch, #running-req: 16, #token: 55730, token usage: 0.06, cuda graph: True, gen throughput (token/s): 493.23, #queue-req: 0, 
[2025-12-14 10:31:44 TP0] Decode batch, #running-req: 16, #token: 56370, token usage: 0.06, cuda graph: True, gen throughput (token/s): 492.27, #queue-req: 0, 
[2025-12-14 10:31:45 TP0] Decode batch, #running-req: 16, #token: 57010, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.29, #queue-req: 0, 
[2025-12-14 10:31:46 TP0] Decode batch, #running-req: 16, #token: 57650, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.34, #queue-req: 0, 
[2025-12-14 10:31:48 TP0] Decode batch, #running-req: 16, #token: 58290, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.76, #queue-req: 0, 
[2025-12-14 10:31:49 TP0] Decode batch, #running-req: 16, #token: 58930, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.08, #queue-req: 0, 
[2025-12-14 10:31:50 TP0] Decode batch, #running-req: 16, #token: 59570, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.38, #queue-req: 0, 
[2025-12-14 10:31:52 TP0] Decode batch, #running-req: 16, #token: 60210, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.33, #queue-req: 0, 
[2025-12-14 10:31:53 TP0] Decode batch, #running-req: 16, #token: 60850, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.19, #queue-req: 0, 
[2025-12-14 10:31:54 TP0] Decode batch, #running-req: 16, #token: 61490, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.85, #queue-req: 0, 
[2025-12-14 10:31:56 TP0] Decode batch, #running-req: 16, #token: 62130, token usage: 0.06, cuda graph: True, gen throughput (token/s): 488.58, #queue-req: 0, 
[2025-12-14 10:31:57 TP0] Decode batch, #running-req: 16, #token: 62770, token usage: 0.06, cuda graph: True, gen throughput (token/s): 488.13, #queue-req: 0, 
[2025-12-14 10:31:58 TP0] Decode batch, #running-req: 16, #token: 63410, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.36, #queue-req: 0, 
[2025-12-14 10:31:59] INFO:     127.0.0.1:38324 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38336 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:31:59] INFO:     127.0.0.1:38342 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38368 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38394 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38404 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38406 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38438 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:31:59] INFO:     127.0.0.1:38440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-12-14 10:32:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-12-14 10:32:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-12-14 10:32:02 TP0] Decode batch, #running-req: 16, #token: 51244, token usage: 0.05, cuda graph: True, gen throughput (token/s): 157.30, #queue-req: 0, 
[2025-12-14 10:32:04 TP0] Decode batch, #running-req: 16, #token: 51884, token usage: 0.05, cuda graph: True, gen throughput (token/s): 497.52, #queue-req: 0, 
[2025-12-14 10:32:05 TP0] Decode batch, #running-req: 16, #token: 52524, token usage: 0.05, cuda graph: True, gen throughput (token/s): 494.29, #queue-req: 0, 
[2025-12-14 10:32:06 TP0] Decode batch, #running-req: 16, #token: 53164, token usage: 0.05, cuda graph: True, gen throughput (token/s): 493.35, #queue-req: 0, 
[2025-12-14 10:32:07 TP0] Decode batch, #running-req: 16, #token: 53804, token usage: 0.05, cuda graph: True, gen throughput (token/s): 493.30, #queue-req: 0, 
[2025-12-14 10:32:09 TP0] Decode batch, #running-req: 16, #token: 54444, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.48, #queue-req: 0, 
[2025-12-14 10:32:10 TP0] Decode batch, #running-req: 16, #token: 55084, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.26, #queue-req: 0, 
[2025-12-14 10:32:11 TP0] Decode batch, #running-req: 16, #token: 55724, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.52, #queue-req: 0, 
[2025-12-14 10:32:13 TP0] Decode batch, #running-req: 16, #token: 56364, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.84, #queue-req: 0, 
[2025-12-14 10:32:14 TP0] Decode batch, #running-req: 16, #token: 57004, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.66, #queue-req: 0, 
[2025-12-14 10:32:15 TP0] Decode batch, #running-req: 16, #token: 57644, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.32, #queue-req: 0, 
[2025-12-14 10:32:17 TP0] Decode batch, #running-req: 16, #token: 58284, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.99, #queue-req: 0, 
[2025-12-14 10:32:18 TP0] Decode batch, #running-req: 16, #token: 58924, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.73, #queue-req: 0, 
[2025-12-14 10:32:19 TP0] Decode batch, #running-req: 16, #token: 59564, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.62, #queue-req: 0, 
[2025-12-14 10:32:20 TP0] Decode batch, #running-req: 16, #token: 60204, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.53, #queue-req: 0, 
[2025-12-14 10:32:22 TP0] Decode batch, #running-req: 16, #token: 60844, token usage: 0.06, cuda graph: True, gen throughput (token/s): 486.60, #queue-req: 0, 
[2025-12-14 10:32:23 TP0] Decode batch, #running-req: 16, #token: 61484, token usage: 0.06, cuda graph: True, gen throughput (token/s): 486.15, #queue-req: 0, 
[2025-12-14 10:32:24 TP0] Decode batch, #running-req: 16, #token: 62124, token usage: 0.06, cuda graph: True, gen throughput (token/s): 484.70, #queue-req: 0, 
[2025-12-14 10:32:26 TP0] Decode batch, #running-req: 16, #token: 62764, token usage: 0.06, cuda graph: True, gen throughput (token/s): 484.90, #queue-req: 0, 
[2025-12-14 10:32:27 TP0] Decode batch, #running-req: 16, #token: 63404, token usage: 0.06, cuda graph: True, gen throughput (token/s): 483.67, #queue-req: 0, 
[2025-12-14 10:32:28] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:32:28] INFO:     127.0.0.1:56424 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56430 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56456 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56472 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28] INFO:     127.0.0.1:56538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-12-14 10:32:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-12-14 10:32:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-12-14 10:32:31 TP0] Decode batch, #running-req: 16, #token: 51246, token usage: 0.05, cuda graph: True, gen throughput (token/s): 155.80, #queue-req: 0, 
[2025-12-14 10:32:32 TP0] Decode batch, #running-req: 16, #token: 51886, token usage: 0.05, cuda graph: True, gen throughput (token/s): 499.78, #queue-req: 0, 
[2025-12-14 10:32:34 TP0] Decode batch, #running-req: 16, #token: 52526, token usage: 0.05, cuda graph: True, gen throughput (token/s): 499.92, #queue-req: 0, 
[2025-12-14 10:32:35 TP0] Decode batch, #running-req: 16, #token: 53166, token usage: 0.05, cuda graph: True, gen throughput (token/s): 496.98, #queue-req: 0, 
[2025-12-14 10:32:36 TP0] Decode batch, #running-req: 16, #token: 53806, token usage: 0.05, cuda graph: True, gen throughput (token/s): 496.86, #queue-req: 0, 
[2025-12-14 10:32:38 TP0] Decode batch, #running-req: 16, #token: 54446, token usage: 0.06, cuda graph: True, gen throughput (token/s): 495.28, #queue-req: 0, 
[2025-12-14 10:32:39 TP0] Decode batch, #running-req: 16, #token: 55086, token usage: 0.06, cuda graph: True, gen throughput (token/s): 493.37, #queue-req: 0, 
[2025-12-14 10:32:40 TP0] Decode batch, #running-req: 16, #token: 55726, token usage: 0.06, cuda graph: True, gen throughput (token/s): 492.36, #queue-req: 0, 
[2025-12-14 10:32:42 TP0] Decode batch, #running-req: 16, #token: 56366, token usage: 0.06, cuda graph: True, gen throughput (token/s): 491.95, #queue-req: 0, 
[2025-12-14 10:32:43 TP0] Decode batch, #running-req: 16, #token: 57006, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.49, #queue-req: 0, 
[2025-12-14 10:32:44 TP0] Decode batch, #running-req: 16, #token: 57646, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.30, #queue-req: 0, 
[2025-12-14 10:32:45 TP0] Decode batch, #running-req: 16, #token: 58286, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.17, #queue-req: 0, 
[2025-12-14 10:32:47 TP0] Decode batch, #running-req: 16, #token: 58926, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.65, #queue-req: 0, 
[2025-12-14 10:32:48 TP0] Decode batch, #running-req: 16, #token: 59566, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.53, #queue-req: 0, 
[2025-12-14 10:32:49 TP0] Decode batch, #running-req: 16, #token: 60206, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.97, #queue-req: 0, 
[2025-12-14 10:32:51 TP0] Decode batch, #running-req: 16, #token: 60846, token usage: 0.06, cuda graph: True, gen throughput (token/s): 490.27, #queue-req: 0, 
[2025-12-14 10:32:52 TP0] Decode batch, #running-req: 16, #token: 61486, token usage: 0.06, cuda graph: True, gen throughput (token/s): 489.84, #queue-req: 0, 
[2025-12-14 10:32:53 TP0] Decode batch, #running-req: 16, #token: 62126, token usage: 0.06, cuda graph: True, gen throughput (token/s): 488.03, #queue-req: 0, 
[2025-12-14 10:32:55 TP0] Decode batch, #running-req: 16, #token: 62766, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.79, #queue-req: 0, 
[2025-12-14 10:32:56 TP0] Decode batch, #running-req: 16, #token: 63406, token usage: 0.06, cuda graph: True, gen throughput (token/s): 487.60, #queue-req: 0, 
[2025-12-14 10:32:57] INFO:     127.0.0.1:45680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45690 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:32:57] INFO:     127.0.0.1:45694 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45696 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45714 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45728 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45730 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45734 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45748 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45760 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45768 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45776 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45784 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57] INFO:     127.0.0.1:45790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:32:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-12-14 10:32:58 TP0] Prefill batch, #new-seq: 6, #new-token: 15990, #cached-token: 3216, token usage: 0.02, #running-req: 6, #queue-req: 4, 
[2025-12-14 10:32:58 TP0] Prefill batch, #new-seq: 4, #new-token: 12793, #cached-token: 11, token usage: 0.04, #running-req: 12, #queue-req: 0, 
[2025-12-14 10:33:00 TP0] Decode batch, #running-req: 16, #token: 51249, token usage: 0.05, cuda graph: True, gen throughput (token/s): 162.33, #queue-req: 0, 
[2025-12-14 10:33:03 TP0] Decode batch, #running-req: 16, #token: 51889, token usage: 0.05, cuda graph: True, gen throughput (token/s): 207.71, #queue-req: 0, 
[2025-12-14 10:33:07 TP0] Decode batch, #running-req: 16, #token: 52529, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.42, #queue-req: 0, 
[2025-12-14 10:33:11 TP0] Decode batch, #running-req: 16, #token: 53169, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.56, #queue-req: 0, 
[2025-12-14 10:33:15 TP0] Decode batch, #running-req: 16, #token: 53809, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.14, #queue-req: 0, 
[2025-12-14 10:33:18 TP0] Decode batch, #running-req: 16, #token: 54449, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.96, #queue-req: 0, 
[2025-12-14 10:33:22 TP0] Decode batch, #running-req: 16, #token: 55089, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.07, #queue-req: 0, 
[2025-12-14 10:33:26 TP0] Decode batch, #running-req: 16, #token: 55729, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.23, #queue-req: 0, 
[2025-12-14 10:33:30 TP0] Decode batch, #running-req: 16, #token: 56369, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.25, #queue-req: 0, 
[2025-12-14 10:33:34 TP0] Decode batch, #running-req: 16, #token: 57009, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.22, #queue-req: 0, 
[2025-12-14 10:33:38 TP0] Decode batch, #running-req: 16, #token: 57649, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.98, #queue-req: 0, 
[2025-12-14 10:33:42 TP0] Decode batch, #running-req: 16, #token: 58289, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.53, #queue-req: 0, 
[2025-12-14 10:33:46 TP0] Decode batch, #running-req: 16, #token: 58929, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.52, #queue-req: 0, 
[2025-12-14 10:33:49 TP0] Decode batch, #running-req: 16, #token: 59569, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.51, #queue-req: 0, 
[2025-12-14 10:33:53 TP0] Decode batch, #running-req: 16, #token: 60209, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.52, #queue-req: 0, 
[2025-12-14 10:33:57 TP0] Decode batch, #running-req: 16, #token: 60849, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.49, #queue-req: 0, 
[2025-12-14 10:34:01 TP0] Decode batch, #running-req: 16, #token: 61489, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.49, #queue-req: 0, 
[2025-12-14 10:34:05 TP0] Decode batch, #running-req: 16, #token: 62129, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.95, #queue-req: 0, 
[2025-12-14 10:34:09 TP0] Decode batch, #running-req: 16, #token: 62769, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.94, #queue-req: 0, 
[2025-12-14 10:34:13 TP0] Decode batch, #running-req: 16, #token: 63409, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.92, #queue-req: 0, 
[2025-12-14 10:34:16] INFO:     127.0.0.1:40274 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40294 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40310 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40320 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40344 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40380 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.00, #running-req: 0, #queue-req: 7, 
[2025-12-14 10:34:16] INFO:     127.0.0.1:40414 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40422 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:16] INFO:     127.0.0.1:40432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:34:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.02, #running-req: 5, #queue-req: 6, 
[2025-12-14 10:34:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.03, #running-req: 10, #queue-req: 1, 
[2025-12-14 10:34:20 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.05, #running-req: 15, #queue-req: 0, 
[2025-12-14 10:34:22 TP0] Decode batch, #running-req: 16, #token: 51244, token usage: 0.05, cuda graph: True, gen throughput (token/s): 71.78, #queue-req: 0, 
[2025-12-14 10:34:26 TP0] Decode batch, #running-req: 16, #token: 51884, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.84, #queue-req: 0, 
[2025-12-14 10:34:29 TP0] Decode batch, #running-req: 16, #token: 52524, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.75, #queue-req: 0, 
[2025-12-14 10:34:33 TP0] Decode batch, #running-req: 16, #token: 53164, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.76, #queue-req: 0, 
[2025-12-14 10:34:37 TP0] Decode batch, #running-req: 16, #token: 53804, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.37, #queue-req: 0, 
[2025-12-14 10:34:41 TP0] Decode batch, #running-req: 16, #token: 54444, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.22, #queue-req: 0, 
[2025-12-14 10:34:45 TP0] Decode batch, #running-req: 16, #token: 55084, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.23, #queue-req: 0, 
[2025-12-14 10:34:49 TP0] Decode batch, #running-req: 16, #token: 55724, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.09, #queue-req: 0, 
[2025-12-14 10:34:53 TP0] Decode batch, #running-req: 16, #token: 56364, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.31, #queue-req: 0, 
[2025-12-14 10:34:57 TP0] Decode batch, #running-req: 16, #token: 57004, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.21, #queue-req: 0, 
[2025-12-14 10:35:00 TP0] Decode batch, #running-req: 16, #token: 57644, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.96, #queue-req: 0, 
[2025-12-14 10:35:04 TP0] Decode batch, #running-req: 16, #token: 58284, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.68, #queue-req: 0, 
[2025-12-14 10:35:08 TP0] Decode batch, #running-req: 16, #token: 58924, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.68, #queue-req: 0, 
[2025-12-14 10:35:12 TP0] Decode batch, #running-req: 16, #token: 59564, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.69, #queue-req: 0, 
[2025-12-14 10:35:16 TP0] Decode batch, #running-req: 16, #token: 60204, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.66, #queue-req: 0, 
[2025-12-14 10:35:20 TP0] Decode batch, #running-req: 16, #token: 60844, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.65, #queue-req: 0, 
[2025-12-14 10:35:24 TP0] Decode batch, #running-req: 16, #token: 61484, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.60, #queue-req: 0, 
[2025-12-14 10:35:28 TP0] Decode batch, #running-req: 16, #token: 62124, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.11, #queue-req: 0, 
[2025-12-14 10:35:32 TP0] Decode batch, #running-req: 16, #token: 62764, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.11, #queue-req: 0, 
[2025-12-14 10:35:35 TP0] Decode batch, #running-req: 16, #token: 63404, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.13, #queue-req: 0, 
[2025-12-14 10:35:39] INFO:     127.0.0.1:42144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42148 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42152 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42158 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42172 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42186 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42190 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42196 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42226 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42250 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42260 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.00, #running-req: 0, #queue-req: 6, 
[2025-12-14 10:35:39] INFO:     127.0.0.1:42264 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39] INFO:     127.0.0.1:42270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:35:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-12-14 10:35:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-12-14 10:35:44 TP0] Decode batch, #running-req: 16, #token: 51244, token usage: 0.05, cuda graph: True, gen throughput (token/s): 75.46, #queue-req: 0, 
[2025-12-14 10:35:48 TP0] Decode batch, #running-req: 16, #token: 51884, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.87, #queue-req: 0, 
[2025-12-14 10:35:52 TP0] Decode batch, #running-req: 16, #token: 52524, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.79, #queue-req: 0, 
[2025-12-14 10:35:56 TP0] Decode batch, #running-req: 16, #token: 53164, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.79, #queue-req: 0, 
[2025-12-14 10:35:59 TP0] Decode batch, #running-req: 16, #token: 53804, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.47, #queue-req: 0, 
[2025-12-14 10:36:03 TP0] Decode batch, #running-req: 16, #token: 54444, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.25, #queue-req: 0, 
[2025-12-14 10:36:07 TP0] Decode batch, #running-req: 16, #token: 55084, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.22, #queue-req: 0, 
[2025-12-14 10:36:11 TP0] Decode batch, #running-req: 16, #token: 55724, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.24, #queue-req: 0, 
[2025-12-14 10:36:15 TP0] Decode batch, #running-req: 16, #token: 56364, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.25, #queue-req: 0, 
[2025-12-14 10:36:19 TP0] Decode batch, #running-req: 16, #token: 57004, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.23, #queue-req: 0, 
[2025-12-14 10:36:23 TP0] Decode batch, #running-req: 16, #token: 57644, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.01, #queue-req: 0, 
[2025-12-14 10:36:27 TP0] Decode batch, #running-req: 16, #token: 58284, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.70, #queue-req: 0, 
[2025-12-14 10:36:30 TP0] Decode batch, #running-req: 16, #token: 58924, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.66, #queue-req: 0, 
[2025-12-14 10:36:34 TP0] Decode batch, #running-req: 16, #token: 59564, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.69, #queue-req: 0, 
[2025-12-14 10:36:38 TP0] Decode batch, #running-req: 16, #token: 60204, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.68, #queue-req: 0, 
[2025-12-14 10:36:42 TP0] Decode batch, #running-req: 16, #token: 60844, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.73, #queue-req: 0, 
[2025-12-14 10:36:46 TP0] Decode batch, #running-req: 16, #token: 61484, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.65, #queue-req: 0, 
[2025-12-14 10:36:50 TP0] Decode batch, #running-req: 16, #token: 62124, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.22, #queue-req: 0, 
[2025-12-14 10:36:54 TP0] Decode batch, #running-req: 16, #token: 62764, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.21, #queue-req: 0, 
[2025-12-14 10:36:58 TP0] Decode batch, #running-req: 16, #token: 63404, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.20, #queue-req: 0, 
[2025-12-14 10:37:01] INFO:     127.0.0.1:43070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43084 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43094 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43108 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43114 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43126 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43140 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43166 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43184 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43192 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 0, #queue-req: 7, 
[2025-12-14 10:37:01] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:01] INFO:     127.0.0.1:43234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:37:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.02, #running-req: 5, #queue-req: 6, 
[2025-12-14 10:37:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.03, #running-req: 10, #queue-req: 1, 
[2025-12-14 10:37:05 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.05, #running-req: 15, #queue-req: 0, 
[2025-12-14 10:37:07 TP0] Decode batch, #running-req: 16, #token: 51246, token usage: 0.05, cuda graph: True, gen throughput (token/s): 71.82, #queue-req: 0, 
[2025-12-14 10:37:10 TP0] Decode batch, #running-req: 16, #token: 51886, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.67, #queue-req: 0, 
[2025-12-14 10:37:14 TP0] Decode batch, #running-req: 16, #token: 52526, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.58, #queue-req: 0, 
[2025-12-14 10:37:18 TP0] Decode batch, #running-req: 16, #token: 53166, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.54, #queue-req: 0, 
[2025-12-14 10:37:22 TP0] Decode batch, #running-req: 16, #token: 53806, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.21, #queue-req: 0, 
[2025-12-14 10:37:26 TP0] Decode batch, #running-req: 16, #token: 54446, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.08, #queue-req: 0, 
[2025-12-14 10:37:30 TP0] Decode batch, #running-req: 16, #token: 55086, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.05, #queue-req: 0, 
[2025-12-14 10:37:34 TP0] Decode batch, #running-req: 16, #token: 55726, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.04, #queue-req: 0, 
[2025-12-14 10:37:38 TP0] Decode batch, #running-req: 16, #token: 56366, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.03, #queue-req: 0, 
[2025-12-14 10:37:41 TP0] Decode batch, #running-req: 16, #token: 57006, token usage: 0.06, cuda graph: True, gen throughput (token/s): 165.01, #queue-req: 0, 
[2025-12-14 10:37:45 TP0] Decode batch, #running-req: 16, #token: 57646, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.86, #queue-req: 0, 
[2025-12-14 10:37:49 TP0] Decode batch, #running-req: 16, #token: 58286, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.52, #queue-req: 0, 
[2025-12-14 10:37:53 TP0] Decode batch, #running-req: 16, #token: 58926, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.44, #queue-req: 0, 
[2025-12-14 10:37:57 TP0] Decode batch, #running-req: 16, #token: 59566, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.46, #queue-req: 0, 
[2025-12-14 10:38:01 TP0] Decode batch, #running-req: 16, #token: 60206, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.49, #queue-req: 0, 
[2025-12-14 10:38:05 TP0] Decode batch, #running-req: 16, #token: 60846, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.49, #queue-req: 0, 
[2025-12-14 10:38:09 TP0] Decode batch, #running-req: 16, #token: 61486, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.39, #queue-req: 0, 
[2025-12-14 10:38:13 TP0] Decode batch, #running-req: 16, #token: 62126, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.96, #queue-req: 0, 
[2025-12-14 10:38:16 TP0] Decode batch, #running-req: 16, #token: 62766, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.93, #queue-req: 0, 
[2025-12-14 10:38:20 TP0] Decode batch, #running-req: 16, #token: 63406, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.91, #queue-req: 0, 
[2025-12-14 10:38:24] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56352 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56378 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 0, #queue-req: 6, 
[2025-12-14 10:38:24] INFO:     127.0.0.1:56390 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:38:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 5, #queue-req: 6, 
[2025-12-14 10:38:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.03, #running-req: 10, #queue-req: 1, 
[2025-12-14 10:38:27 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.05, #running-req: 15, #queue-req: 0, 
[2025-12-14 10:38:29 TP0] Decode batch, #running-req: 16, #token: 51247, token usage: 0.05, cuda graph: True, gen throughput (token/s): 71.86, #queue-req: 0, 
[2025-12-14 10:38:33 TP0] Decode batch, #running-req: 16, #token: 51887, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.49, #queue-req: 0, 
[2025-12-14 10:38:37 TP0] Decode batch, #running-req: 16, #token: 52527, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.42, #queue-req: 0, 
[2025-12-14 10:38:41 TP0] Decode batch, #running-req: 16, #token: 53167, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.42, #queue-req: 0, 
[2025-12-14 10:38:45 TP0] Decode batch, #running-req: 16, #token: 53807, token usage: 0.05, cuda graph: True, gen throughput (token/s): 165.09, #queue-req: 0, 
[2025-12-14 10:38:49 TP0] Decode batch, #running-req: 16, #token: 54447, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.93, #queue-req: 0, 
[2025-12-14 10:38:53 TP0] Decode batch, #running-req: 16, #token: 55087, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.86, #queue-req: 0, 
[2025-12-14 10:38:56 TP0] Decode batch, #running-req: 16, #token: 55727, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.88, #queue-req: 0, 
[2025-12-14 10:39:00 TP0] Decode batch, #running-req: 16, #token: 56367, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.87, #queue-req: 0, 
[2025-12-14 10:39:04 TP0] Decode batch, #running-req: 16, #token: 57007, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.86, #queue-req: 0, 
[2025-12-14 10:39:08 TP0] Decode batch, #running-req: 16, #token: 57647, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.68, #queue-req: 0, 
[2025-12-14 10:39:12 TP0] Decode batch, #running-req: 16, #token: 58287, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.42, #queue-req: 0, 
[2025-12-14 10:39:16 TP0] Decode batch, #running-req: 16, #token: 58927, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.37, #queue-req: 0, 
[2025-12-14 10:39:20 TP0] Decode batch, #running-req: 16, #token: 59567, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.41, #queue-req: 0, 
[2025-12-14 10:39:24 TP0] Decode batch, #running-req: 16, #token: 60207, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.41, #queue-req: 0, 
[2025-12-14 10:39:28 TP0] Decode batch, #running-req: 16, #token: 60847, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.34, #queue-req: 0, 
[2025-12-14 10:39:31 TP0] Decode batch, #running-req: 16, #token: 61487, token usage: 0.06, cuda graph: True, gen throughput (token/s): 164.30, #queue-req: 0, 
[2025-12-14 10:39:35 TP0] Decode batch, #running-req: 16, #token: 62127, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.85, #queue-req: 0, 
[2025-12-14 10:39:39 TP0] Decode batch, #running-req: 16, #token: 62767, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.82, #queue-req: 0, 
[2025-12-14 10:39:43 TP0] Decode batch, #running-req: 16, #token: 63407, token usage: 0.06, cuda graph: True, gen throughput (token/s): 163.81, #queue-req: 0, 
[2025-12-14 10:39:47] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-14 10:39:47] INFO:     127.0.0.1:42684 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-14 10:39:47] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-14 10:39:47] INFO:     127.0.0.1:42696 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-14 10:39:57] INFO:     127.0.0.1:45058 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-14 10:40:04] INFO:     127.0.0.1:60614 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:40:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:40:04 TP0] Decode batch, #running-req: 1, #token: 3205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 28.66, #queue-req: 0, 
[2025-12-14 10:40:06] INFO:     127.0.0.1:60628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:40:06] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:40:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:40:06] INFO:     127.0.0.1:60642 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:40:06] INFO:     127.0.0.1:60650 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:40:06 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-12-14 10:40:06 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 30.99, #queue-req: 0, 
[2025-12-14 10:40:07 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-14 10:40:08 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-14 10:40:09 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.89, #queue-req: 0, 
[2025-12-14 10:40:10 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-14 10:40:12 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:40:13 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:40:14 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:40:15 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:40:16 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:40:17 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.33, #queue-req: 0, 
[2025-12-14 10:40:18 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-14 10:40:19 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-14 10:40:20 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-14 10:40:21 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:40:22 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.21, #queue-req: 0, 
[2025-12-14 10:40:23 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-14 10:40:24 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-14 10:40:25 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.82, #queue-req: 0, 
[2025-12-14 10:40:26 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.83, #queue-req: 0, 
[2025-12-14 10:40:27] INFO:     127.0.0.1:45432 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:40:27] INFO:     127.0.0.1:45446 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:40:27] INFO:     127.0.0.1:45452 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:40:27] INFO:     127.0.0.1:45458 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:40:27 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:40:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:40:28 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 108.07, #queue-req: 0, 
[2025-12-14 10:40:29 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.94, #queue-req: 0, 
[2025-12-14 10:40:30 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.93, #queue-req: 0, 
[2025-12-14 10:40:31 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.90, #queue-req: 0, 
[2025-12-14 10:40:32 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-14 10:40:33 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-14 10:40:34 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:40:35 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-14 10:40:36 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-14 10:40:37 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:40:38 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.36, #queue-req: 0, 
[2025-12-14 10:40:39 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-14 10:40:40 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-14 10:40:42 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-14 10:40:43 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:40:44 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-14 10:40:45 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.04, #queue-req: 0, 
[2025-12-14 10:40:46 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.76, #queue-req: 0, 
[2025-12-14 10:40:47 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.75, #queue-req: 0, 
[2025-12-14 10:40:48 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.76, #queue-req: 0, 
[2025-12-14 10:40:49] INFO:     127.0.0.1:39604 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:40:49] INFO:     127.0.0.1:39608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:40:49] INFO:     127.0.0.1:39616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:40:49] INFO:     127.0.0.1:39624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:40:49 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:40:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:40:49 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 107.95, #queue-req: 0, 
[2025-12-14 10:40:50 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.93, #queue-req: 0, 
[2025-12-14 10:40:51 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.93, #queue-req: 0, 
[2025-12-14 10:40:53 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.93, #queue-req: 0, 
[2025-12-14 10:40:54 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.67, #queue-req: 0, 
[2025-12-14 10:40:55 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:40:56 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:40:57 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:40:58 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:40:59 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:41:00 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.37, #queue-req: 0, 
[2025-12-14 10:41:01 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-14 10:41:02 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:41:03 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:41:04 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:41:05 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:41:06 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.03, #queue-req: 0, 
[2025-12-14 10:41:07 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.76, #queue-req: 0, 
[2025-12-14 10:41:08 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.77, #queue-req: 0, 
[2025-12-14 10:41:09 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.77, #queue-req: 0, 
[2025-12-14 10:41:10] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:41:10] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:41:10] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:41:10] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:41:10 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:41:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:41:11 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 107.75, #queue-req: 0, 
[2025-12-14 10:41:12 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.99, #queue-req: 0, 
[2025-12-14 10:41:13 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.97, #queue-req: 0, 
[2025-12-14 10:41:14 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.94, #queue-req: 0, 
[2025-12-14 10:41:15 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-14 10:41:16 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:41:17 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:41:18 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:41:19 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:41:20 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:41:21 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.36, #queue-req: 0, 
[2025-12-14 10:41:23 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-14 10:41:24 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:41:25 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:41:26 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:41:27 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:41:28 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-14 10:41:29 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.82, #queue-req: 0, 
[2025-12-14 10:41:30 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.82, #queue-req: 0, 
[2025-12-14 10:41:31 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.83, #queue-req: 0, 
[2025-12-14 10:41:32] INFO:     127.0.0.1:37314 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:41:32] INFO:     127.0.0.1:37318 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:41:32] INFO:     127.0.0.1:37330 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:41:32] INFO:     127.0.0.1:37334 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:41:32 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:41:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:41:33 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 107.35, #queue-req: 0, 
[2025-12-14 10:41:34 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.97, #queue-req: 0, 
[2025-12-14 10:41:35 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-14 10:41:36 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-14 10:41:37 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-14 10:41:38 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:41:39 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-14 10:41:40 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-14 10:41:41 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:41:42 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:41:43 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.38, #queue-req: 0, 
[2025-12-14 10:41:44 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-14 10:41:45 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-14 10:41:46 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-14 10:41:47 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-14 10:41:48 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:41:49 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.03, #queue-req: 0, 
[2025-12-14 10:41:50 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.74, #queue-req: 0, 
[2025-12-14 10:41:52 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.77, #queue-req: 0, 
[2025-12-14 10:41:53 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.74, #queue-req: 0, 
[2025-12-14 10:41:53] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:41:53] INFO:     127.0.0.1:52708 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:41:53] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:41:53 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:41:53] INFO:     127.0.0.1:52726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:41:54 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-14 10:41:54 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 107.91, #queue-req: 0, 
[2025-12-14 10:41:55 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.00, #queue-req: 0, 
[2025-12-14 10:41:56 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-14 10:41:57 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-14 10:41:58 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:41:59 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-14 10:42:00 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:42:01 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-14 10:42:03 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:42:04 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-14 10:42:05 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.37, #queue-req: 0, 
[2025-12-14 10:42:06 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:42:07 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-14 10:42:08 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-14 10:42:09 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:42:10 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:42:11 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.05, #queue-req: 0, 
[2025-12-14 10:42:12 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.74, #queue-req: 0, 
[2025-12-14 10:42:13 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.77, #queue-req: 0, 
[2025-12-14 10:42:14 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.76, #queue-req: 0, 
[2025-12-14 10:42:15] INFO:     127.0.0.1:50488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:42:15] INFO:     127.0.0.1:50500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:42:15] INFO:     127.0.0.1:50508 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:42:15] INFO:     127.0.0.1:50524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:42:15 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:42:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:42:16 TP0] Decode batch, #running-req: 4, #token: 12844, token usage: 0.01, cuda graph: True, gen throughput (token/s): 107.91, #queue-req: 0, 
[2025-12-14 10:42:17 TP0] Decode batch, #running-req: 4, #token: 13004, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.97, #queue-req: 0, 
[2025-12-14 10:42:18 TP0] Decode batch, #running-req: 4, #token: 13164, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.96, #queue-req: 0, 
[2025-12-14 10:42:19 TP0] Decode batch, #running-req: 4, #token: 13324, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.96, #queue-req: 0, 
[2025-12-14 10:42:20 TP0] Decode batch, #running-req: 4, #token: 13484, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-14 10:42:21 TP0] Decode batch, #running-req: 4, #token: 13644, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:42:22 TP0] Decode batch, #running-req: 4, #token: 13804, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:42:23 TP0] Decode batch, #running-req: 4, #token: 13964, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:42:24 TP0] Decode batch, #running-req: 4, #token: 14124, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:42:25 TP0] Decode batch, #running-req: 4, #token: 14284, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:42:26 TP0] Decode batch, #running-req: 4, #token: 14444, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.40, #queue-req: 0, 
[2025-12-14 10:42:27 TP0] Decode batch, #running-req: 4, #token: 14604, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.21, #queue-req: 0, 
[2025-12-14 10:42:28 TP0] Decode batch, #running-req: 4, #token: 14764, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:42:29 TP0] Decode batch, #running-req: 4, #token: 14924, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:42:30 TP0] Decode batch, #running-req: 4, #token: 15084, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:42:31 TP0] Decode batch, #running-req: 4, #token: 15244, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:42:33 TP0] Decode batch, #running-req: 4, #token: 15404, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-14 10:42:34 TP0] Decode batch, #running-req: 4, #token: 15564, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.79, #queue-req: 0, 
[2025-12-14 10:42:35 TP0] Decode batch, #running-req: 4, #token: 15724, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.81, #queue-req: 0, 
[2025-12-14 10:42:36 TP0] Decode batch, #running-req: 4, #token: 15884, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.83, #queue-req: 0, 
[2025-12-14 10:42:36] INFO:     127.0.0.1:40560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:42:36] INFO:     127.0.0.1:40572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:42:36] INFO:     127.0.0.1:40584 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:42:37] INFO:     127.0.0.1:40596 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:42:37 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:42:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:42:37 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 108.95, #queue-req: 0, 
[2025-12-14 10:42:38 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.04, #queue-req: 0, 
[2025-12-14 10:42:39 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.02, #queue-req: 0, 
[2025-12-14 10:42:40 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.97, #queue-req: 0, 
[2025-12-14 10:42:41 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-14 10:42:42 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:42:44 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:42:45 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:42:46 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:42:47 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:42:48 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.40, #queue-req: 0, 
[2025-12-14 10:42:49 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:42:50 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.25, #queue-req: 0, 
[2025-12-14 10:42:51 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.23, #queue-req: 0, 
[2025-12-14 10:42:52 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:42:53 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.23, #queue-req: 0, 
[2025-12-14 10:42:54 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-14 10:42:55 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.80, #queue-req: 0, 
[2025-12-14 10:42:56 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.81, #queue-req: 0, 
[2025-12-14 10:42:57 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.82, #queue-req: 0, 
[2025-12-14 10:42:58] INFO:     127.0.0.1:52504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:42:58] INFO:     127.0.0.1:52520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:42:58] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:42:58] INFO:     127.0.0.1:52538 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:42:58 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:42:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:42:59 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 107.70, #queue-req: 0, 
[2025-12-14 10:43:00 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.03, #queue-req: 0, 
[2025-12-14 10:43:01 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.04, #queue-req: 0, 
[2025-12-14 10:43:02 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.06, #queue-req: 0, 
[2025-12-14 10:43:03 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.81, #queue-req: 0, 
[2025-12-14 10:43:04 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:43:05 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:43:06 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:43:07 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:43:08 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:43:09 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.46, #queue-req: 0, 
[2025-12-14 10:43:10 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-14 10:43:11 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.23, #queue-req: 0, 
[2025-12-14 10:43:12 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:43:14 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:43:15 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.23, #queue-req: 0, 
[2025-12-14 10:43:16 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-14 10:43:17 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.79, #queue-req: 0, 
[2025-12-14 10:43:18 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.82, #queue-req: 0, 
[2025-12-14 10:43:19 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.82, #queue-req: 0, 
[2025-12-14 10:43:20] INFO:     127.0.0.1:34266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:43:20] INFO:     127.0.0.1:34278 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:43:20] INFO:     127.0.0.1:34288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:43:20] INFO:     127.0.0.1:34304 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:43:20 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:43:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:43:20 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 107.58, #queue-req: 0, 
[2025-12-14 10:43:21 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.05, #queue-req: 0, 
[2025-12-14 10:43:22 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.04, #queue-req: 0, 
[2025-12-14 10:43:23 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.96, #queue-req: 0, 
[2025-12-14 10:43:25 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-14 10:43:26 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:43:27 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:43:28 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:43:29 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:43:30 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:43:31 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.42, #queue-req: 0, 
[2025-12-14 10:43:32 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.25, #queue-req: 0, 
[2025-12-14 10:43:33 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.27, #queue-req: 0, 
[2025-12-14 10:43:34 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.27, #queue-req: 0, 
[2025-12-14 10:43:35 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.29, #queue-req: 0, 
[2025-12-14 10:43:36 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.27, #queue-req: 0, 
[2025-12-14 10:43:37 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-14 10:43:38 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-14 10:43:39 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.84, #queue-req: 0, 
[2025-12-14 10:43:40 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-14 10:43:41] INFO:     127.0.0.1:47208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:43:41] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:43:41] INFO:     127.0.0.1:47234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:43:41] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:43:41 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:43:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:43:42 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 107.24, #queue-req: 0, 
[2025-12-14 10:43:43 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.01, #queue-req: 0, 
[2025-12-14 10:43:44 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.00, #queue-req: 0, 
[2025-12-14 10:43:45 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.98, #queue-req: 0, 
[2025-12-14 10:43:46 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-14 10:43:47 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:43:48 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:43:49 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:43:50 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:43:51 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:43:52 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.45, #queue-req: 0, 
[2025-12-14 10:43:53 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.25, #queue-req: 0, 
[2025-12-14 10:43:55 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.25, #queue-req: 0, 
[2025-12-14 10:43:56 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.23, #queue-req: 0, 
[2025-12-14 10:43:57 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:43:58 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.25, #queue-req: 0, 
[2025-12-14 10:43:59 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-14 10:44:00 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.82, #queue-req: 0, 
[2025-12-14 10:44:01 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.80, #queue-req: 0, 
[2025-12-14 10:44:02 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.81, #queue-req: 0, 
[2025-12-14 10:44:03] INFO:     127.0.0.1:53722 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:44:03] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:44:03] INFO:     127.0.0.1:53732 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:44:03] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:44:03 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:44:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:44:03 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 107.26, #queue-req: 0, 
[2025-12-14 10:44:05 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.03, #queue-req: 0, 
[2025-12-14 10:44:06 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.03, #queue-req: 0, 
[2025-12-14 10:44:07 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.98, #queue-req: 0, 
[2025-12-14 10:44:08 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-14 10:44:09 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-14 10:44:10 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:44:11 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:44:12 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:44:13 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:44:14 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.39, #queue-req: 0, 
[2025-12-14 10:44:15 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:44:16 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.21, #queue-req: 0, 
[2025-12-14 10:44:17 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.21, #queue-req: 0, 
[2025-12-14 10:44:18 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-14 10:44:19 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-14 10:44:20 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.06, #queue-req: 0, 
[2025-12-14 10:44:21 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.82, #queue-req: 0, 
[2025-12-14 10:44:22 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.80, #queue-req: 0, 
[2025-12-14 10:44:24 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.81, #queue-req: 0, 
[2025-12-14 10:44:24] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:44:24] INFO:     127.0.0.1:51620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:44:24] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:44:24] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:44:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:44:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-14 10:44:25 TP0] Decode batch, #running-req: 4, #token: 12845, token usage: 0.01, cuda graph: True, gen throughput (token/s): 106.95, #queue-req: 0, 
[2025-12-14 10:44:26 TP0] Decode batch, #running-req: 4, #token: 13005, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.98, #queue-req: 0, 
[2025-12-14 10:44:27 TP0] Decode batch, #running-req: 4, #token: 13165, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.98, #queue-req: 0, 
[2025-12-14 10:44:28 TP0] Decode batch, #running-req: 4, #token: 13325, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.96, #queue-req: 0, 
[2025-12-14 10:44:29 TP0] Decode batch, #running-req: 4, #token: 13485, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.72, #queue-req: 0, 
[2025-12-14 10:44:30 TP0] Decode batch, #running-req: 4, #token: 13645, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:44:31 TP0] Decode batch, #running-req: 4, #token: 13805, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:44:32 TP0] Decode batch, #running-req: 4, #token: 13965, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:44:33 TP0] Decode batch, #running-req: 4, #token: 14125, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:44:35 TP0] Decode batch, #running-req: 4, #token: 14285, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:44:36 TP0] Decode batch, #running-req: 4, #token: 14445, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.39, #queue-req: 0, 
[2025-12-14 10:44:37 TP0] Decode batch, #running-req: 4, #token: 14605, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:44:38 TP0] Decode batch, #running-req: 4, #token: 14765, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-14 10:44:39 TP0] Decode batch, #running-req: 4, #token: 14925, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:44:40 TP0] Decode batch, #running-req: 4, #token: 15085, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.21, #queue-req: 0, 
[2025-12-14 10:44:41 TP0] Decode batch, #running-req: 4, #token: 15245, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.23, #queue-req: 0, 
[2025-12-14 10:44:42 TP0] Decode batch, #running-req: 4, #token: 15405, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.05, #queue-req: 0, 
[2025-12-14 10:44:43 TP0] Decode batch, #running-req: 4, #token: 15565, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.78, #queue-req: 0, 
[2025-12-14 10:44:44 TP0] Decode batch, #running-req: 4, #token: 15725, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.78, #queue-req: 0, 
[2025-12-14 10:44:45 TP0] Decode batch, #running-req: 4, #token: 15885, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.79, #queue-req: 0, 
[2025-12-14 10:44:46] INFO:     127.0.0.1:36244 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:44:46] INFO:     127.0.0.1:36258 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:44:46] INFO:     127.0.0.1:36262 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:44:46] INFO:     127.0.0.1:36276 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:44:46 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:44:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:44:47 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 106.22, #queue-req: 0, 
[2025-12-14 10:44:48 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.02, #queue-req: 0, 
[2025-12-14 10:44:49 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.03, #queue-req: 0, 
[2025-12-14 10:44:50 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.95, #queue-req: 0, 
[2025-12-14 10:44:51 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-14 10:44:52 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:44:53 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:44:54 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:44:55 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:44:56 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:44:57 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.42, #queue-req: 0, 
[2025-12-14 10:44:58 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.21, #queue-req: 0, 
[2025-12-14 10:44:59 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:45:00 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.23, #queue-req: 0, 
[2025-12-14 10:45:01 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.26, #queue-req: 0, 
[2025-12-14 10:45:02 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-14 10:45:04 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-14 10:45:05 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.85, #queue-req: 0, 
[2025-12-14 10:45:06 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.85, #queue-req: 0, 
[2025-12-14 10:45:07 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.85, #queue-req: 0, 
[2025-12-14 10:45:07] INFO:     127.0.0.1:48836 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:45:07] INFO:     127.0.0.1:48848 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:45:07] INFO:     127.0.0.1:48862 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:45:07] INFO:     127.0.0.1:48868 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:45:07 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:45:08 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-14 10:45:08 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.56, #queue-req: 0, 
[2025-12-14 10:45:09 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.98, #queue-req: 0, 
[2025-12-14 10:45:10 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.95, #queue-req: 0, 
[2025-12-14 10:45:11 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-14 10:45:12 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-14 10:45:13 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:45:14 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:45:15 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:45:17 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:45:18 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:45:19 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.39, #queue-req: 0, 
[2025-12-14 10:45:20 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-14 10:45:21 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.26, #queue-req: 0, 
[2025-12-14 10:45:22 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-14 10:45:23 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-14 10:45:24 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-14 10:45:25 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.10, #queue-req: 0, 
[2025-12-14 10:45:26 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.83, #queue-req: 0, 
[2025-12-14 10:45:27 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.82, #queue-req: 0, 
[2025-12-14 10:45:28 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.83, #queue-req: 0, 
[2025-12-14 10:45:29] INFO:     127.0.0.1:49354 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:45:29] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:45:29] INFO:     127.0.0.1:49370 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:45:29] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:45:29 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:45:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:45:30 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.82, #queue-req: 0, 
[2025-12-14 10:45:31 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-14 10:45:32 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.88, #queue-req: 0, 
[2025-12-14 10:45:33 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.86, #queue-req: 0, 
[2025-12-14 10:45:34 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:45:35 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-14 10:45:36 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.52, #queue-req: 0, 
[2025-12-14 10:45:37 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:45:38 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-14 10:45:39 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-14 10:45:40 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.30, #queue-req: 0, 
[2025-12-14 10:45:41 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.05, #queue-req: 0, 
[2025-12-14 10:45:42 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.04, #queue-req: 0, 
[2025-12-14 10:45:43 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.03, #queue-req: 0, 
[2025-12-14 10:45:44 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.05, #queue-req: 0, 
[2025-12-14 10:45:45 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.05, #queue-req: 0, 
[2025-12-14 10:45:47 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.93, #queue-req: 0, 
[2025-12-14 10:45:48 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.61, #queue-req: 0, 
[2025-12-14 10:45:49 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.61, #queue-req: 0, 
[2025-12-14 10:45:50 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.61, #queue-req: 0, 
[2025-12-14 10:45:50] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:45:50] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:45:50] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:45:50] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:45:50 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:45:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:45:51 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.01, #queue-req: 0, 
[2025-12-14 10:45:52 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-14 10:45:53 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-14 10:45:54 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.89, #queue-req: 0, 
[2025-12-14 10:45:55 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-14 10:45:56 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-14 10:45:57 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-14 10:45:59 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-14 10:46:00 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-14 10:46:01 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-14 10:46:02 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.33, #queue-req: 0, 
[2025-12-14 10:46:03 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-14 10:46:04 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-14 10:46:05 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-14 10:46:06 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-14 10:46:07 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-14 10:46:08 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.02, #queue-req: 0, 
[2025-12-14 10:46:09 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.77, #queue-req: 0, 
[2025-12-14 10:46:10 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.76, #queue-req: 0, 
[2025-12-14 10:46:11 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.75, #queue-req: 0, 
[2025-12-14 10:46:12] INFO:     127.0.0.1:53928 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:46:12] INFO:     127.0.0.1:53944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:46:12] INFO:     127.0.0.1:53954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:46:12] INFO:     127.0.0.1:53962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:46:12 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:46:12 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-14 10:46:13 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.31, #queue-req: 0, 
[2025-12-14 10:46:14 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.06, #queue-req: 0, 
[2025-12-14 10:46:15 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.06, #queue-req: 0, 
[2025-12-14 10:46:16 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.03, #queue-req: 0, 
[2025-12-14 10:46:17 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.76, #queue-req: 0, 
[2025-12-14 10:46:18 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-14 10:46:19 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-14 10:46:20 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.67, #queue-req: 0, 
[2025-12-14 10:46:21 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:46:22 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-14 10:46:23 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.51, #queue-req: 0, 
[2025-12-14 10:46:24 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.38, #queue-req: 0, 
[2025-12-14 10:46:25 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.42, #queue-req: 0, 
[2025-12-14 10:46:26 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.39, #queue-req: 0, 
[2025-12-14 10:46:27 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.42, #queue-req: 0, 
[2025-12-14 10:46:28 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.43, #queue-req: 0, 
[2025-12-14 10:46:30 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.32, #queue-req: 0, 
[2025-12-14 10:46:31 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.96, #queue-req: 0, 
[2025-12-14 10:46:32 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.95, #queue-req: 0, 
[2025-12-14 10:46:33 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.98, #queue-req: 0, 
[2025-12-14 10:46:33] INFO:     127.0.0.1:37474 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:46:33] INFO:     127.0.0.1:37488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:46:33] INFO:     127.0.0.1:37492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:46:33] INFO:     127.0.0.1:37504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:46:33 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:46:34 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-14 10:46:34 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.22, #queue-req: 0, 
[2025-12-14 10:46:35 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.96, #queue-req: 0, 
[2025-12-14 10:46:36 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.98, #queue-req: 0, 
[2025-12-14 10:46:37 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.95, #queue-req: 0, 
[2025-12-14 10:46:38 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-14 10:46:39 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:46:40 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:46:41 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:46:43 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:46:44 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:46:45 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.40, #queue-req: 0, 
[2025-12-14 10:46:46 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-14 10:46:47 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.25, #queue-req: 0, 
[2025-12-14 10:46:48 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:46:49 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-14 10:46:50 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.34, #queue-req: 0, 
[2025-12-14 10:46:51 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-14 10:46:52 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.84, #queue-req: 0, 
[2025-12-14 10:46:53 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.84, #queue-req: 0, 
[2025-12-14 10:46:54 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-14 10:46:55] INFO:     127.0.0.1:33944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:46:55] INFO:     127.0.0.1:33948 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:46:55] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:46:55] INFO:     127.0.0.1:33974 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:46:55 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:46:55 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-14 10:46:56 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.15, #queue-req: 0, 
[2025-12-14 10:46:57 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.02, #queue-req: 0, 
[2025-12-14 10:46:58 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.03, #queue-req: 0, 
[2025-12-14 10:46:59 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.98, #queue-req: 0, 
[2025-12-14 10:47:00 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-14 10:47:01 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:47:02 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:47:03 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:47:04 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:47:05 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:47:06 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.39, #queue-req: 0, 
[2025-12-14 10:47:07 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-14 10:47:08 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.25, #queue-req: 0, 
[2025-12-14 10:47:09 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:47:10 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:47:11 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:47:12 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.09, #queue-req: 0, 
[2025-12-14 10:47:14 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.83, #queue-req: 0, 
[2025-12-14 10:47:15 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.85, #queue-req: 0, 
[2025-12-14 10:47:16 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.85, #queue-req: 0, 
[2025-12-14 10:47:16] INFO:     127.0.0.1:37770 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:47:16] INFO:     127.0.0.1:37774 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:47:16] INFO:     127.0.0.1:37790 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:47:16] INFO:     127.0.0.1:37792 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:47:16 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:47:17 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-14 10:47:17 TP0] Decode batch, #running-req: 4, #token: 12845, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.28, #queue-req: 0, 
[2025-12-14 10:47:18 TP0] Decode batch, #running-req: 4, #token: 13005, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.03, #queue-req: 0, 
[2025-12-14 10:47:19 TP0] Decode batch, #running-req: 4, #token: 13165, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.05, #queue-req: 0, 
[2025-12-14 10:47:20 TP0] Decode batch, #running-req: 4, #token: 13325, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.08, #queue-req: 0, 
[2025-12-14 10:47:21 TP0] Decode batch, #running-req: 4, #token: 13485, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.76, #queue-req: 0, 
[2025-12-14 10:47:22 TP0] Decode batch, #running-req: 4, #token: 13645, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-14 10:47:23 TP0] Decode batch, #running-req: 4, #token: 13805, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.68, #queue-req: 0, 
[2025-12-14 10:47:24 TP0] Decode batch, #running-req: 4, #token: 13965, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-14 10:47:26 TP0] Decode batch, #running-req: 4, #token: 14125, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:47:27 TP0] Decode batch, #running-req: 4, #token: 14285, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:47:28 TP0] Decode batch, #running-req: 4, #token: 14445, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.47, #queue-req: 0, 
[2025-12-14 10:47:29 TP0] Decode batch, #running-req: 4, #token: 14605, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.26, #queue-req: 0, 
[2025-12-14 10:47:30 TP0] Decode batch, #running-req: 4, #token: 14765, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.28, #queue-req: 0, 
[2025-12-14 10:47:31 TP0] Decode batch, #running-req: 4, #token: 14925, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-14 10:47:32 TP0] Decode batch, #running-req: 4, #token: 15085, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.27, #queue-req: 0, 
[2025-12-14 10:47:33 TP0] Decode batch, #running-req: 4, #token: 15245, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.27, #queue-req: 0, 
[2025-12-14 10:47:34 TP0] Decode batch, #running-req: 4, #token: 15405, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-14 10:47:35 TP0] Decode batch, #running-req: 4, #token: 15565, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-14 10:47:36 TP0] Decode batch, #running-req: 4, #token: 15725, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-14 10:47:37 TP0] Decode batch, #running-req: 4, #token: 15885, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.89, #queue-req: 0, 
[2025-12-14 10:47:38] INFO:     127.0.0.1:43590 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:47:38] INFO:     127.0.0.1:43606 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:47:38] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:47:38] INFO:     127.0.0.1:43618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:47:38 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:47:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:47:39 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.16, #queue-req: 0, 
[2025-12-14 10:47:40 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.99, #queue-req: 0, 
[2025-12-14 10:47:41 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.01, #queue-req: 0, 
[2025-12-14 10:47:42 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.96, #queue-req: 0, 
[2025-12-14 10:47:43 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.71, #queue-req: 0, 
[2025-12-14 10:47:44 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:47:45 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:47:46 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:47:47 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:47:48 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:47:49 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.41, #queue-req: 0, 
[2025-12-14 10:47:50 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:47:51 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.23, #queue-req: 0, 
[2025-12-14 10:47:52 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.28, #queue-req: 0, 
[2025-12-14 10:47:53 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.31, #queue-req: 0, 
[2025-12-14 10:47:54 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.29, #queue-req: 0, 
[2025-12-14 10:47:55 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-14 10:47:57 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.86, #queue-req: 0, 
[2025-12-14 10:47:58 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.88, #queue-req: 0, 
[2025-12-14 10:47:59 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.87, #queue-req: 0, 
[2025-12-14 10:47:59] INFO:     127.0.0.1:35618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:47:59] INFO:     127.0.0.1:35630 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:47:59] INFO:     127.0.0.1:35644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:47:59] INFO:     127.0.0.1:35646 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:47:59 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:48:00 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:48:00 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.12, #queue-req: 0, 
[2025-12-14 10:48:01 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.04, #queue-req: 0, 
[2025-12-14 10:48:02 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.97, #queue-req: 0, 
[2025-12-14 10:48:03 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-14 10:48:04 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.70, #queue-req: 0, 
[2025-12-14 10:48:05 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:48:06 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:48:07 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:48:08 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:48:10 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:48:11 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.44, #queue-req: 0, 
[2025-12-14 10:48:12 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:48:13 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.24, #queue-req: 0, 
[2025-12-14 10:48:14 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.25, #queue-req: 0, 
[2025-12-14 10:48:15 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.26, #queue-req: 0, 
[2025-12-14 10:48:16 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.25, #queue-req: 0, 
[2025-12-14 10:48:17 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-14 10:48:18 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.78, #queue-req: 0, 
[2025-12-14 10:48:19 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.79, #queue-req: 0, 
[2025-12-14 10:48:20 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.80, #queue-req: 0, 
[2025-12-14 10:48:21] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:48:21] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:48:21] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:48:21] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:48:21 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:48:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:48:22 TP0] Decode batch, #running-req: 4, #token: 12845, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.83, #queue-req: 0, 
[2025-12-14 10:48:23 TP0] Decode batch, #running-req: 4, #token: 13005, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.02, #queue-req: 0, 
[2025-12-14 10:48:24 TP0] Decode batch, #running-req: 4, #token: 13165, token usage: 0.01, cuda graph: True, gen throughput (token/s): 152.01, #queue-req: 0, 
[2025-12-14 10:48:25 TP0] Decode batch, #running-req: 4, #token: 13325, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.97, #queue-req: 0, 
[2025-12-14 10:48:26 TP0] Decode batch, #running-req: 4, #token: 13485, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-14 10:48:27 TP0] Decode batch, #running-req: 4, #token: 13645, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.63, #queue-req: 0, 
[2025-12-14 10:48:28 TP0] Decode batch, #running-req: 4, #token: 13805, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:48:29 TP0] Decode batch, #running-req: 4, #token: 13965, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:48:30 TP0] Decode batch, #running-req: 4, #token: 14125, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:48:31 TP0] Decode batch, #running-req: 4, #token: 14285, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:48:32 TP0] Decode batch, #running-req: 4, #token: 14445, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.38, #queue-req: 0, 
[2025-12-14 10:48:33 TP0] Decode batch, #running-req: 4, #token: 14605, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.21, #queue-req: 0, 
[2025-12-14 10:48:34 TP0] Decode batch, #running-req: 4, #token: 14765, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.21, #queue-req: 0, 
[2025-12-14 10:48:35 TP0] Decode batch, #running-req: 4, #token: 14925, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:48:36 TP0] Decode batch, #running-req: 4, #token: 15085, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:48:37 TP0] Decode batch, #running-req: 4, #token: 15245, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:48:38 TP0] Decode batch, #running-req: 4, #token: 15405, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.06, #queue-req: 0, 
[2025-12-14 10:48:40 TP0] Decode batch, #running-req: 4, #token: 15565, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.79, #queue-req: 0, 
[2025-12-14 10:48:41 TP0] Decode batch, #running-req: 4, #token: 15725, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.80, #queue-req: 0, 
[2025-12-14 10:48:42 TP0] Decode batch, #running-req: 4, #token: 15885, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.80, #queue-req: 0, 
[2025-12-14 10:48:42] INFO:     127.0.0.1:40972 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:48:42] INFO:     127.0.0.1:40982 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:48:42] INFO:     127.0.0.1:40996 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:48:42] INFO:     127.0.0.1:41008 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:48:42 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:48:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:48:43 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.63, #queue-req: 0, 
[2025-12-14 10:48:44 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.94, #queue-req: 0, 
[2025-12-14 10:48:45 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-14 10:48:46 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.89, #queue-req: 0, 
[2025-12-14 10:48:47 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.67, #queue-req: 0, 
[2025-12-14 10:48:48 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:48:49 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:48:50 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.58, #queue-req: 0, 
[2025-12-14 10:48:51 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:48:53 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:48:54 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.35, #queue-req: 0, 
[2025-12-14 10:48:55 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-14 10:48:56 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-14 10:48:57 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-14 10:48:58 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-14 10:48:59 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.13, #queue-req: 0, 
[2025-12-14 10:49:00 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.99, #queue-req: 0, 
[2025-12-14 10:49:01 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.73, #queue-req: 0, 
[2025-12-14 10:49:02 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.74, #queue-req: 0, 
[2025-12-14 10:49:03 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.75, #queue-req: 0, 
[2025-12-14 10:49:04] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:49:04] INFO:     127.0.0.1:52610 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:49:04] INFO:     127.0.0.1:52618 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:49:04] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:49:04 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:49:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:49:05 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.49, #queue-req: 0, 
[2025-12-14 10:49:06 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.98, #queue-req: 0, 
[2025-12-14 10:49:07 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.95, #queue-req: 0, 
[2025-12-14 10:49:08 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-14 10:49:09 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.65, #queue-req: 0, 
[2025-12-14 10:49:10 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:49:11 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-14 10:49:12 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:49:13 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-14 10:49:14 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.59, #queue-req: 0, 
[2025-12-14 10:49:15 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.37, #queue-req: 0, 
[2025-12-14 10:49:16 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:49:17 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.28, #queue-req: 0, 
[2025-12-14 10:49:18 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:49:19 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:49:20 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.22, #queue-req: 0, 
[2025-12-14 10:49:21 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-14 10:49:23 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.79, #queue-req: 0, 
[2025-12-14 10:49:24 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.78, #queue-req: 0, 
[2025-12-14 10:49:25 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.79, #queue-req: 0, 
[2025-12-14 10:49:25] INFO:     127.0.0.1:56678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:49:25] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:49:25] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:49:25] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:49:25 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:49:26 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-14 10:49:26 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.67, #queue-req: 0, 
[2025-12-14 10:49:27 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.86, #queue-req: 0, 
[2025-12-14 10:49:28 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.85, #queue-req: 0, 
[2025-12-14 10:49:29 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.86, #queue-req: 0, 
[2025-12-14 10:49:30 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:49:31 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.53, #queue-req: 0, 
[2025-12-14 10:49:32 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-14 10:49:33 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.53, #queue-req: 0, 
[2025-12-14 10:49:34 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.53, #queue-req: 0, 
[2025-12-14 10:49:36 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.55, #queue-req: 0, 
[2025-12-14 10:49:37 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.31, #queue-req: 0, 
[2025-12-14 10:49:38 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-14 10:49:39 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-14 10:49:40 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.07, #queue-req: 0, 
[2025-12-14 10:49:41 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.06, #queue-req: 0, 
[2025-12-14 10:49:42 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.05, #queue-req: 0, 
[2025-12-14 10:49:43 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.95, #queue-req: 0, 
[2025-12-14 10:49:44 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.65, #queue-req: 0, 
[2025-12-14 10:49:45 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.64, #queue-req: 0, 
[2025-12-14 10:49:46 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.65, #queue-req: 0, 
[2025-12-14 10:49:47] INFO:     127.0.0.1:48504 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:49:47] INFO:     127.0.0.1:48516 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:49:47] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:49:47] INFO:     127.0.0.1:48534 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:49:47 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:49:47 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-14 10:49:48 TP0] Decode batch, #running-req: 4, #token: 12845, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.81, #queue-req: 0, 
[2025-12-14 10:49:49 TP0] Decode batch, #running-req: 4, #token: 13005, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.95, #queue-req: 0, 
[2025-12-14 10:49:50 TP0] Decode batch, #running-req: 4, #token: 13165, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.96, #queue-req: 0, 
[2025-12-14 10:49:51 TP0] Decode batch, #running-req: 4, #token: 13325, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.95, #queue-req: 0, 
[2025-12-14 10:49:52 TP0] Decode batch, #running-req: 4, #token: 13485, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.69, #queue-req: 0, 
[2025-12-14 10:49:53 TP0] Decode batch, #running-req: 4, #token: 13645, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:49:54 TP0] Decode batch, #running-req: 4, #token: 13805, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:49:55 TP0] Decode batch, #running-req: 4, #token: 13965, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:49:56 TP0] Decode batch, #running-req: 4, #token: 14125, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:49:57 TP0] Decode batch, #running-req: 4, #token: 14285, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:49:58 TP0] Decode batch, #running-req: 4, #token: 14445, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.40, #queue-req: 0, 
[2025-12-14 10:49:59 TP0] Decode batch, #running-req: 4, #token: 14605, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-14 10:50:00 TP0] Decode batch, #running-req: 4, #token: 14765, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-14 10:50:01 TP0] Decode batch, #running-req: 4, #token: 14925, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.14, #queue-req: 0, 
[2025-12-14 10:50:02 TP0] Decode batch, #running-req: 4, #token: 15085, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-14 10:50:03 TP0] Decode batch, #running-req: 4, #token: 15245, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:50:04 TP0] Decode batch, #running-req: 4, #token: 15405, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.04, #queue-req: 0, 
[2025-12-14 10:50:06 TP0] Decode batch, #running-req: 4, #token: 15565, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.77, #queue-req: 0, 
[2025-12-14 10:50:07 TP0] Decode batch, #running-req: 4, #token: 15725, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.77, #queue-req: 0, 
[2025-12-14 10:50:08 TP0] Decode batch, #running-req: 4, #token: 15885, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.77, #queue-req: 0, 
[2025-12-14 10:50:08] INFO:     127.0.0.1:37118 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:50:08] INFO:     127.0.0.1:37120 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:50:08] INFO:     127.0.0.1:37124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:50:08 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:50:08] INFO:     127.0.0.1:37132 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:50:09 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-14 10:50:09 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 112.81, #queue-req: 0, 
[2025-12-14 10:50:10 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-14 10:50:11 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.93, #queue-req: 0, 
[2025-12-14 10:50:12 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.90, #queue-req: 0, 
[2025-12-14 10:50:13 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-14 10:50:14 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:50:15 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:50:16 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:50:18 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:50:19 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.61, #queue-req: 0, 
[2025-12-14 10:50:20 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.36, #queue-req: 0, 
[2025-12-14 10:50:21 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.15, #queue-req: 0, 
[2025-12-14 10:50:22 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:50:23 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-14 10:50:24 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:50:25 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.19, #queue-req: 0, 
[2025-12-14 10:50:26 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.04, #queue-req: 0, 
[2025-12-14 10:50:27 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.75, #queue-req: 0, 
[2025-12-14 10:50:28 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.74, #queue-req: 0, 
[2025-12-14 10:50:29 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.75, #queue-req: 0, 
[2025-12-14 10:50:30] INFO:     127.0.0.1:60554 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:50:30] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:50:30] INFO:     127.0.0.1:60578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:50:30] INFO:     127.0.0.1:60586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:50:30 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:50:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.01, #running-req: 3, #queue-req: 0, 
[2025-12-14 10:50:31 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.34, #queue-req: 0, 
[2025-12-14 10:50:32 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.95, #queue-req: 0, 
[2025-12-14 10:50:33 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-14 10:50:34 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.91, #queue-req: 0, 
[2025-12-14 10:50:35 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.66, #queue-req: 0, 
[2025-12-14 10:50:36 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:50:37 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:50:38 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.60, #queue-req: 0, 
[2025-12-14 10:50:39 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:50:40 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.62, #queue-req: 0, 
[2025-12-14 10:50:41 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.34, #queue-req: 0, 
[2025-12-14 10:50:42 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:50:43 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.18, #queue-req: 0, 
[2025-12-14 10:50:44 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-14 10:50:45 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.17, #queue-req: 0, 
[2025-12-14 10:50:46 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.20, #queue-req: 0, 
[2025-12-14 10:50:47 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.04, #queue-req: 0, 
[2025-12-14 10:50:49 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.76, #queue-req: 0, 
[2025-12-14 10:50:50 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.77, #queue-req: 0, 
[2025-12-14 10:50:51 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.75, #queue-req: 0, 
[2025-12-14 10:50:51] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:50:51] INFO:     127.0.0.1:53020 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:50:51] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:50:51] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:50:51 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:50:52 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-14 10:50:52 TP0] Decode batch, #running-req: 4, #token: 12846, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.45, #queue-req: 0, 
[2025-12-14 10:50:53 TP0] Decode batch, #running-req: 4, #token: 13006, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.93, #queue-req: 0, 
[2025-12-14 10:50:54 TP0] Decode batch, #running-req: 4, #token: 13166, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.92, #queue-req: 0, 
[2025-12-14 10:50:55 TP0] Decode batch, #running-req: 4, #token: 13326, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.89, #queue-req: 0, 
[2025-12-14 10:50:56 TP0] Decode batch, #running-req: 4, #token: 13486, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.64, #queue-req: 0, 
[2025-12-14 10:50:57 TP0] Decode batch, #running-req: 4, #token: 13646, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-14 10:50:58 TP0] Decode batch, #running-req: 4, #token: 13806, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-14 10:50:59 TP0] Decode batch, #running-req: 4, #token: 13966, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-14 10:51:00 TP0] Decode batch, #running-req: 4, #token: 14126, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.56, #queue-req: 0, 
[2025-12-14 10:51:02 TP0] Decode batch, #running-req: 4, #token: 14286, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.57, #queue-req: 0, 
[2025-12-14 10:51:03 TP0] Decode batch, #running-req: 4, #token: 14446, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.30, #queue-req: 0, 
[2025-12-14 10:51:04 TP0] Decode batch, #running-req: 4, #token: 14606, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.11, #queue-req: 0, 
[2025-12-14 10:51:05 TP0] Decode batch, #running-req: 4, #token: 14766, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.12, #queue-req: 0, 
[2025-12-14 10:51:06 TP0] Decode batch, #running-req: 4, #token: 14926, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.08, #queue-req: 0, 
[2025-12-14 10:51:07 TP0] Decode batch, #running-req: 4, #token: 15086, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.15, #queue-req: 0, 
[2025-12-14 10:51:08 TP0] Decode batch, #running-req: 4, #token: 15246, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.15, #queue-req: 0, 
[2025-12-14 10:51:09 TP0] Decode batch, #running-req: 4, #token: 15406, token usage: 0.02, cuda graph: True, gen throughput (token/s): 151.00, #queue-req: 0, 
[2025-12-14 10:51:10 TP0] Decode batch, #running-req: 4, #token: 15566, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.69, #queue-req: 0, 
[2025-12-14 10:51:11 TP0] Decode batch, #running-req: 4, #token: 15726, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.69, #queue-req: 0, 
[2025-12-14 10:51:12 TP0] Decode batch, #running-req: 4, #token: 15886, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.69, #queue-req: 0, 
[2025-12-14 10:51:13] INFO:     127.0.0.1:46560 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:51:13] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:51:13] INFO:     127.0.0.1:46578 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:51:13] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:51:13 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:51:13 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-12-14 10:51:14 TP0] Decode batch, #running-req: 4, #token: 12844, token usage: 0.01, cuda graph: True, gen throughput (token/s): 113.58, #queue-req: 0, 
[2025-12-14 10:51:15 TP0] Decode batch, #running-req: 4, #token: 13004, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.75, #queue-req: 0, 
[2025-12-14 10:51:16 TP0] Decode batch, #running-req: 4, #token: 13164, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.74, #queue-req: 0, 
[2025-12-14 10:51:17 TP0] Decode batch, #running-req: 4, #token: 13324, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.73, #queue-req: 0, 
[2025-12-14 10:51:18 TP0] Decode batch, #running-req: 4, #token: 13484, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.54, #queue-req: 0, 
[2025-12-14 10:51:19 TP0] Decode batch, #running-req: 4, #token: 13644, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.42, #queue-req: 0, 
[2025-12-14 10:51:20 TP0] Decode batch, #running-req: 4, #token: 13804, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.42, #queue-req: 0, 
[2025-12-14 10:51:21 TP0] Decode batch, #running-req: 4, #token: 13964, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.41, #queue-req: 0, 
[2025-12-14 10:51:22 TP0] Decode batch, #running-req: 4, #token: 14124, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.37, #queue-req: 0, 
[2025-12-14 10:51:23 TP0] Decode batch, #running-req: 4, #token: 14284, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.40, #queue-req: 0, 
[2025-12-14 10:51:24 TP0] Decode batch, #running-req: 4, #token: 14444, token usage: 0.01, cuda graph: True, gen throughput (token/s): 151.16, #queue-req: 0, 
[2025-12-14 10:51:25 TP0] Decode batch, #running-req: 4, #token: 14604, token usage: 0.01, cuda graph: True, gen throughput (token/s): 150.90, #queue-req: 0, 
[2025-12-14 10:51:26 TP0] Decode batch, #running-req: 4, #token: 14764, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.92, #queue-req: 0, 
[2025-12-14 10:51:27 TP0] Decode batch, #running-req: 4, #token: 14924, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.91, #queue-req: 0, 
[2025-12-14 10:51:28 TP0] Decode batch, #running-req: 4, #token: 15084, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.91, #queue-req: 0, 
[2025-12-14 10:51:29 TP0] Decode batch, #running-req: 4, #token: 15244, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.91, #queue-req: 0, 
[2025-12-14 10:51:30 TP0] Decode batch, #running-req: 4, #token: 15404, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.80, #queue-req: 0, 
[2025-12-14 10:51:32 TP0] Decode batch, #running-req: 4, #token: 15564, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.54, #queue-req: 0, 
[2025-12-14 10:51:33 TP0] Decode batch, #running-req: 4, #token: 15724, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.54, #queue-req: 0, 
[2025-12-14 10:51:34 TP0] Decode batch, #running-req: 4, #token: 15884, token usage: 0.02, cuda graph: True, gen throughput (token/s): 150.54, #queue-req: 0, 
[2025-12-14 10:51:34] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-14 10:51:34] INFO:     127.0.0.1:41894 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-14 10:51:35] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-14 10:51:35] INFO:     127.0.0.1:41906 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-14 10:51:45] INFO:     127.0.0.1:50238 - "GET /v1/models HTTP/1.1" 200 OK
[2025-12-14 10:51:51] INFO:     127.0.0.1:50252 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:51:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:51:52 TP0] Decode batch, #running-req: 1, #token: 3213, token usage: 0.00, cuda graph: True, gen throughput (token/s): 6.98, #queue-req: 0, 
[2025-12-14 10:51:53] INFO:     127.0.0.1:35654 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:51:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:51:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 18.56, #queue-req: 0, 
[2025-12-14 10:51:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:51:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:51:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-14 10:51:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 10:51:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:52:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 10:52:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 10:52:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 10:52:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 10:52:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:52:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:52:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:52:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 10:52:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:52:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:52:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 10:52:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:52:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:52:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:52:13] INFO:     127.0.0.1:53678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:52:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:52:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-14 10:52:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:52:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:52:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 10:52:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 10:52:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:52:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:52:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:52:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:52:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:52:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:52:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 10:52:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.13, #queue-req: 0, 
[2025-12-14 10:52:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:52:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 10:52:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:52:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 10:52:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:52:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:52:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:52:33] INFO:     127.0.0.1:48024 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:52:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:52:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.08, #queue-req: 0, 
[2025-12-14 10:52:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:52:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:52:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 10:52:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 10:52:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:52:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:52:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:52:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:52:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:52:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:52:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:52:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:52:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:52:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:52:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:52:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 10:52:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 10:52:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:52:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:52:53] INFO:     127.0.0.1:58492 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:52:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:52:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.09, #queue-req: 0, 
[2025-12-14 10:52:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:52:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:52:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:52:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 10:52:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:53:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:53:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:53:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:53:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:53:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:53:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:53:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:53:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:53:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:53:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:53:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 10:53:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:53:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:53:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:53:13] INFO:     127.0.0.1:38952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:53:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:53:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-14 10:53:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:53:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:53:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:53:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 10:53:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:53:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:53:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:53:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:53:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:53:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 10:53:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:53:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:53:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:53:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:53:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:53:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 10:53:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:53:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:53:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 10:53:33] INFO:     127.0.0.1:33112 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:53:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:53:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 10:53:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:53:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:53:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 10:53:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 10:53:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:53:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:53:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:53:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:53:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:53:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 10:53:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:53:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:53:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:53:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:53:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 10:53:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 10:53:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:53:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:53:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:53:53] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:53:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:53:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 10:53:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:53:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:53:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-14 10:53:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 10:53:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:54:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:54:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:54:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:54:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:54:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 10:54:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 10:54:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:54:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:54:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:54:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:54:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 10:54:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:54:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 10:54:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:54:13] INFO:     127.0.0.1:47150 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:54:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:54:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.08, #queue-req: 0, 
[2025-12-14 10:54:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:54:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:54:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:54:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 10:54:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:54:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:54:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 10:54:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:54:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:54:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:54:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:54:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:54:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:54:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:54:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:54:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 10:54:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:54:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:54:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:54:33] INFO:     127.0.0.1:48266 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:54:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:54:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.02, #queue-req: 0, 
[2025-12-14 10:54:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:54:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:54:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:54:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 10:54:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:54:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:54:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:54:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:54:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:54:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 10:54:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:54:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:54:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:54:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:54:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:54:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 10:54:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-14 10:54:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:54:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:54:53] INFO:     127.0.0.1:45804 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:54:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:54:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-14 10:54:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:54:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:54:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:54:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 10:54:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:55:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:55:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:55:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:55:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:55:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 10:55:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:55:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:55:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:55:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:55:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:55:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 10:55:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:55:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:55:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 10:55:13] INFO:     127.0.0.1:36124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:55:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:55:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 10:55:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:55:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:55:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:55:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 10:55:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:55:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:55:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:55:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:55:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:55:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:55:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:55:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:55:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:55:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:55:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:55:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 10:55:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:55:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 10:55:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:55:33] INFO:     127.0.0.1:33300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:55:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:55:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 10:55:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:55:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:55:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:55:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 10:55:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:55:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:55:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:55:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:55:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:55:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:55:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:55:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:55:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:55:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:55:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:55:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 10:55:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:55:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:55:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:55:53] INFO:     127.0.0.1:37048 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:55:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:55:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-14 10:55:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:55:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:55:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-14 10:55:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:55:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 10:56:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:56:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 10:56:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:56:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:56:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 10:56:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:56:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:56:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:56:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:56:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:56:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 10:56:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:56:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:56:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:56:13] INFO:     127.0.0.1:60966 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:56:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:56:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-14 10:56:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:56:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:56:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-14 10:56:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 10:56:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:56:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:56:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:56:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:56:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:56:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:56:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:56:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 10:56:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:56:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:56:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:56:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 10:56:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:56:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:56:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 10:56:33] INFO:     127.0.0.1:43874 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:56:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:56:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.25, #queue-req: 0, 
[2025-12-14 10:56:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:56:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 10:56:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:56:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 10:56:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:56:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:56:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:56:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:56:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:56:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 10:56:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:56:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:56:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:56:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:56:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:56:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 10:56:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:56:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:56:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 10:56:53] INFO:     127.0.0.1:49834 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:56:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:56:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-14 10:56:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:56:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:56:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:56:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 10:56:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:57:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 10:57:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:57:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:57:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:57:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 10:57:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:57:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:57:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:57:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:57:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:57:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 10:57:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 10:57:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-14 10:57:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 10:57:13] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:57:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:57:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-14 10:57:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:57:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:57:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 10:57:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:57:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:57:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:57:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 10:57:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:57:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:57:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:57:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:57:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:57:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:57:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:57:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:57:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 10:57:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:57:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:57:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:57:33] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:57:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:57:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-14 10:57:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:57:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:57:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 10:57:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 10:57:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:57:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:57:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:57:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:57:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:57:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:57:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:57:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 10:57:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:57:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:57:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:57:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 10:57:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:57:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:57:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 10:57:53] INFO:     127.0.0.1:49658 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:57:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:57:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-14 10:57:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:57:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:57:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:57:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 10:57:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:58:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:58:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:58:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:58:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:58:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 10:58:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:58:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:58:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:58:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:58:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:58:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 10:58:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:58:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:58:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:58:13] INFO:     127.0.0.1:54068 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:58:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:58:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-14 10:58:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:58:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:58:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:58:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 10:58:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:58:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:58:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:58:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:58:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:58:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 10:58:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:58:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:58:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:58:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:58:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:58:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 10:58:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:58:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:58:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 10:58:33] INFO:     127.0.0.1:33878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:58:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:58:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 10:58:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:58:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:58:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:58:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 10:58:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:58:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:58:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:58:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:58:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:58:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:58:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:58:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:58:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:58:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 10:58:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:58:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 10:58:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:58:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 10:58:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 10:58:53] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:58:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:58:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-14 10:58:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:58:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:58:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 10:58:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 10:58:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 10:59:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:59:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:59:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:59:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 10:59:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 10:59:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:59:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:59:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:59:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:59:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 10:59:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 10:59:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:59:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 10:59:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:59:13] INFO:     127.0.0.1:43586 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:59:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:59:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.17, #queue-req: 0, 
[2025-12-14 10:59:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 10:59:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:59:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 10:59:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 10:59:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 10:59:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 10:59:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 10:59:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 10:59:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 10:59:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 10:59:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:59:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:59:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:59:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 10:59:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 10:59:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 10:59:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 10:59:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 10:59:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 10:59:33] INFO:     127.0.0.1:37706 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:59:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:59:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 10:59:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 10:59:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 10:59:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 10:59:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 10:59:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 10:59:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 10:59:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 10:59:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 10:59:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 10:59:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 10:59:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:59:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:59:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:59:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 10:59:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 10:59:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 10:59:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 10:59:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 10:59:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 10:59:53] INFO:     127.0.0.1:43156 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 10:59:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 10:59:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 10:59:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:59:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 10:59:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 10:59:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-14 10:59:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:00:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:00:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:00:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:00:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:00:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:00:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:00:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:00:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:00:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:00:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:00:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:00:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:00:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:00:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:00:13] INFO:     127.0.0.1:44012 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:00:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:00:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 11:00:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:00:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:00:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:00:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-14 11:00:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:00:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:00:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:00:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:00:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:00:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-14 11:00:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:00:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:00:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:00:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:00:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:00:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:00:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:00:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:00:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:00:33] INFO:     127.0.0.1:59512 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:00:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:00:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:00:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:00:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:00:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:00:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:00:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:00:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:00:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:00:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:00:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:00:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:00:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:00:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:00:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:00:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:00:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:00:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:00:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:00:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:00:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:00:53] INFO:     127.0.0.1:46672 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:00:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:00:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-14 11:00:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:00:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:00:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:00:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:00:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:01:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:01:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:01:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:01:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:01:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:01:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:01:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:01:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:01:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:01:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:01:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:01:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:01:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:01:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:01:13] INFO:     127.0.0.1:57830 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:01:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:01:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:01:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:01:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:01:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:01:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:01:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:01:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:01:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:01:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:01:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:01:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:01:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:01:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:01:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:01:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:01:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:01:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:01:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:01:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:01:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:01:33] INFO:     127.0.0.1:46522 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:01:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:01:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:01:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:01:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:01:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:01:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:01:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:01:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:01:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:01:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:01:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:01:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-14 11:01:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:01:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:01:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:01:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:01:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:01:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:01:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:01:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:01:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:01:53] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:01:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:01:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:01:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:01:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:01:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:01:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:01:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:02:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:02:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:02:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:02:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:02:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:02:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:02:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:02:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:02:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:02:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:02:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:02:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:02:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:02:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:02:13] INFO:     127.0.0.1:38238 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:02:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:02:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:02:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:02:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:02:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:02:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:02:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:02:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:02:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:02:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:02:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:02:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:02:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:02:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:02:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:02:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:02:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:02:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:02:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:02:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:02:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:02:33] INFO:     127.0.0.1:39852 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:02:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:02:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-14 11:02:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:02:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:02:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:02:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:02:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:02:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:02:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:02:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:02:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:02:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:02:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:02:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:02:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:02:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:02:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:02:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:02:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:02:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:02:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:02:53] INFO:     127.0.0.1:49644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:02:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:02:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:02:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:02:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:02:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:02:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:02:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:03:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:03:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:03:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:03:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:03:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:03:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:03:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:03:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:03:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:03:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:03:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:03:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:03:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:03:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:03:13] INFO:     127.0.0.1:42478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:03:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:03:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-14 11:03:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:03:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:03:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:03:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:03:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:03:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:03:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:03:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:03:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:03:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:03:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:03:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:03:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:03:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:03:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:03:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:03:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:03:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:03:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:03:33] INFO:     127.0.0.1:49376 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:03:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:03:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-14 11:03:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:03:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:03:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:03:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:03:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:03:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:03:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:03:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:03:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:03:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:03:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:03:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:03:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:03:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:03:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:03:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:03:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:03:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:03:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:03:53] INFO:     127.0.0.1:37802 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:03:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:03:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 33.49, #queue-req: 0, 
[2025-12-14 11:03:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:03:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:03:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:03:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:03:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:04:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:04:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:04:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:04:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:04:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:04:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:04:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:04:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:04:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:04:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:04:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:04:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:04:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:04:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:04:13] INFO:     127.0.0.1:46952 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:04:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:04:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-14 11:04:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:04:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:04:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:04:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:04:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:04:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:04:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:04:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:04:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:04:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:04:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:04:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:04:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:04:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:04:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:04:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:04:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:04:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:04:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:04:33] INFO:     127.0.0.1:45270 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:04:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:04:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.31, #queue-req: 0, 
[2025-12-14 11:04:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:04:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:04:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:04:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:04:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:04:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:04:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:04:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:04:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:04:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:04:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:04:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:04:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:04:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:04:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:04:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:04:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:04:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:04:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:04:53] INFO:     127.0.0.1:39488 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:04:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:04:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-14 11:04:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:04:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:04:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:04:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-14 11:04:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:05:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:05:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:05:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:05:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:05:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-14 11:05:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:05:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:05:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:05:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:05:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:05:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:05:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:05:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:05:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:05:13] INFO:     127.0.0.1:55648 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:05:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:05:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-14 11:05:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:05:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:05:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:05:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:05:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:05:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:05:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:05:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:05:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:05:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:05:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:05:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:05:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:05:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:05:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:05:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:05:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:05:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:05:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:05:33] INFO:     127.0.0.1:41050 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:05:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:05:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 33.73, #queue-req: 0, 
[2025-12-14 11:05:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:05:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:05:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:05:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:05:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:05:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:05:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:05:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:05:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:05:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:05:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:05:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:05:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:05:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:05:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:05:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:05:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:05:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:05:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:05:53] INFO:     127.0.0.1:56476 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:05:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:05:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.09, #queue-req: 0, 
[2025-12-14 11:05:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:05:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:05:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:05:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:05:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:06:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:06:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:06:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:06:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:06:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:06:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:06:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:06:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:06:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:06:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:06:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:06:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:06:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:06:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:06:13] INFO:     127.0.0.1:53052 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:06:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:06:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:06:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:06:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:06:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:06:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:06:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:06:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:06:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:06:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:06:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:06:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:06:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:06:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:06:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:06:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:06:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:06:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:06:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:06:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:06:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:06:33] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:06:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:06:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:06:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:06:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:06:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:06:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-14 11:06:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:06:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:06:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:06:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:06:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:06:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:06:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:06:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:06:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:06:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:06:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:06:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:06:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:06:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:06:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:06:53] INFO:     127.0.0.1:32896 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:06:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:06:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.31, #queue-req: 0, 
[2025-12-14 11:06:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:06:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:06:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:06:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:06:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:07:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:07:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:07:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:07:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:07:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:07:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:07:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:07:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:07:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:07:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:07:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:07:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:07:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:07:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:07:13] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:07:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:07:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-14 11:07:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:07:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:07:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:07:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:07:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:07:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:07:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:07:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:07:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:07:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:07:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:07:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:07:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:07:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:07:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:07:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:07:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:07:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:07:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:07:33] INFO:     127.0.0.1:38680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:07:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:07:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-14 11:07:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-14 11:07:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:07:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:07:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:07:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:07:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:07:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:07:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:07:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:07:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:07:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:07:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:07:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:07:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:07:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:07:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:07:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:07:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:07:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:07:53] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:07:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:07:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:07:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:07:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:07:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:07:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:07:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:08:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:08:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:08:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:08:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-14 11:08:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-14 11:08:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:08:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:08:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:08:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:08:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:08:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:08:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:08:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:08:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:08:13] INFO:     127.0.0.1:42058 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:08:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:08:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:08:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:08:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:08:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:08:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-14 11:08:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:08:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:08:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:08:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:08:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:08:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:08:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:08:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:08:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:08:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:08:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:08:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:08:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:08:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:08:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:08:33] INFO:     127.0.0.1:37572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:08:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:08:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.31, #queue-req: 0, 
[2025-12-14 11:08:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:08:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:08:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:08:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:08:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-14 11:08:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:08:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:08:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:08:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:08:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:08:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:08:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:08:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:08:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:08:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:08:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:08:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:08:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:08:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:08:53] INFO:     127.0.0.1:37000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:08:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:08:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-14 11:08:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:08:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:08:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:08:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:08:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:09:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:09:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:09:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:09:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:09:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:09:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:09:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:09:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:09:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:09:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:09:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:09:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:09:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:09:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:09:13] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:09:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:09:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 11:09:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:09:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:09:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:09:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:09:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:09:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:09:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:09:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:09:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:09:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:09:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:09:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:09:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:09:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:09:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:09:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:09:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:09:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:09:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:09:33] INFO:     127.0.0.1:40240 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:09:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:09:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-14 11:09:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:09:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-14 11:09:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:09:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:09:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:09:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:09:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:09:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:09:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:09:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:09:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:09:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:09:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:09:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:09:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:09:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:09:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:09:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:09:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:09:53] INFO:     127.0.0.1:49704 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:09:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:09:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:09:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:09:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:09:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:09:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:09:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:10:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:10:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:10:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:10:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:10:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:10:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:10:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:10:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:10:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:10:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:10:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:10:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:10:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:10:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:10:13] INFO:     127.0.0.1:46624 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:10:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:10:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:10:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:10:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:10:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:10:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:10:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:10:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:10:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:10:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:10:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:10:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:10:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:10:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:10:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:10:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:10:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:10:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:10:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:10:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:10:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:10:33] INFO:     127.0.0.1:40520 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:10:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:10:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-14 11:10:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:10:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:10:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:10:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:10:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:10:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:10:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:10:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:10:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:10:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:10:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:10:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:10:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:10:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:10:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:10:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:10:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:10:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:10:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:10:53] INFO:     127.0.0.1:38256 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:10:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:10:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:10:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-14 11:10:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:10:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:10:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:10:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:11:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:11:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:11:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:11:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:11:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:11:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:11:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:11:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:11:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:11:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:11:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:11:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:11:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:11:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:11:13] INFO:     127.0.0.1:37822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:11:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:11:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.31, #queue-req: 0, 
[2025-12-14 11:11:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:11:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:11:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:11:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:11:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:11:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:11:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:11:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:11:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:11:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-14 11:11:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:11:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:11:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:11:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:11:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:11:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:11:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:11:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:11:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:11:33] INFO:     127.0.0.1:47738 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:11:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:11:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:11:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:11:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:11:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:11:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:11:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:11:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:11:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:11:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:11:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:11:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:11:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:11:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:11:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:11:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:11:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:11:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:11:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:11:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:11:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:11:53] INFO:     127.0.0.1:55288 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:11:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:11:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 11:11:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:11:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:11:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-14 11:11:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:11:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:12:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:12:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:12:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-14 11:12:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-14 11:12:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:12:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:12:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:12:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:12:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:12:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:12:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:12:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.13, #queue-req: 0, 
[2025-12-14 11:12:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.14, #queue-req: 0, 
[2025-12-14 11:12:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.16, #queue-req: 0, 
[2025-12-14 11:12:13] INFO:     127.0.0.1:50122 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:12:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:12:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-14 11:12:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:12:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:12:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:12:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:12:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:12:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:12:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:12:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:12:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:12:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:12:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:12:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:12:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:12:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:12:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:12:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:12:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:12:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:12:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:12:33] INFO:     127.0.0.1:48478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:12:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:12:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.31, #queue-req: 0, 
[2025-12-14 11:12:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:12:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:12:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:12:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:12:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:12:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:12:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:12:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:12:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:12:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:12:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:12:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:12:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:12:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:12:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:12:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:12:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:12:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:12:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:12:53] INFO:     127.0.0.1:38494 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:12:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:12:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-14 11:12:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:12:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:12:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:12:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:12:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:13:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:13:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:13:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:13:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:13:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:13:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:13:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:13:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:13:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:13:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:13:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:13:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:13:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:13:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:13:13] INFO:     127.0.0.1:60860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:13:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:13:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:13:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:13:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:13:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:13:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:13:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:13:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:13:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:13:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:13:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:13:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:13:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:13:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:13:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:13:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:13:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:13:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:13:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:13:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:13:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:13:33] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:13:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:13:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 11:13:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:13:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:13:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-14 11:13:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:13:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:13:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:13:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:13:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:13:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:13:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:13:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:13:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:13:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:13:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:13:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:13:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:13:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.18, #queue-req: 0, 
[2025-12-14 11:13:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.16, #queue-req: 0, 
[2025-12-14 11:13:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.18, #queue-req: 0, 
[2025-12-14 11:13:53] INFO:     127.0.0.1:57234 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:13:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:13:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-14 11:13:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:13:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:13:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-14 11:13:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:13:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-14 11:14:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-14 11:14:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:14:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:14:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:14:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:14:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:14:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:14:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:14:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:14:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:14:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:14:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.17, #queue-req: 0, 
[2025-12-14 11:14:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.17, #queue-req: 0, 
[2025-12-14 11:14:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.17, #queue-req: 0, 
[2025-12-14 11:14:13] INFO:     127.0.0.1:35444 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:14:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:14:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:14:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:14:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:14:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-14 11:14:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:14:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-14 11:14:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:14:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-14 11:14:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:14:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:14:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:14:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:14:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:14:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:14:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:14:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:14:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:14:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-14 11:14:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.17, #queue-req: 0, 
[2025-12-14 11:14:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.17, #queue-req: 0, 
[2025-12-14 11:14:33] INFO:     127.0.0.1:52486 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:14:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:14:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 11:14:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:14:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:14:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:14:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:14:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-14 11:14:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:14:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-14 11:14:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:14:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:14:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:14:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:14:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:14:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:14:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:14:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:14:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:14:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:14:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:14:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-14 11:14:53] INFO:     127.0.0.1:38028 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:14:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:14:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 11:14:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:14:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:14:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:14:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:14:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:15:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:15:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:15:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:15:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:15:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:15:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:15:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:15:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:15:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:15:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:15:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:15:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:15:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:15:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:15:13] INFO:     127.0.0.1:48080 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:15:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:15:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:15:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:15:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:15:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:15:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:15:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:15:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:15:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:15:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:15:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:15:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:15:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:15:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:15:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:15:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:15:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:15:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:15:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:15:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:15:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:15:33] INFO:     127.0.0.1:58652 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:15:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:15:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-14 11:15:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:15:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:15:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:15:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:15:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:15:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:15:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:15:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:15:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:15:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:15:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:15:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:15:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:15:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:15:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:15:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:15:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:15:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:15:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:15:53] INFO:     127.0.0.1:53616 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:15:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:15:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 11:15:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:15:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:15:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:15:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:15:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:16:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:16:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:16:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:16:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:16:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:16:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:16:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:16:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:16:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:16:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:16:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:16:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:16:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:16:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:16:13] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:16:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:16:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:16:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:16:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:16:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:16:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:16:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:16:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:16:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:16:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:16:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:16:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:16:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:16:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:16:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:16:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:16:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:16:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:16:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:16:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:16:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:16:33] INFO:     127.0.0.1:43340 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:16:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:16:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 11:16:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:16:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:16:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:16:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:16:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:16:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:16:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:16:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:16:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:16:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:16:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:16:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:16:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:16:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:16:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:16:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:16:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:16:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:16:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:16:53] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:16:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:16:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 32.30, #queue-req: 0, 
[2025-12-14 11:16:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:16:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:16:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:16:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:16:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:17:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:17:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:17:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:17:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:17:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:17:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:17:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:17:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:17:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:17:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:17:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:17:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:17:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:17:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:17:13] INFO:     127.0.0.1:41924 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:17:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:17:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-14 11:17:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:17:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.61, #queue-req: 0, 
[2025-12-14 11:17:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:17:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:17:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:17:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:17:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:17:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:17:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:17:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:17:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:17:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:17:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:17:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:17:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:17:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:17:31 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:17:32 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:17:33 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:17:33] INFO:     127.0.0.1:33864 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:17:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:17:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:17:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:17:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:17:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:17:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:17:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:17:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:17:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:17:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:17:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:17:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:17:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:17:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:17:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:17:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:17:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:17:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:17:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:17:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:17:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:17:53] INFO:     127.0.0.1:42542 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:17:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:17:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:17:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:17:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:17:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:17:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-14 11:17:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:18:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:18:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:18:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:18:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:18:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:18:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:18:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:18:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:18:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:18:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:18:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:18:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:18:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:18:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:18:13] INFO:     127.0.0.1:34968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:18:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:18:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 11:18:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:18:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:18:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:18:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:18:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:18:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:18:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:18:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:18:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:18:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:18:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:18:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:18:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:18:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:18:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:18:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:18:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:18:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:18:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:18:33] INFO:     127.0.0.1:42124 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:18:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:18:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 33.18, #queue-req: 0, 
[2025-12-14 11:18:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:18:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:18:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:18:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:18:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:18:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.59, #queue-req: 0, 
[2025-12-14 11:18:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:18:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:18:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:18:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:18:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:18:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:18:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:18:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:18:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:18:50 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:18:51 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:18:52 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:18:53 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:18:53] INFO:     127.0.0.1:33562 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:18:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:18:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.07, #queue-req: 0, 
[2025-12-14 11:18:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:18:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:18:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:18:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:18:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:19:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:19:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:19:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:19:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:19:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:19:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:19:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:19:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:19:08 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:19:09 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:19:10 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:19:11 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:19:12 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:19:13 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:19:13] INFO:     127.0.0.1:34546 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:19:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:19:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:19:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:19:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:19:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:19:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:19:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:19:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:19:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:19:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:19:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:19:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-14 11:19:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:19:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:19:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:19:28 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:19:29 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:19:30 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:19:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:19:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:19:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:19:33] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:19:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:19:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:19:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:19:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:19:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:19:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-14 11:19:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:19:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:19:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:19:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:19:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:19:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:19:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:19:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:19:47 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:19:48 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:19:49 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:19:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:19:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:19:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:19:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:19:53] INFO:     127.0.0.1:50954 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:19:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:19:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:19:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:19:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:19:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:19:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-14 11:19:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:20:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:20:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:20:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:20:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:20:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:20:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:20:06 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:20:07 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:20:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:20:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:20:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:20:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:20:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:20:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:20:13] INFO:     127.0.0.1:43210 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:20:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:20:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 33.98, #queue-req: 0, 
[2025-12-14 11:20:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:20:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:20:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:20:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:20:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:20:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:20:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:20:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:20:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:20:24 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:20:25 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:20:26 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:20:27 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:20:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:20:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:20:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:20:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:20:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:20:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:20:33] INFO:     127.0.0.1:52866 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:20:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:20:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-14 11:20:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:20:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:20:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:20:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:20:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:20:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:20:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:20:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:20:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:20:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:20:45 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:20:46 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:20:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:20:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:20:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:20:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:20:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:20:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:20:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:20:53] INFO:     127.0.0.1:56860 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:20:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:20:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:20:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:20:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:20:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:20:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:20:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:21:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:21:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:21:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:21:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:21:04 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:21:05 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:21:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:21:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:21:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:21:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:21:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:21:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:21:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:21:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:21:13] INFO:     127.0.0.1:51062 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:21:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:21:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.31, #queue-req: 0, 
[2025-12-14 11:21:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:21:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:21:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:21:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-14 11:21:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:21:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:21:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:21:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:21:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:21:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:21:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:21:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:21:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:21:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:21:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:21:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:21:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:21:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:21:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:21:33] INFO:     127.0.0.1:44888 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:21:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:21:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:21:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:21:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:21:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:21:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:21:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:21:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:21:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:21:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:21:42 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:21:43 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:21:44 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:21:45 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:21:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:21:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:21:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:21:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:21:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:21:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:21:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:21:53] INFO:     127.0.0.1:46712 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:21:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:21:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-14 11:21:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:21:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:21:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:21:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:21:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:22:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:22:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:22:01 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:22:02 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:22:03 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:22:04 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:22:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:22:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:22:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:22:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:22:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:22:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:22:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:22:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:22:13] INFO:     127.0.0.1:53322 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:22:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:22:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-14 11:22:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-14 11:22:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:22:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:22:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:22:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:22:19 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:22:20 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:22:21 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:22:22 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:22:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:22:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:22:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:22:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:22:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:22:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:22:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:22:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:22:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:22:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:22:33] INFO:     127.0.0.1:40846 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:22:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:22:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-14 11:22:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:22:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:22:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:22:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:22:38 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:22:39 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:22:40 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:22:41 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:22:42 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:22:43 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-14 11:22:44 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:22:45 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:22:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:22:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:22:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:22:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:22:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:22:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:22:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:22:53] INFO:     127.0.0.1:41300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:22:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:22:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:22:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:22:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:22:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:22:57 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-14 11:22:58 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:22:59 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:23:00 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:23:01 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:23:02 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:23:03 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:23:04 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:23:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:23:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:23:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:23:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:23:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:23:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:23:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:23:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:23:13] INFO:     127.0.0.1:45870 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:23:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:23:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 11:23:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:23:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:23:16 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:23:17 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:23:18 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:23:19 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:23:20 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:23:21 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:23:22 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:23:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:23:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:23:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:23:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:23:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:23:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:23:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:23:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:23:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:23:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:23:33] INFO:     127.0.0.1:37678 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:23:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:23:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.21, #queue-req: 0, 
[2025-12-14 11:23:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:23:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:23:36 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:23:37 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:23:38 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:23:39 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:23:40 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:23:41 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:23:42 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:23:43 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:23:44 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:23:45 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:23:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:23:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:23:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:23:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:23:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:23:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:23:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:23:53] INFO:     127.0.0.1:37842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:23:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:23:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:23:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:23:55 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:23:56 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:23:57 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:23:58 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:23:59 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:24:00 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:24:01 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:24:02 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:24:03 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:24:04 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:24:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:24:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:24:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:24:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:24:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:24:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:24:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:24:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:24:13] INFO:     127.0.0.1:36644 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:24:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:24:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-14 11:24:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:24:15 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:24:16 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:24:17 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:24:18 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:24:19 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:24:20 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:24:21 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:24:22 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:24:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-14 11:24:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:24:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:24:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:24:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:24:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:24:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:24:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:24:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:24:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:24:33] INFO:     127.0.0.1:43878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:24:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:24:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:24:34 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:24:35 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:24:36 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-14 11:24:37 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:24:38 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:24:39 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:24:40 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:24:41 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:24:42 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:24:43 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:24:44 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:24:45 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:24:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:24:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:24:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:24:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:24:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:24:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:24:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:24:53] INFO:     127.0.0.1:39208 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:24:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:24:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-14 11:24:54 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:24:55 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:24:56 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:24:57 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:24:58 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:24:59 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:25:00 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:25:01 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:25:02 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:25:03 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:25:04 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:25:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:25:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:25:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:25:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:25:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:25:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:25:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:25:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:25:13] INFO:     127.0.0.1:34884 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:25:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:25:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.22, #queue-req: 0, 
[2025-12-14 11:25:14 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:25:15 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:25:16 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:25:17 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:25:18 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:25:19 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:25:20 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:25:21 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:25:22 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:25:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:25:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:25:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:25:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:25:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:25:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:25:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:25:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:25:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:25:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:25:33] INFO:     127.0.0.1:60968 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:25:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:25:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:25:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:25:35 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:25:36 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-14 11:25:37 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:25:38 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:25:39 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:25:40 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:25:41 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:25:42 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:25:43 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:25:44 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:25:45 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:25:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:25:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:25:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:25:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:25:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:25:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:25:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:25:53] INFO:     127.0.0.1:41388 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:25:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:25:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-14 11:25:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:25:55 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:25:56 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:25:57 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:25:58 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:25:59 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:26:00 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:26:01 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:26:02 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:26:03 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-14 11:26:04 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:26:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:26:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:26:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:26:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:26:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:26:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:26:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:26:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:26:13] INFO:     127.0.0.1:49572 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:26:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:26:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-14 11:26:14 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:26:15 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:26:16 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:26:17 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.49, #queue-req: 0, 
[2025-12-14 11:26:18 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:26:19 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:26:20 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:26:21 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:26:22 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:26:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:26:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:26:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:26:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:26:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:26:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:26:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:26:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:26:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:26:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:26:33] INFO:     127.0.0.1:39608 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:26:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:26:33 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:26:34 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:26:35 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:26:36 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-14 11:26:37 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:26:38 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:26:39 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:26:40 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:26:41 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:26:42 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:26:43 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:26:44 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:26:45 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:26:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:26:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:26:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:26:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:26:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.16, #queue-req: 0, 
[2025-12-14 11:26:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.15, #queue-req: 0, 
[2025-12-14 11:26:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.18, #queue-req: 0, 
[2025-12-14 11:26:53] INFO:     127.0.0.1:38944 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:26:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:26:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.18, #queue-req: 0, 
[2025-12-14 11:26:54 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:26:55 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:26:56 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.50, #queue-req: 0, 
[2025-12-14 11:26:57 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-14 11:26:58 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.35, #queue-req: 0, 
[2025-12-14 11:26:59 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-14 11:27:00 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:27:01 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-14 11:27:02 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.36, #queue-req: 0, 
[2025-12-14 11:27:03 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:27:04 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:27:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:27:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:27:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:27:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:27:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-14 11:27:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.13, #queue-req: 0, 
[2025-12-14 11:27:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.13, #queue-req: 0, 
[2025-12-14 11:27:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.13, #queue-req: 0, 
[2025-12-14 11:27:13] INFO:     127.0.0.1:50306 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:27:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:27:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:27:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:27:15 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:27:16 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:27:17 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:27:18 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:27:19 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:27:20 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:27:21 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:27:22 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:27:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:27:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:27:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:27:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:27:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:27:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:27:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:27:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:27:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:27:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:27:33] INFO:     127.0.0.1:35906 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:27:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:27:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:27:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:27:35 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:27:36 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:27:37 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:27:38 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:27:39 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:27:40 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:27:41 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:27:42 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:27:43 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:27:44 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:27:45 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:27:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:27:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:27:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:27:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:27:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:27:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:27:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:27:53] INFO:     127.0.0.1:40350 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:27:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:27:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:27:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:27:55 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:27:56 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:27:57 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:27:58 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:27:59 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:28:00 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:28:01 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:28:02 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:28:03 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:28:04 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:28:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:28:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:28:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:28:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:28:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:28:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:28:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:28:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:28:13] INFO:     127.0.0.1:35688 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:28:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:28:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:28:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:28:15 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:28:16 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-14 11:28:17 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:28:18 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:28:19 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:28:20 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:28:21 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:28:22 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:28:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:28:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:28:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:28:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:28:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:28:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:28:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:28:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:28:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:28:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:28:33] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:28:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:28:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.23, #queue-req: 0, 
[2025-12-14 11:28:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:28:35 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:28:36 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:28:37 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:28:38 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:28:39 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:28:40 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:28:41 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:28:42 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:28:43 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:28:44 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:28:45 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:28:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:28:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:28:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:28:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:28:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:28:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:28:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:28:53] INFO:     127.0.0.1:41964 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:28:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:28:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:28:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:28:55 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:28:56 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:28:57 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:28:58 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:28:59 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:29:00 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:29:01 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:29:02 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:29:03 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:29:04 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:29:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:29:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:29:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:29:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:29:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:29:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:29:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:29:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:29:13] INFO:     127.0.0.1:36620 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:29:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:29:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 11:29:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:29:15 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:29:16 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:29:17 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:29:18 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:29:19 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:29:20 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:29:21 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:29:22 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:29:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:29:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:29:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:29:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:29:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:29:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:29:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:29:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:29:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:29:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:29:33] INFO:     127.0.0.1:45074 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:29:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:29:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:29:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:29:35 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:29:36 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-14 11:29:37 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:29:38 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:29:39 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:29:40 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:29:41 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:29:42 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:29:43 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:29:44 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:29:45 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:29:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:29:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:29:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:29:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:29:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:29:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:29:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:29:53] INFO:     127.0.0.1:53500 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:29:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:29:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.26, #queue-req: 0, 
[2025-12-14 11:29:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:29:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:29:56 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.51, #queue-req: 0, 
[2025-12-14 11:29:57 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:29:58 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:29:59 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:30:00 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:30:01 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:30:02 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:30:03 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:30:04 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:30:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:30:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:30:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:30:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:30:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:30:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:30:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-14 11:30:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:30:13] INFO:     127.0.0.1:43300 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:30:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:30:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.20, #queue-req: 0, 
[2025-12-14 11:30:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:30:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:30:16 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:30:17 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:30:18 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:30:19 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:30:20 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:30:21 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:30:22 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:30:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:30:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:30:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:30:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:30:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:30:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:30:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:30:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.17, #queue-req: 0, 
[2025-12-14 11:30:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.18, #queue-req: 0, 
[2025-12-14 11:30:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.19, #queue-req: 0, 
[2025-12-14 11:30:33] INFO:     127.0.0.1:59144 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:30:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:30:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 11:30:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:30:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:30:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-14 11:30:37 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:30:38 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:30:39 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:30:40 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:30:41 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:30:42 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:30:43 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:30:44 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:30:45 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:30:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:30:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:30:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:30:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.25, #queue-req: 0, 
[2025-12-14 11:30:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:30:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:30:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:30:53] INFO:     127.0.0.1:57856 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:30:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:30:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:30:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:30:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:30:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:30:57 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:30:58 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:30:59 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:31:00 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:31:01 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:31:02 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:31:03 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:31:04 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:31:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:31:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:31:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:31:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:31:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:31:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:31:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:31:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:31:13] INFO:     127.0.0.1:39360 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:31:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:31:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.28, #queue-req: 0, 
[2025-12-14 11:31:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:31:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:31:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:31:17 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:31:18 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:31:19 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:31:20 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:31:21 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:31:22 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:31:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:31:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:31:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:31:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:31:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:31:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:31:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:31:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:31:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:31:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:31:33] INFO:     127.0.0.1:36826 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:31:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:31:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 33.25, #queue-req: 0, 
[2025-12-14 11:31:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.54, #queue-req: 0, 
[2025-12-14 11:31:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:31:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-14 11:31:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:31:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:31:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:31:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:31:41 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:31:42 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:31:43 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:31:44 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:31:45 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:31:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:31:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:31:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:31:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:31:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:31:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:31:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:31:53] INFO:     127.0.0.1:43154 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:31:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:31:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.24, #queue-req: 0, 
[2025-12-14 11:31:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:31:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:31:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:31:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:31:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:32:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:32:00 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:32:01 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:32:02 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:32:03 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:32:04 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:32:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:32:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:32:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:32:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:32:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:32:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:32:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:32:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:32:13] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:32:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:32:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-14 11:32:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:32:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.58, #queue-req: 0, 
[2025-12-14 11:32:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:32:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:32:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:32:19 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:32:20 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:32:21 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:32:22 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:32:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:32:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:32:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:32:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:32:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:32:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:32:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:32:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:32:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:32:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:32:33] INFO:     127.0.0.1:42842 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:32:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:32:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.30, #queue-req: 0, 
[2025-12-14 11:32:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:32:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:32:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:32:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:32:38 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:32:39 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:32:40 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:32:41 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:32:42 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:32:43 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.34, #queue-req: 0, 
[2025-12-14 11:32:44 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:32:45 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:32:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:32:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:32:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:32:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.26, #queue-req: 0, 
[2025-12-14 11:32:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:32:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:32:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:32:53] INFO:     127.0.0.1:37680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:32:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:32:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:32:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:32:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:32:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.52, #queue-req: 0, 
[2025-12-14 11:32:57 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:32:58 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:32:59 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:33:00 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.37, #queue-req: 0, 
[2025-12-14 11:33:01 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:33:02 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:33:03 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:33:04 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:33:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:33:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:33:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:33:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:33:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:33:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.15, #queue-req: 0, 
[2025-12-14 11:33:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.07, #queue-req: 0, 
[2025-12-14 11:33:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:33:13] INFO:     127.0.0.1:43202 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:33:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:33:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 33.98, #queue-req: 0, 
[2025-12-14 11:33:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.45, #queue-req: 0, 
[2025-12-14 11:33:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.48, #queue-req: 0, 
[2025-12-14 11:33:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:33:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:33:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:33:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:33:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:33:21 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:33:22 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:33:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:33:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:33:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:33:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:33:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:33:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:33:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.16, #queue-req: 0, 
[2025-12-14 11:33:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.09, #queue-req: 0, 
[2025-12-14 11:33:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.08, #queue-req: 0, 
[2025-12-14 11:33:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.08, #queue-req: 0, 
[2025-12-14 11:33:33] INFO:     127.0.0.1:34680 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:33:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:33:34 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.10, #queue-req: 0, 
[2025-12-14 11:33:35 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:33:36 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.57, #queue-req: 0, 
[2025-12-14 11:33:37 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:33:38 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.47, #queue-req: 0, 
[2025-12-14 11:33:39 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:33:40 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.42, #queue-req: 0, 
[2025-12-14 11:33:41 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:33:42 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.44, #queue-req: 0, 
[2025-12-14 11:33:43 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:33:44 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.33, #queue-req: 0, 
[2025-12-14 11:33:44 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:33:45 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:33:46 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:33:47 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:33:48 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:33:49 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.27, #queue-req: 0, 
[2025-12-14 11:33:50 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:33:51 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:33:52 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:33:53] INFO:     127.0.0.1:42000 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:33:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:33:54 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.29, #queue-req: 0, 
[2025-12-14 11:33:55 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:33:56 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.56, #queue-req: 0, 
[2025-12-14 11:33:57 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:33:58 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.43, #queue-req: 0, 
[2025-12-14 11:33:59 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:34:00 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.40, #queue-req: 0, 
[2025-12-14 11:34:01 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:34:02 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:34:03 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.41, #queue-req: 0, 
[2025-12-14 11:34:03 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.32, #queue-req: 0, 
[2025-12-14 11:34:04 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:34:05 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:34:06 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:34:07 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:34:08 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:34:09 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.24, #queue-req: 0, 
[2025-12-14 11:34:10 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:34:11 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:34:12 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.21, #queue-req: 0, 
[2025-12-14 11:34:13] INFO:     127.0.0.1:54440 - "POST /generate HTTP/1.1" 200 OK
[2025-12-14 11:34:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-14 11:34:14 TP0] Decode batch, #running-req: 1, #token: 3221, token usage: 0.00, cuda graph: True, gen throughput (token/s): 34.27, #queue-req: 0, 
[2025-12-14 11:34:15 TP0] Decode batch, #running-req: 1, #token: 3261, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:34:16 TP0] Decode batch, #running-req: 1, #token: 3301, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.55, #queue-req: 0, 
[2025-12-14 11:34:17 TP0] Decode batch, #running-req: 1, #token: 3341, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.53, #queue-req: 0, 
[2025-12-14 11:34:18 TP0] Decode batch, #running-req: 1, #token: 3381, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.46, #queue-req: 0, 
[2025-12-14 11:34:19 TP0] Decode batch, #running-req: 1, #token: 3421, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.38, #queue-req: 0, 
[2025-12-14 11:34:20 TP0] Decode batch, #running-req: 1, #token: 3461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:34:21 TP0] Decode batch, #running-req: 1, #token: 3501, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:34:22 TP0] Decode batch, #running-req: 1, #token: 3541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:34:23 TP0] Decode batch, #running-req: 1, #token: 3581, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.39, #queue-req: 0, 
[2025-12-14 11:34:23 TP0] Decode batch, #running-req: 1, #token: 3621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.31, #queue-req: 0, 
[2025-12-14 11:34:24 TP0] Decode batch, #running-req: 1, #token: 3661, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.30, #queue-req: 0, 
[2025-12-14 11:34:25 TP0] Decode batch, #running-req: 1, #token: 3701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.29, #queue-req: 0, 
[2025-12-14 11:34:26 TP0] Decode batch, #running-req: 1, #token: 3741, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:34:27 TP0] Decode batch, #running-req: 1, #token: 3781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:34:28 TP0] Decode batch, #running-req: 1, #token: 3821, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.28, #queue-req: 0, 
[2025-12-14 11:34:29 TP0] Decode batch, #running-req: 1, #token: 3861, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.23, #queue-req: 0, 
[2025-12-14 11:34:30 TP0] Decode batch, #running-req: 1, #token: 3901, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.20, #queue-req: 0, 
[2025-12-14 11:34:31 TP0] Decode batch, #running-req: 1, #token: 3941, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:34:32 TP0] Decode batch, #running-req: 1, #token: 3981, token usage: 0.00, cuda graph: True, gen throughput (token/s): 40.22, #queue-req: 0, 
[2025-12-14 11:34:33] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-14 11:34:33] INFO:     127.0.0.1:56454 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-14 11:34:33] Endpoint '/get_server_info' is deprecated and will be removed in a future version. Please use '/server_info' instead.
[2025-12-14 11:34:33] INFO:     127.0.0.1:56460 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-12-14 11:34:36] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-12-14 11:34:36] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
