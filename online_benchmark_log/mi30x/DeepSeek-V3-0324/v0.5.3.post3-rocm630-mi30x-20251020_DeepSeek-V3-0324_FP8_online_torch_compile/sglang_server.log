INFO 10-21 00:10:46 __init__.py:179] Automatically detected platform rocm.
WARNING 10-21 00:10:46 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-21 00:10:48] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-21 00:10:49] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, mem_fraction_static=0.9, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', elastic_ep_backend=None, mooncake_ib_device=None, tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=146107124, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, enable_trace=False, oltp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='triton', max_lora_chunk_size=16, attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill='flashmla_prefill', nsa_decode='fa3', enable_beta_spec=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=True, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8)
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-21 00:10:49] Using default HuggingFace chat template with detected content format: string
INFO 10-21 00:11:07 __init__.py:179] Automatically detected platform rocm.
INFO 10-21 00:11:07 __init__.py:179] Automatically detected platform rocm.
INFO 10-21 00:11:07 __init__.py:179] Automatically detected platform rocm.
INFO 10-21 00:11:07 __init__.py:179] Automatically detected platform rocm.
INFO 10-21 00:11:07 __init__.py:179] Automatically detected platform rocm.
INFO 10-21 00:11:08 __init__.py:179] Automatically detected platform rocm.
INFO 10-21 00:11:08 __init__.py:179] Automatically detected platform rocm.
INFO 10-21 00:11:08 __init__.py:179] Automatically detected platform rocm.
INFO 10-21 00:11:08 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-21 00:11:09] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-21 00:11:09] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-21 00:11:09 TP1] Process 289 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-21 00:11:09 TP3] Process 291 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-21 00:11:10] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-21 00:11:10 TP1] Init torch distributed begin.
[2025-10-21 00:11:10 TP3] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-21 00:11:10] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-21 00:11:10 TP0] Process 288 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-21 00:11:10] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-21 00:11:10 TP2] Process 290 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
`torch_dtype` is deprecated! Use `dtype` instead!
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:60: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
[2025-10-21 00:11:10] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-21 00:11:10 TP5] Process 293 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-21 00:11:10 TP0] Init torch distributed begin.
[2025-10-21 00:11:10] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-21 00:11:10 TP7] Process 295 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-21 00:11:10] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-21 00:11:10 TP6] Process 294 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-21 00:11:11 TP2] Init torch distributed begin.
[2025-10-21 00:11:11 TP5] Init torch distributed begin.
[2025-10-21 00:11:11] INFO trace.py:48: opentelemetry package is not installed, tracing disabled
[2025-10-21 00:11:11 TP4] Process 292 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
`torch_dtype` is deprecated! Use `dtype` instead!
[2025-10-21 00:11:11 TP7] Init torch distributed begin.
[2025-10-21 00:11:11 TP6] Init torch distributed begin.
[2025-10-21 00:11:11 TP4] Init torch distributed begin.
[2025-10-21 00:11:11 TP0] sglang is using nccl==2.21.5
[2025-10-21 00:11:13 TP0] Init torch distributed ends. mem usage=3.63 GB
[2025-10-21 00:11:13 TP5] Init torch distributed ends. mem usage=3.91 GB
[2025-10-21 00:11:13 TP6] Init torch distributed ends. mem usage=3.93 GB
[2025-10-21 00:11:13 TP7] Init torch distributed ends. mem usage=3.92 GB
[2025-10-21 00:11:13 TP4] Init torch distributed ends. mem usage=3.99 GB
[2025-10-21 00:11:13 TP3] Init torch distributed ends. mem usage=4.04 GB
[2025-10-21 00:11:13 TP2] Init torch distributed ends. mem usage=4.05 GB
[2025-10-21 00:11:13 TP1] Init torch distributed ends. mem usage=4.05 GB
[2025-10-21 00:11:15 TP5] Load weight begin. avail mem=187.35 GB
[2025-10-21 00:11:15 TP4] Load weight begin. avail mem=187.27 GB
[2025-10-21 00:11:15 TP6] Load weight begin. avail mem=187.33 GB
[2025-10-21 00:11:15 TP7] Load weight begin. avail mem=187.34 GB
[2025-10-21 00:11:15 TP0] Load weight begin. avail mem=187.63 GB
[2025-10-21 00:11:15 TP0] Detected fp8 checkpoint.
[2025-10-21 00:11:15 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-10-21 00:11:15 TP3] Load weight begin. avail mem=187.22 GB
[2025-10-21 00:11:15 TP1] Load weight begin. avail mem=187.21 GB
[2025-10-21 00:11:15 TP2] Load weight begin. avail mem=187.21 GB
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:40,  3.99it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:31,  5.18it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:27,  5.76it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:24,  6.47it/s]
Loading safetensors checkpoint shards:   4% Completed | 6/163 [00:00<00:15,  9.83it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:01<00:23,  6.56it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:01<00:39,  3.87it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:01<00:26,  5.71it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:02<00:20,  7.46it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:02<00:16,  8.93it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:02<00:19,  7.58it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:03<00:29,  4.91it/s]
Loading safetensors checkpoint shards:  12% Completed | 20/163 [00:03<00:31,  4.56it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:03<00:33,  4.29it/s]
Loading safetensors checkpoint shards:  13% Completed | 22/163 [00:04<00:32,  4.38it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:04<00:33,  4.16it/s]
Loading safetensors checkpoint shards:  15% Completed | 25/163 [00:04<00:22,  6.18it/s]
Loading safetensors checkpoint shards:  17% Completed | 28/163 [00:04<00:15,  8.55it/s]
Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:04<00:11, 11.39it/s]
Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:05<00:11, 11.29it/s]
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:05<00:21,  5.88it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:05<00:15,  7.82it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:06<00:13,  9.17it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:06<00:11, 10.42it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:06<00:12,  9.46it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:06<00:10, 10.89it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:06<00:09, 12.27it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:06<00:08, 13.57it/s]
Loading safetensors checkpoint shards:  33% Completed | 54/163 [00:06<00:07, 14.77it/s]
Loading safetensors checkpoint shards:  34% Completed | 56/163 [00:07<00:07, 14.02it/s]
Loading safetensors checkpoint shards:  36% Completed | 58/163 [00:07<00:07, 14.85it/s]
Loading safetensors checkpoint shards:  37% Completed | 60/163 [00:07<00:07, 13.35it/s]
Loading safetensors checkpoint shards:  39% Completed | 63/163 [00:07<00:06, 16.37it/s]
Loading safetensors checkpoint shards:  40% Completed | 65/163 [00:08<00:15,  6.48it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:08<00:10,  8.82it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:08<00:08, 10.89it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:08<00:08, 10.78it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:08<00:07, 11.97it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:09<00:05, 14.67it/s]
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:09<00:05, 13.88it/s]
Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:09<00:05, 15.64it/s]
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:09<00:05, 14.37it/s]
Loading safetensors checkpoint shards:  53% Completed | 87/163 [00:09<00:04, 15.47it/s]
Loading safetensors checkpoint shards:  55% Completed | 89/163 [00:09<00:04, 16.28it/s]
Loading safetensors checkpoint shards:  56% Completed | 92/163 [00:09<00:03, 18.27it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:10<00:03, 18.42it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:10<00:06, 10.13it/s]
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:10<00:05, 11.24it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:10<00:05, 12.51it/s]
Loading safetensors checkpoint shards:  63% Completed | 103/163 [00:11<00:09,  6.15it/s]
Loading safetensors checkpoint shards:  65% Completed | 106/163 [00:11<00:06,  8.24it/s]
Loading safetensors checkpoint shards:  66% Completed | 108/163 [00:11<00:06,  9.14it/s]
Loading safetensors checkpoint shards:  67% Completed | 110/163 [00:11<00:05, 10.59it/s]
Loading safetensors checkpoint shards:  69% Completed | 113/163 [00:12<00:03, 13.28it/s]
Loading safetensors checkpoint shards:  71% Completed | 116/163 [00:12<00:03, 15.30it/s]
Loading safetensors checkpoint shards:  73% Completed | 119/163 [00:12<00:02, 17.08it/s]
Loading safetensors checkpoint shards:  75% Completed | 122/163 [00:12<00:03, 12.73it/s]
Loading safetensors checkpoint shards:  76% Completed | 124/163 [00:12<00:03, 12.62it/s]
Loading safetensors checkpoint shards:  78% Completed | 127/163 [00:13<00:02, 14.73it/s]
Loading safetensors checkpoint shards:  80% Completed | 130/163 [00:13<00:01, 16.87it/s]
Loading safetensors checkpoint shards:  82% Completed | 133/163 [00:13<00:02, 13.77it/s]
Loading safetensors checkpoint shards:  83% Completed | 135/163 [00:13<00:01, 14.77it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:13<00:01, 16.88it/s]
Loading safetensors checkpoint shards:  86% Completed | 140/163 [00:13<00:01, 14.87it/s]
Loading safetensors checkpoint shards:  87% Completed | 142/163 [00:14<00:01, 15.22it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:14<00:01, 14.70it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:14<00:01, 15.39it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:14<00:01, 13.88it/s]
Loading safetensors checkpoint shards:  93% Completed | 151/163 [00:15<00:02,  5.60it/s]
Loading safetensors checkpoint shards:  94% Completed | 153/163 [00:15<00:01,  6.99it/s]
Loading safetensors checkpoint shards:  95% Completed | 155/163 [00:15<00:01,  7.72it/s]
Loading safetensors checkpoint shards:  97% Completed | 158/163 [00:15<00:00, 10.13it/s]
Loading safetensors checkpoint shards:  98% Completed | 160/163 [00:15<00:00, 11.52it/s]
Loading safetensors checkpoint shards:  99% Completed | 162/163 [00:16<00:00, 10.86it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:16<00:00, 10.03it/s]

[2025-10-21 00:12:07 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.64 GB, mem usage=79.56 GB.
[2025-10-21 00:12:07 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.65 GB, mem usage=79.56 GB.
[2025-10-21 00:12:08 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.06 GB, mem usage=79.56 GB.
[2025-10-21 00:12:11 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.65 GB, mem usage=79.56 GB.
[2025-10-21 00:12:26 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.79 GB, mem usage=79.56 GB.
[2025-10-21 00:12:30 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.70 GB, mem usage=79.56 GB.
[2025-10-21 00:12:31 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.77 GB, mem usage=79.56 GB.
[2025-10-21 00:12:36 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.78 GB, mem usage=79.56 GB.
[2025-10-21 00:12:36 TP0] Using KV cache dtype: torch.bfloat16
[2025-10-21 00:12:36 TP7] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-21 00:12:36 TP2] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-21 00:12:36 TP4] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-21 00:12:36 TP5] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-21 00:12:36 TP7] Memory pool end. avail mem=43.53 GB
[2025-10-21 00:12:36 TP6] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-21 00:12:36 TP2] Memory pool end. avail mem=43.39 GB
[2025-10-21 00:12:36 TP4] Memory pool end. avail mem=43.45 GB
[2025-10-21 00:12:36 TP5] Memory pool end. avail mem=43.54 GB
[2025-10-21 00:12:36 TP6] Memory pool end. avail mem=43.52 GB
[2025-10-21 00:12:36 TP3] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-21 00:12:36 TP3] Memory pool end. avail mem=43.40 GB
[2025-10-21 00:12:36 TP0] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-21 00:12:36 TP1] KV Cache is allocated. #tokens: 971748, KV size: 63.60 GB
[2025-10-21 00:12:36 TP0] Memory pool end. avail mem=43.81 GB
[2025-10-21 00:12:36 TP1] Memory pool end. avail mem=43.40 GB
[2025-10-21 00:12:38 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.33 GB
[2025-10-21 00:12:38 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.61 GB
[2025-10-21 00:12:38 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-10-21 00:12:38 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.19 GB
[2025-10-21 00:12:38 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.20 GB
[2025-10-21 00:12:38 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.19 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.97 GB):   0%|          | 0/52 [00:00<?, ?it/s][2025-10-21 00:12:39 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.33 GB
[2025-10-21 00:12:39 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.25 GB
[2025-10-21 00:12:39 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.32 GB
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-21 00:12:42 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-21 00:12:42 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:42 TP6] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:42 TP4] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-21 00:12:43 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:43 TP7] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-21 00:12:43 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:43 TP5] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-21 00:12:44 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-21 00:12:44 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-21 00:12:44 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-21 00:12:44 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:44 TP6] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:44 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:44 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:44 TP4] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:44 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:12:44 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:44 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:44 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:12:44 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:44 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:12:44 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:12:44 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:45 TP7] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:45 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:45 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:45 TP5] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:12:45 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:45 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:45 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:45 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:12:45 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:12:45 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:12:45 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:12:45 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-21 00:13:05 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-21 00:13:05 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-21 00:13:05 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-10-21 00:13:05 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:05 TP3] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:05 TP1] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:05 TP0] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:05 TP2] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-21 00:13:06 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-21 00:13:06 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-21 00:13:07 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-10-21 00:13:07 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP3] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:07 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP1] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:07 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP2] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP0] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:07 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:07 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:07 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
Capturing batches (bs=512 avail_mem=42.97 GB):   2%|         | 1/52 [00:29<24:57, 29.37s/it]Capturing batches (bs=496 avail_mem=42.30 GB):   2%|         | 1/52 [00:29<24:57, 29.37s/it][aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:08 TP4] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:08 TP7] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:08 TP2] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:08 TP1] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:08 TP3] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:08 TP0] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:09 TP5] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:09 TP6] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=496 avail_mem=42.30 GB):   4%|         | 2/52 [00:31<10:54, 13.09s/it]Capturing batches (bs=480 avail_mem=42.29 GB):   4%|         | 2/52 [00:31<10:54, 13.09s/it][aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:09 TP6] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:09 TP4] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:09 TP5] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:09 TP7] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:09 TP0] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:09 TP3] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:09 TP2] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:09 TP1] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=480 avail_mem=42.29 GB):   6%|         | 3/52 [00:31<05:57,  7.30s/it]Capturing batches (bs=464 avail_mem=42.28 GB):   6%|         | 3/52 [00:31<05:57,  7.30s/it][aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP5] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP4] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP7] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP6] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP3] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP1] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP2] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP0] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=464 avail_mem=42.28 GB):   8%|         | 4/52 [00:31<03:39,  4.58s/it]Capturing batches (bs=448 avail_mem=42.28 GB):   8%|         | 4/52 [00:31<03:39,  4.58s/it][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=448 avail_mem=42.28 GB):  10%|         | 5/52 [00:32<02:25,  3.09s/it]Capturing batches (bs=432 avail_mem=42.27 GB):  10%|         | 5/52 [00:32<02:25,  3.09s/it][aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP2] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP5] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP3] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP1] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP6] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP7] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP4] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:10 TP0] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=432 avail_mem=42.27 GB):  12%|        | 6/52 [00:32<01:40,  2.18s/it]Capturing batches (bs=416 avail_mem=42.27 GB):  12%|        | 6/52 [00:32<01:40,  2.18s/it][aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP6] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP7] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP4] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP1] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP3] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP2] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP5] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP0] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=416 avail_mem=42.27 GB):  13%|        | 7/52 [00:33<01:12,  1.60s/it]Capturing batches (bs=400 avail_mem=42.26 GB):  13%|        | 7/52 [00:33<01:12,  1.60s/it][aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP6] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP7] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP0] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP5] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP3] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP1] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP4] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:11 TP2] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=400 avail_mem=42.26 GB):  15%|        | 8/52 [00:33<00:53,  1.23s/it]Capturing batches (bs=384 avail_mem=42.26 GB):  15%|        | 8/52 [00:33<00:53,  1.23s/it][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=384 avail_mem=42.26 GB):  17%|        | 9/52 [00:33<00:39,  1.08it/s]Capturing batches (bs=368 avail_mem=42.25 GB):  17%|        | 9/52 [00:33<00:39,  1.08it/s][aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP2] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP6] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP1] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP4] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP7] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP0] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP3] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP5] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=368 avail_mem=42.25 GB):  19%|        | 10/52 [00:34<00:32,  1.29it/s]Capturing batches (bs=352 avail_mem=42.25 GB):  19%|        | 10/52 [00:34<00:32,  1.29it/s][aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP3] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP1] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP0] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP2] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP7] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP5] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP6] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:12 TP4] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=352 avail_mem=42.25 GB):  21%|        | 11/52 [00:34<00:27,  1.50it/s]Capturing batches (bs=336 avail_mem=42.24 GB):  21%|        | 11/52 [00:34<00:27,  1.50it/s][aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP6] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP7] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP0] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP3] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP2] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP1] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP4] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP5] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=336 avail_mem=42.24 GB):  23%|       | 12/52 [00:35<00:23,  1.68it/s]Capturing batches (bs=320 avail_mem=42.24 GB):  23%|       | 12/52 [00:35<00:23,  1.68it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=320 avail_mem=42.24 GB):  25%|       | 13/52 [00:35<00:19,  2.00it/s]Capturing batches (bs=304 avail_mem=42.24 GB):  25%|       | 13/52 [00:35<00:19,  2.00it/s][aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP5] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP7] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP6] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP3] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP0] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP4] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP1] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:13 TP2] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=304 avail_mem=42.24 GB):  27%|       | 14/52 [00:35<00:18,  2.10it/s]Capturing batches (bs=288 avail_mem=42.23 GB):  27%|       | 14/52 [00:35<00:18,  2.10it/s][aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP2] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP0] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP1] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP4] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP7] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP5] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP6] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP3] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=288 avail_mem=42.23 GB):  29%|       | 15/52 [00:36<00:15,  2.40it/s]Capturing batches (bs=272 avail_mem=42.23 GB):  29%|       | 15/52 [00:36<00:15,  2.40it/s][aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP4] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP5] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP2] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP0] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP3] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP1] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP6] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:14 TP7] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=272 avail_mem=42.23 GB):  31%|       | 16/52 [00:36<00:15,  2.38it/s]Capturing batches (bs=256 avail_mem=42.22 GB):  31%|       | 16/52 [00:36<00:15,  2.38it/s][aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:14 TP2] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:14 TP0] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:14 TP1] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:14 TP5] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:14 TP4] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:14 TP7] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:14 TP3] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:14 TP6] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP6] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP2] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:15 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP0] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:15 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:15 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP1] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP7] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:15 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP5] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:15 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:15 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP3] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP4] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:15 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:15 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:15 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=256 avail_mem=42.22 GB):  33%|      | 17/52 [00:36<00:14,  2.37it/s]Capturing batches (bs=248 avail_mem=42.22 GB):  33%|      | 17/52 [00:36<00:14,  2.37it/s][aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP3] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP2] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP4] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP1] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP6] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP7] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP0] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP5] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=248 avail_mem=42.22 GB):  35%|      | 18/52 [00:37<00:14,  2.36it/s]Capturing batches (bs=240 avail_mem=42.21 GB):  35%|      | 18/52 [00:37<00:14,  2.36it/s][aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP5] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP4] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP6] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP3] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP1] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP0] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP2] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:15 TP7] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=240 avail_mem=42.21 GB):  37%|      | 19/52 [00:37<00:13,  2.36it/s]Capturing batches (bs=232 avail_mem=42.21 GB):  37%|      | 19/52 [00:37<00:13,  2.36it/s][aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP0] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP2] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP1] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP5] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP3] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP6] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP7] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP4] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=232 avail_mem=42.21 GB):  38%|      | 20/52 [00:38<00:13,  2.36it/s]Capturing batches (bs=224 avail_mem=42.20 GB):  38%|      | 20/52 [00:38<00:13,  2.36it/s][aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP6] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP7] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP1] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP2] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP3] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP5] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP4] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:16 TP0] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=224 avail_mem=42.20 GB):  40%|      | 21/52 [00:38<00:13,  2.37it/s]Capturing batches (bs=216 avail_mem=42.20 GB):  40%|      | 21/52 [00:38<00:13,  2.37it/s][aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP6] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP7] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP5] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP4] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP3] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP2] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP1] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP0] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=216 avail_mem=42.20 GB):  42%|     | 22/52 [00:39<00:12,  2.36it/s]Capturing batches (bs=208 avail_mem=42.19 GB):  42%|     | 22/52 [00:39<00:12,  2.36it/s][aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP4] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP5] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP7] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP6] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP1] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP0] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP2] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP3] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=208 avail_mem=42.19 GB):  44%|     | 23/52 [00:39<00:10,  2.65it/s]Capturing batches (bs=200 avail_mem=42.19 GB):  44%|     | 23/52 [00:39<00:10,  2.65it/s][aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP7] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP6] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP2] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP5] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP1] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP3] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP4] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:17 TP0] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=200 avail_mem=42.19 GB):  46%|     | 24/52 [00:39<00:10,  2.55it/s]Capturing batches (bs=192 avail_mem=42.18 GB):  46%|     | 24/52 [00:39<00:10,  2.55it/s][aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=192 avail_mem=42.18 GB):  48%|     | 25/52 [00:40<00:09,  2.80it/s]Capturing batches (bs=184 avail_mem=42.18 GB):  48%|     | 25/52 [00:40<00:09,  2.80it/s][aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP1] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP4] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP2] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP0] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP3] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP5] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP7] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP6] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=184 avail_mem=42.18 GB):  50%|     | 26/52 [00:40<00:08,  3.01it/s]Capturing batches (bs=176 avail_mem=42.18 GB):  50%|     | 26/52 [00:40<00:08,  3.01it/s][aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP1] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP0] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP3] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP2] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP5] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP4] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP7] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:18 TP6] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=176 avail_mem=42.18 GB):  52%|    | 27/52 [00:40<00:07,  3.16it/s]Capturing batches (bs=168 avail_mem=42.17 GB):  52%|    | 27/52 [00:40<00:07,  3.16it/s][aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP3] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP6] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP0] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP1] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP2] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP7] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP5] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP4] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=168 avail_mem=42.17 GB):  54%|    | 28/52 [00:41<00:08,  2.85it/s]Capturing batches (bs=160 avail_mem=42.17 GB):  54%|    | 28/52 [00:41<00:08,  2.85it/s][aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP2] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP0] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP1] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP3] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP5] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP4] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP7] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP6] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=160 avail_mem=42.17 GB):  56%|    | 29/52 [00:41<00:07,  3.05it/s]Capturing batches (bs=152 avail_mem=42.17 GB):  56%|    | 29/52 [00:41<00:07,  3.05it/s][aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP7] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP6] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP4] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP5] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP3] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP0] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP2] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:19 TP1] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=152 avail_mem=42.17 GB):  58%|    | 30/52 [00:41<00:07,  2.80it/s]Capturing batches (bs=144 avail_mem=42.16 GB):  58%|    | 30/52 [00:41<00:07,  2.80it/s][aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP0] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP1] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP2] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP5] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP4] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP3] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP7] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP6] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=144 avail_mem=42.16 GB):  60%|    | 31/52 [00:42<00:06,  3.01it/s]Capturing batches (bs=136 avail_mem=42.16 GB):  60%|    | 31/52 [00:42<00:06,  3.01it/s][aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP2] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP0] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP1] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP5] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP4] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP3] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP7] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP6] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=136 avail_mem=42.16 GB):  62%|   | 32/52 [00:42<00:06,  3.17it/s]Capturing batches (bs=128 avail_mem=42.16 GB):  62%|   | 32/52 [00:42<00:06,  3.17it/s][aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:20 TP2] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:20 TP1] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:20 TP0] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:20 TP4] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:20 TP3] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:20 TP5] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:20 TP7] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:20 TP6] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP2] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP0] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP1] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP5] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP4] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP3] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP7] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP6] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:20 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:20 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:20 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:20 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:20 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:20 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:20 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:20 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:20 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:20 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:20 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=128 avail_mem=42.16 GB):  63%|   | 33/52 [00:42<00:05,  3.26it/s]Capturing batches (bs=120 avail_mem=42.15 GB):  63%|   | 33/52 [00:42<00:05,  3.26it/s][aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP1] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP2] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP5] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP6] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP7] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP0] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP3] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP4] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.15 GB):  65%|   | 34/52 [00:42<00:06,  2.93it/s]Capturing batches (bs=112 avail_mem=42.15 GB):  65%|   | 34/52 [00:42<00:06,  2.93it/s][aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP2] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP1] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP0] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP5] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP4] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP3] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP7] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP6] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.15 GB):  67%|   | 35/52 [00:43<00:05,  3.09it/s]Capturing batches (bs=104 avail_mem=42.14 GB):  67%|   | 35/52 [00:43<00:05,  3.09it/s][aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP4] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP6] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP0] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP2] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP1] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP3] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP7] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:21 TP5] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.14 GB):  69%|   | 36/52 [00:43<00:05,  2.83it/s]Capturing batches (bs=96 avail_mem=42.14 GB):  69%|   | 36/52 [00:43<00:05,  2.83it/s] [aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.14 GB):  71%|   | 37/52 [00:43<00:04,  3.03it/s]Capturing batches (bs=88 avail_mem=42.14 GB):  71%|   | 37/52 [00:43<00:04,  3.03it/s][aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP6] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP0] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP2] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP1] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP5] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP3] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP4] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP7] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.14 GB):  73%|  | 38/52 [00:44<00:05,  2.79it/s]Capturing batches (bs=80 avail_mem=42.13 GB):  73%|  | 38/52 [00:44<00:05,  2.79it/s][aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP2] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP1] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP0] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP3] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP4] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP5] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP7] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:22 TP6] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.13 GB):  75%|  | 39/52 [00:44<00:04,  3.00it/s]Capturing batches (bs=72 avail_mem=42.13 GB):  75%|  | 39/52 [00:44<00:04,  3.00it/s][aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP6] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP5] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP1] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP3] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP0] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP2] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP4] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP7] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.13 GB):  77%|  | 40/52 [00:45<00:04,  2.77it/s]Capturing batches (bs=64 avail_mem=42.12 GB):  77%|  | 40/52 [00:45<00:04,  2.77it/s][aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:23 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:23 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:23 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:23 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:23 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:23 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:23 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-10-21 00:13:23 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:23 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:23 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:23 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:23 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:23 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:23 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:23 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:23 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:23 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-10-21 00:13:23 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.12 GB):  79%|  | 41/52 [00:45<00:03,  2.98it/s]Capturing batches (bs=56 avail_mem=42.12 GB):  79%|  | 41/52 [00:45<00:03,  2.98it/s][aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP4] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP7] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP6] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP0] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP1] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP2] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP3] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:23 TP5] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.12 GB):  81%|  | 42/52 [00:45<00:03,  2.77it/s]Capturing batches (bs=48 avail_mem=42.11 GB):  81%|  | 42/52 [00:45<00:03,  2.77it/s][aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP2] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP0] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP1] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP4] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP3] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP5] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP7] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP6] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.11 GB):  83%| | 43/52 [00:46<00:03,  2.96it/s]Capturing batches (bs=40 avail_mem=42.11 GB):  83%| | 43/52 [00:46<00:03,  2.96it/s][aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP7] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP6] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP5] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP3] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP4] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP1] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP2] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:13:24 TP0] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.11 GB):  85%| | 44/52 [00:46<00:02,  2.78it/s]Capturing batches (bs=32 avail_mem=42.10 GB):  85%| | 44/52 [00:46<00:02,  2.78it/s][rank2]:W1021 00:13:30.711000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1021 00:13:30.736000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:13:30.756000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1021 00:13:30.786000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:13:30.795000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1021 00:13:30.823000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:13:30.832000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:13:30.842000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1021 00:13:30.871000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:13:30.911000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:13:30.919000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:13:30.928000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1021 00:13:30.957000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:13:30.976000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:13:30.995000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank3]:W1021 00:13:31.004000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1021 00:13:31.054000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1021 00:13:31.081000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1021 00:13:31.128000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:13:31.168000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1021 00:13:31.307000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1021 00:13:31.505000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.dynamic_per_token_scaled_quant. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank1]:W1021 00:13:31.595000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:13:31.735000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/0] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.gemm_a8w8_blockscale. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1021 00:13:32.658000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:13:32.689000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:13:32.714000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:13:32.735000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:13:32.789000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:13:32.880000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:13:33.058000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:13:33.474000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:13:34.065000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:13:34.093000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:13:34.141000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:13:34.170000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:13:34.254000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:13:34.273000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:13:34.299000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:34 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:34 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1021 00:13:34.376000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:13:34.474000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:13:34.478000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:34 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1021 00:13:34.539000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:13:34.553000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:13:34.574000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:13:34.616000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:13:34.651000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:13:34.656000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:34 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1021 00:13:34.717000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:13:34.753000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:34 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1021 00:13:34.801000 290 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:34 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1021 00:13:34.819000 293 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank6]:W1021 00:13:34.863000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:13:34.953000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:13:35.029000 288 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank6]:W1021 00:13:35.070000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:35 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1021 00:13:35.227000 292 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank7]:W1021 00:13:35.293000 295 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank1]:W1021 00:13:35.341000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:13:35.372000 291 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank1]:W1021 00:13:35.433000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:13:35.558000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:13:35 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1021 00:13:35.674000 294 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
[rank1]:W1021 00:13:36.135000 289 torch/_inductor/utils.py:1349] [28/0] Please pip install Composable Kernel package
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_3 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_7 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_9 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_1 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_2 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.4238 seconds and 0.5223 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_15 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_7 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_1 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_5 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_13 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_3 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_2 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4502 seconds and 0.4803 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  bmm 0.0065 ms 100.0% 
  triton_bmm_5 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_6 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_9 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_2 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_3 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3794 seconds and 0.4964 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_13 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_1 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_2 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_6 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_11 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3382 seconds and 0.4863 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_13 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_15 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_7 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0065 ms 97.6% 
  triton_bmm_5 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_11 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_19 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_1 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_2 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4179 seconds and 0.4823 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_15 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_3 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_6 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_13 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0064 ms 98.1% 
  triton_bmm_7 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_11 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_9 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4664 seconds and 0.4879 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_11 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_3 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_5 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_6 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_13 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_2 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_15 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4260 seconds and 0.4757 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x128, 16x128x512)
  triton_bmm_3 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0064 ms 99.4% 
  triton_bmm_11 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_23 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_1 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_2 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_5 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_6 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_7 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_13 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.5338 seconds and 0.4733 seconds precompiling for 27 choices
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1021 00:13:46.276000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:13:46.301000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:13:46.358000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin posix.stat. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1021 00:13:46.785000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:13:46.813000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:13:46.862000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:13:47.081000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:13:47.090000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:13:47.222000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:13:47.643000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:13:47.647000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:13:47.731000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:13:47.861000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:13:48.257000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:13:50.749000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:13:52.337000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/0_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1021 00:13:56.964000 288 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
[rank2]:W1021 00:13:57.044000 290 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
[rank3]:W1021 00:13:57.392000 291 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1021 00:13:58.683000 293 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
[rank1]:W1021 00:13:59.552000 289 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank4]:W1021 00:14:02.462000 292 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
[rank7]:W1021 00:14:02.599000 295 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.mla_decode_stage1_asm_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1021 00:14:03.434000 294 torch/_inductor/utils.py:1349] [40/0_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0068 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0070 ms 96.6% 
  triton_bmm_41 0.0074 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 85.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0085 ms 79.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0085 ms 79.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0087 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0088 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0094 ms 72.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_49 0.0095 ms 71.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 7.9536 seconds and 0.5441 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0071 ms 95.5% 
  triton_bmm_41 0.0075 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 85.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0084 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0085 ms 79.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0090 ms 75.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0092 ms 73.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0095 ms 70.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_32 0.0095 ms 70.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.2856 seconds and 0.5344 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0067 ms 100.0% 
  triton_bmm_41 0.0073 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0079 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0083 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0085 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0085 ms 78.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0087 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0093 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_49 0.0093 ms 72.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.1008 seconds and 0.5341 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  bmm 0.0067 ms 100.0% 
  triton_bmm_29 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_41 0.0073 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0080 ms 83.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0081 ms 82.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0085 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0087 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0089 ms 74.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0092 ms 72.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_51 0.0093 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 7.9348 seconds and 0.5207 seconds precompiling for 27 choices
[rank0]:W1021 00:14:06.657000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:06.693000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:06.733000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0071 ms 94.3% 
  triton_bmm_41 0.0073 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0077 ms 86.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0083 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0084 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0087 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_48 0.0088 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0090 ms 74.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_51 0.0094 ms 71.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 6.6039 seconds and 0.5216 seconds precompiling for 27 choices
[rank5]:W1021 00:14:06.769000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:06.835000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:06.870000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:06 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:06 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1021 00:14:06.944000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:07.021000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:07.120000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:07 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1021 00:14:07.300000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:07.377000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:07.478000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:07 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1021 00:14:08.311000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x32x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_29 0.0067 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_41 0.0072 ms 91.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0076 ms 87.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0082 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0085 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0088 ms 75.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0088 ms 75.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0088 ms 75.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_31 0.0094 ms 70.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2989 seconds and 0.5276 seconds precompiling for 27 choices
[rank1]:W1021 00:14:08.389000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:08.488000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:08 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x32x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_29 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_41 0.0072 ms 89.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0076 ms 85.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0084 ms 77.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0086 ms 75.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_37 0.0088 ms 73.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_46 0.0091 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_49 0.0092 ms 70.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4104 seconds and 0.5283 seconds precompiling for 27 choices
AUTOTUNE bmm(16x32x512, 16x512x128)
  triton_bmm_29 0.0067 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0067 ms 99.4% 
  triton_bmm_41 0.0074 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_28 0.0078 ms 85.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_39 0.0083 ms 80.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_33 0.0085 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_37 0.0087 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_48 0.0088 ms 76.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_46 0.0088 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_49 0.0094 ms 71.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3081 seconds and 0.5279 seconds precompiling for 27 choices
[rank4]:W1021 00:14:09.817000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:09.894000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:09.996000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:10 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1021 00:14:10.064000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:10.140000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:10.241000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:10 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1021 00:14:10.484000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:10.562000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:10.667000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/2] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:10 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:11 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:11 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:11 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:12 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1021 00:14:12.914000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:13.001000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:13 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1021 00:14:13.111000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:13.136000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:13 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1021 00:14:13.197000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:13.211000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:13.273000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:13.320000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:13 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1021 00:14:13.380000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:13 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1021 00:14:13.647000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:13.722000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:13.830000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:13 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:13 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1021 00:14:14.818000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:14.902000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:14 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1021 00:14:15.007000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:15 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1021 00:14:15.129000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:15.205000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:15.313000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:15 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:15 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1021 00:14:16.061000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:16.135000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:16.242000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:16 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1021 00:14:16.649000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:16.725000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:16.834000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/3] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:16 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1021 00:14:17.938000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:18.160000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:18.171000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:18.222000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:18.395000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:18.406000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:18.465000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:18 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1021 00:14:18.631000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:18.687000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:18 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:18 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1021 00:14:18.848000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:19.087000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:19.318000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:19 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1021 00:14:19.846000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:19.908000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:19.959000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:19.983000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:20.036000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:20.050000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:20.082000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:20.091000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:20.099000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:20.127000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:20 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1021 00:14:20.137000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:20 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1021 00:14:20.229000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:20 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1021 00:14:20.330000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:20.343000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:20 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1021 00:14:20.628000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:20 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1021 00:14:20.883000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:20.970000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:21.040000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:21.072000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:21 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank7]:W1021 00:14:21.319000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:21.556000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:21 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1021 00:14:21.788000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:21.939000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:22.014000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:22.028000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:22.067000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:22.115000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:22.144000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:22 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1021 00:14:22.244000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:22 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1021 00:14:22.311000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:22 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1021 00:14:23.122000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:23.198000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:23.297000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:23 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank6]:W1021 00:14:23.609000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:23.688000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:23.799000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/4] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:14:23 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.biased_grouped_topk. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_55 0.0284 ms 34.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0394 ms 24.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0450 ms 21.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0486 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0575 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0672 ms 14.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0679 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0685 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0702 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4951 seconds and 0.3947 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0310 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0413 ms 23.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0450 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0520 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0629 ms 15.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_74 0.0698 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0704 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0706 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_75 0.0717 ms 13.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6499 seconds and 0.3802 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_55 0.0287 ms 33.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0399 ms 24.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0455 ms 21.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0490 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0585 ms 16.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0681 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0685 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0693 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0710 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5922 seconds and 0.3907 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0332 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0395 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0452 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0489 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0623 ms 15.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_77 0.0692 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0693 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0703 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_75 0.0719 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4429 seconds and 0.3816 seconds precompiling for 27 choices
[rank0]:W1021 00:14:27.970000 288 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26ec40>
[rank3]:W1021 00:14:27.985000 291 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4639884ed0>
[rank0]:W1021 00:14:28.002000 288 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26de00>
[rank3]:W1021 00:14:28.017000 291 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f46398855c0>
[rank5]:W1021 00:14:28.056000 293 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568dbf0>
[rank5]:W1021 00:14:28.088000 293 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568db90>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:14:28 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:14:28 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:14:28 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_55 0.0285 ms 33.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0401 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0451 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0491 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0584 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0684 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0685 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0687 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0706 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5726 seconds and 0.3892 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0290 ms 32.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0394 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0451 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0486 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0575 ms 16.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0679 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0680 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0687 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0704 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6123 seconds and 0.3825 seconds precompiling for 27 choices
[rank2]:W1021 00:14:28.815000 290 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d560>
[rank2]:W1021 00:14:28.859000 290 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d4d0>
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:14:29 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1021 00:14:29.530000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0288 ms 33.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0396 ms 24.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0450 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0489 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0581 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0675 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_74 0.0681 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0682 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_66 0.0704 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5739 seconds and 0.3818 seconds precompiling for 27 choices
[rank5]:W1021 00:14:29.751000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:29.766000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:29.930000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:29.987000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:30.005000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:30.120000 292 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5ad0>
[rank4]:W1021 00:14:30.151000 292 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5680>
[rank3]:W1021 00:14:30.166000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:30.223000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_55 0.0298 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_67 0.0395 ms 24.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_54 0.0452 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_65 0.0489 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_59 0.0584 ms 16.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_58 0.0681 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_77 0.0683 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_74 0.0689 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_66 0.0704 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6193 seconds and 0.3996 seconds precompiling for 27 choices
[rank0]:W1021 00:14:30.245000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:14:30 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1021 00:14:30.333000 289 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38790717a0>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1021 00:14:30.374000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:30.403000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:30.416000 289 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3879071da0>
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1021 00:14:30.459000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:30.486000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:14:30 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1021 00:14:30.611000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:30.643000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1021 00:14:30.695000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:30.727000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:30.851000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:30.891000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:30.930000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:30.975000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:30.981000 295 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b9542b80>
[rank7]:W1021 00:14:31.012000 295 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b95414a0>
[rank2]:W1021 00:14:31.087000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:31.166000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:31.183000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:14:31 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank0]:W1021 00:14:31.214000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1021 00:14:31.323000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:31.402000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:31.421000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:31.456000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:31.568000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:31.640000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:31.671000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:31.754000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:31.803000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:31.874000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:31.911000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:31.994000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:32.038000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:32.124000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:32.147000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:32.171000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:32.234000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:32.317000 294 torch/_dynamo/variables/builtin.py:1091] [72/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d9b0>
[rank2]:W1021 00:14:32.330000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:32.346000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:32.348000 294 torch/_dynamo/variables/builtin.py:1091] [73/0] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d950>
[rank4]:W1021 00:14:32.363000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:32.386000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:32.411000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:14:32 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1021 00:14:32.473000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:32.527000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.moe_sorting_fwd. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank2]:W1021 00:14:32.566000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:32.582000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:32.603000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:32.622000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py:679: UserWarning: Graph break due to unsupported builtin aiter.jit.aiter_.PyCapsule.fmoe_fp8_blockscale_g1u1. This function is either a Python builtin (e.g. _warnings.warn) or a third-party C/C++ Python extension (perhaps created with pybind). If it is a Python builtin, please file an issue on GitHub so the PyTorch team can add support for it and see the next case for a workaround. If it is a third-party C/C++ Python extension, please either wrap it into a PyTorch-understood custom operator (see https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html for more details) or, if it is traceable, use torch.compiler.allow_in_graph.
  torch._dynamo.utils.warn_once(msg)
[rank5]:W1021 00:14:32.651000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:32.709000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:32.767000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:32.802000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:32.818000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:32.843000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:32.858000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:32.891000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:32.945000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:33.002000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:33.039000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:33.055000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:33.082000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:33.096000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:33.127000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:33.183000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:33.238000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:33.275000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:33.291000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:33.318000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:33.332000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:33.363000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:33.422000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:33.475000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:33.519000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:33.535000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:33.554000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:33.575000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:33.598000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:33.658000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:33.714000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:33.751000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:33.771000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:33.790000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:33.811000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:33.835000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:33.895000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:33.951000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:33.987000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:34.032000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:34.050000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:34.062000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:34.076000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:34.134000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:34.192000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:34.222000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:34.271000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:34.286000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:34.302000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:34.308000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:34.315000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:34.370000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:34.432000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:34.458000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:34.542000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:34.551000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:34.557000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:34.571000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:34.586000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:34.610000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:34.695000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:34.727000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:34.783000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:34.790000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:34.795000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:34.810000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:34.831000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:34.846000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:34.931000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:34.966000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:35.019000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:35.030000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:35.035000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:35.050000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:35.071000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:35.082000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:35.167000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:35.206000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:35.255000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:35.271000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:35.275000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:35.290000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:35.311000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:35.386000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:35.407000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:35.446000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:35.495000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:35.511000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:35.515000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:35.526000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:35.551000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:35.627000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:35.643000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:35.687000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:35.731000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:35.755000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:35.766000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:35.787000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:35.823000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:35.866000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:35.927000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:35.947000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:35.983000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:35.995000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:36.002000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:36.039000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:36.063000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:36.110000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:36.162000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:36.183000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:36.267000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:36.302000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:36.419000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:36.430000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:36.469000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:36.478000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:36.506000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:36.542000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:36.563000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:36.583000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:36.663000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:36.669000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:36.711000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:36.719000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:36.743000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:36.783000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:36.806000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:36.822000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:36.902000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:36.912000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:36.950000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:36.983000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:37.019000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:37.025000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:37.045000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:37.058000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:37.144000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:37.155000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:37.191000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:37.223000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:37.263000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:37.267000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:37.286000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:37.295000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:37.383000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:37.399000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:37.432000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:37.463000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:37.506000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:37.511000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:37.530000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:37.547000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:37.640000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:37.797000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:37.879000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:38.018000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:38.051000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:38.112000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:38.155000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:38.276000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:38.278000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:38.295000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:38.359000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:38.367000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:38.383000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:38.392000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:38.464000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:38.523000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:38.544000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:38.604000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:38.632000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:38.639000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:38.641000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:38.679000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:38.703000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:38.762000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:38.788000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:38.873000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:38.879000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:38.888000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:38.899000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:38.926000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:38.947000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:39.004000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:39.039000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:39.131000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:39.138000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:39.149000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:39.162000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:39.176000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:39.186000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:39.243000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:39.290000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:39.389000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:39.399000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:39.427000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:39.431000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:39.435000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:39.482000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:39.491000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:39.534000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:39.656000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:39.667000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:39.683000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:39.683000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:39.695000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:39.722000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:39.755000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:39.775000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:39.910000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:39.917000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:39.922000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:39.930000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:39.943000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:40.019000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:40.024000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:40.034000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:40.164000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:40.168000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:40.178000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:40.192000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:40.258000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:40.285000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:40.287000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:40.295000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:40.420000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:40.422000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:40.424000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:40.447000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:40.498000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:40.534000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:40.541000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:40.551000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:40.663000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:40.668000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:40.674000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:40.780000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:40.796000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:40.797000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:40.815000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:40.819000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:40.903000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:40.907000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:40.930000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:41.022000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:41.050000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:41.059000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:41.061000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:41.071000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:41.143000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:41.147000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:41.180000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:41.266000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:41.302000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:41.307000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:41.317000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:41.327000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:41.383000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:41.387000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:41.427000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:41.506000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:41.550000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:41.560000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:41.585000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:41.605000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:41.622000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:41.627000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:41.681000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:41.746000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:41.798000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:41.818000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:41.831000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:41.860000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:41.865000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:41.869000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:41.931000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:41.986000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:42.059000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:42.071000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:42.083000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:42.103000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:42.115000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:42.117000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:42.196000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:42.227000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:42.310000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:42.324000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:42.333000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:42.354000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:42.371000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:42.415000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:42.444000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:42.467000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:42.555000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:42.584000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:42.590000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:42.655000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:42.659000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:42.681000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:42.706000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:42.780000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:42.803000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:42.835000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:42.842000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:42.899000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:42.906000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:42.924000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:42.948000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:43.048000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:43.049000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:43.095000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:43.098000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:43.139000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:43.163000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:43.168000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:43.186000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:43.291000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:43.312000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:43.357000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:43.368000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:43.378000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:43.410000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:43.426000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:43.446000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:43.545000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/40_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:43.576000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:43.607000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:43.618000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:43.630000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:43.650000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:14:43.666000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:43.708000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:43.786000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/41_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:43.852000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:43.858000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:43.890000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:43.968000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:43.969000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:43.970000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:44.031000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/42_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:44.100000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:44.115000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:44.130000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:44.216000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:44.228000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:44.235000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:44.275000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/43_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:44.339000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:44.375000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:44.393000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:44.502000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:44.522000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/44_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:44.523000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:14:44.526000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:44.583000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:44.618000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:44.660000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:44.789000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:44.792000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:44.829000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:44.839000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/45_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:44.863000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:44.915000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:14:45.048000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:45.052000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:45.071000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:45.083000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/46_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:45.103000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:45.167000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:45.311000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:45.326000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/47_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:45.346000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:45.420000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:14:45.423000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:45.554000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:45.575000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/48_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:45.598000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:45.699000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:45.799000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:45.823000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/49_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:45.842000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:45.973000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:46.043000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:46.071000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/50_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:46.087000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:46.238000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:14:46.287000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:46.315000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/51_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:14:46.331000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:46.505000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:46.558000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/52_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:46.760000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:46.807000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/53_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:47.039000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:47.064000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/54_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:47.316000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/55_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:47.422000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:47.558000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/56_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:14:47.728000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:47.802000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/57_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:48.042000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/58_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:48.286000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/59_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:14:48.540000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/60_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_81 0.1028 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1052 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1122 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_100 0.1129 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1130 ms 53.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_91 0.1142 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1173 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1563 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1565 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.4913 seconds and 0.3501 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_81 0.1035 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1053 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1097 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_100 0.1134 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1135 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_91 0.1145 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1155 ms 52.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1540 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1578 ms 38.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.3146 seconds and 0.3403 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_81 0.1034 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1051 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1104 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_100 0.1128 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1137 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_91 0.1140 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1165 ms 52.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1521 ms 39.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1565 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 6.3302 seconds and 0.3454 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_81 0.1031 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1055 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1094 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1136 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1140 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1145 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1192 ms 50.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1571 ms 38.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1572 ms 38.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 7.8951 seconds and 0.3420 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0603 ms 100.0% 
  triton_mm_81 0.1031 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1055 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1102 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1136 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1136 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1150 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1160 ms 52.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1539 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1575 ms 38.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 9.0022 seconds and 0.3370 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0596 ms 100.0% 
  triton_mm_81 0.1003 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1024 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1078 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1100 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_101 0.1101 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1103 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_92 0.1152 ms 51.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_99 0.1460 ms 40.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_98 0.1460 ms 40.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 8.9745 seconds and 0.3377 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_81 0.1032 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1055 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1124 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1148 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_91 0.1151 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_100 0.1151 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_92 0.1190 ms 51.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1517 ms 40.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1567 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.8844 seconds and 0.3514 seconds precompiling for 27 choices
AUTOTUNE mm(32x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_81 0.1033 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_93 0.1053 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_80 0.1118 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_101 0.1147 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_100 0.1153 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_91 0.1154 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_92 0.1179 ms 51.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_90 0.1558 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_99 0.1566 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 7.3874 seconds and 0.3393 seconds precompiling for 27 choices
Capturing batches (bs=32 avail_mem=42.10 GB):  87%| | 45/52 [02:19<03:17, 28.26s/it]Capturing batches (bs=24 avail_mem=41.50 GB):  87%| | 45/52 [02:19<03:17, 28.26s/it][rank4]:W1021 00:15:02.620000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:02.697000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:02.717000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:02.794000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:02.806000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:02.903000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:02.922000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:02.998000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:03.034000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:03.104000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:03.109000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:03.214000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:03.262000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:03.337000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:03.441000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:03.445000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:03.521000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:03.629000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:03.701000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:03.776000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:03.886000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:03.893000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:03.962000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:04.068000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/5] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:04.374000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:04.447000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:04.678000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:04.774000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:05.006000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:05.198000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:05.446000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:05.610000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:06.302000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:06.378000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:06.452000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:06.481000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:06.493000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:06.530000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:06.570000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:06.633000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:06.660000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:06.672000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:06.739000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:06.842000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:06.872000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:06.949000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:06.960000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:06.963000 289 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank2]:W1021 00:15:07.038000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:07.053000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:07.107000 292 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank3]:W1021 00:15:07.124000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:07.142000 295 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank2]:W1021 00:15:07.145000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:07.213000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:07.218000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:07.323000 294 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank0]:W1021 00:15:07.328000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:07.355000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:07.443000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/6] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:07.538000 293 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank2]:W1021 00:15:07.660000 290 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank3]:W1021 00:15:07.847000 291 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
[rank0]:W1021 00:15:07.933000 288 torch/_inductor/utils.py:1349] [28/1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_111 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0064 ms 99.4% 
  triton_bmm_110 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_115 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_104 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_105 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_109 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_123 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2802 seconds and 0.3817 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_115 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_126 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_109 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_120 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_129 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_106 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2898 seconds and 0.4100 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_105 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_111 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_109 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_110 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_115 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_126 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_129 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.3335 seconds and 0.3797 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_107 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_109 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_117 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_126 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_105 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_110 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_106 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_120 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.2542 seconds and 0.3827 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_111 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_107 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_110 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_115 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_119 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_109 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_105 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 96.9% 
  triton_bmm_106 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3014 seconds and 0.3779 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_111 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_110 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_119 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_121 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0064 ms 98.1% 
  triton_bmm_107 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_129 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_105 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_106 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3074 seconds and 0.3828 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  triton_bmm_109 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_110 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_107 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_115 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0064 ms 98.1% 
  triton_bmm_111 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_129 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_117 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_105 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.3652 seconds and 0.3766 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_105 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_106 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_107 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_109 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_110 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_115 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_117 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_119 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_111 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.3124 seconds and 0.3735 seconds precompiling for 27 choices
[rank1]:W1021 00:15:18.226000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:18.617000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:18.652000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:18.726000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:18.745000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:19.129000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:19.161000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:19.171000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:19.251000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:19.255000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:19.330000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:19.678000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/1_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:19.687000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:19.763000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:19.834000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:20.193000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/61_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:21.086000 289 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank6]:W1021 00:15:21.242000 294 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank0]:W1021 00:15:21.786000 288 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank2]:W1021 00:15:21.854000 290 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank5]:W1021 00:15:21.923000 293 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank3]:W1021 00:15:22.851000 291 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank7]:W1021 00:15:23.174000 295 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
[rank4]:W1021 00:15:23.816000 292 torch/_inductor/utils.py:1349] [40/1_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0077 ms 84.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0082 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0083 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0085 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0088 ms 74.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0088 ms 74.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0092 ms 70.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1871 seconds and 0.5340 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0077 ms 83.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0085 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_141 0.0087 ms 74.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0090 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_152 0.0090 ms 72.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_155 0.0093 ms 70.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1895 seconds and 0.5330 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_133 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0079 ms 81.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0081 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0086 ms 74.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_141 0.0089 ms 71.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0091 ms 70.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0093 ms 69.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_155 0.0094 ms 68.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1330 seconds and 0.5346 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_133 0.0066 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0076 ms 86.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0085 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0088 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_152 0.0088 ms 74.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0090 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_155 0.0093 ms 70.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2176 seconds and 0.5217 seconds precompiling for 27 choices
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_133 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 87.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0077 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0082 ms 78.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0084 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_150 0.0090 ms 71.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_151 0.0095 ms 67.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_155 0.0095 ms 67.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2000 seconds and 0.5258 seconds precompiling for 27 choices
[rank1]:W1021 00:15:27.759000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:27.835000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:27.885000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:27.928000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:28.004000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:28.054000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:28.605000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0066 ms 100.0% 
  triton_bmm_133 0.0067 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 90.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0077 ms 85.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_150 0.0084 ms 78.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_137 0.0085 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0087 ms 75.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0090 ms 73.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_155 0.0091 ms 71.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2103 seconds and 0.5332 seconds precompiling for 27 choices
[rank5]:W1021 00:15:28.683000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:28.734000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:28.868000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:28.945000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:28.996000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:29.023000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0072 ms 91.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0076 ms 86.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0081 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0083 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0085 ms 76.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0086 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0088 ms 74.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0090 ms 72.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.2449 seconds and 0.5444 seconds precompiling for 27 choices
[rank2]:W1021 00:15:29.101000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:29.151000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:29.559000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x24x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_133 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_145 0.0073 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_132 0.0076 ms 84.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_143 0.0083 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_137 0.0083 ms 77.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_150 0.0085 ms 75.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_141 0.0087 ms 74.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_152 0.0089 ms 72.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_135 0.0091 ms 71.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1980 seconds and 0.5229 seconds precompiling for 27 choices
[rank3]:W1021 00:15:29.636000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:29.697000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:29.924000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:30.000000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:30.050000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:30.498000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:30.576000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:30.625000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/7] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:33.885000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:33.961000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:34.079000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:34.082000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:34.157000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:34.267000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:34.737000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:34.815000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:34.924000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:35.035000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:35.111000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:35.115000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:35.191000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:35.221000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:35.298000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:36.270000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:36.304000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:36.346000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:36.381000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:36.453000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:36.488000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:36.775000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:36.854000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/8_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:36.966000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/8] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:38.242000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:38.315000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:38.487000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:38.560000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:38.734000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:38.807000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:39.382000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:39.626000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:39.707000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:39.871000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:39.952000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:40.137000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:40.165000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:40.196000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:40.234000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:40.243000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:40.312000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:40.345000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:40.382000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:40.415000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:40.454000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:40.536000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:40.622000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:40.699000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:40.788000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:40.951000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:41.037000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:41.462000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:41.539000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:41.605000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:41.639000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:41.681000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:41.731000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/62_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:41.782000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:41.975000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/63_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:42.208000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:42.219000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/64_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:42.287000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:42.382000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:42.393000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:42.459000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:42.563000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:42.755000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:42.833000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:42.940000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:43.640000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:43.718000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/9_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:43.820000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/9] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0285 ms 32.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0400 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0451 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0486 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0586 ms 16.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0679 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0685 ms 13.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0691 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.4872 seconds and 0.3771 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0301 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0401 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0451 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0486 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0594 ms 15.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0680 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_178 0.0693 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_162 0.0695 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6214 seconds and 0.3799 seconds precompiling for 27 choices
[rank6]:W1021 00:15:47.982000 294 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d9b0>
[rank1]:W1021 00:15:48.042000 289 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38790717a0>
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0299 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0405 ms 23.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0455 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0491 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0596 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0682 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0694 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0696 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0711 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6403 seconds and 0.3783 seconds precompiling for 27 choices
[rank1]:W1021 00:15:48.065000 289 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3879071da0>
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0308 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0415 ms 22.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0453 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0526 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0662 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_178 0.0701 ms 13.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0722 ms 13.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0727 ms 12.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.5610 seconds and 0.3816 seconds precompiling for 27 choices
[rank6]:W1021 00:15:48.130000 294 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d950>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:15:48 TP1] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:15:48 TP6] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_159 0.0301 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0401 ms 23.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0452 ms 21.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0486 ms 19.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0621 ms 15.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0678 ms 14.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_178 0.0692 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0704 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_179 0.0708 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5258 seconds and 0.3837 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_159 0.0285 ms 33.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0400 ms 23.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0451 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0485 ms 19.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0586 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0675 ms 14.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0686 ms 14.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0689 ms 13.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0704 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6321 seconds and 0.3923 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_159 0.0282 ms 33.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0401 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0452 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0487 ms 19.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0595 ms 15.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0681 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0689 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0693 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0705 ms 13.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5554 seconds and 0.3840 seconds precompiling for 27 choices
[rank1]:W1021 00:15:49.395000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:49.480000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:49.517000 288 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26ec40>
[rank0]:W1021 00:15:49.540000 288 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26de00>
[rank5]:W1021 00:15:49.551000 293 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568dbf0>
[rank5]:W1021 00:15:49.573000 293 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568db90>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:15:49 TP0] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1021 00:15:49.639000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:15:49 TP5] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1021 00:15:49.731000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:49.883000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:49.979000 290 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d560>
[rank6]:W1021 00:15:49.980000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:50.001000 290 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d4d0>
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:15:50 TP2] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1021 00:15:50.130000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_159 0.0281 ms 33.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_171 0.0400 ms 23.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_158 0.0451 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_169 0.0486 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_163 0.0595 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_181 0.0678 ms 13.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_162 0.0685 ms 13.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_178 0.0694 ms 13.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_170 0.0706 ms 13.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5433 seconds and 0.3785 seconds precompiling for 27 choices
[rank6]:W1021 00:15:50.228000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:50.292000 291 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4639884ed0>
[rank3]:W1021 00:15:50.316000 291 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f46398855c0>
[rank1]:W1021 00:15:50.374000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:15:50 TP3] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1021 00:15:50.475000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:50.568000 295 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b9542b80>
[rank7]:W1021 00:15:50.591000 295 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b95414a0>
[rank1]:W1021 00:15:50.622000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:15:50 TP7] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1021 00:15:50.723000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:50.865000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:50.971000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:51.043000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:51.062000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:51.112000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:51.219000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:51.287000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:51.307000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:51.356000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:51.468000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:51.490000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:51.530000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:51.558000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:51.598000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:51.667000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:51.720000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:51.728000 292 torch/_dynamo/variables/builtin.py:1091] [72/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5ad0>
[rank2]:W1021 00:15:51.738000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:51.782000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:51.804000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:51.854000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:51.899000 292 torch/_dynamo/variables/builtin.py:1091] [73/1] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5680>
[rank3]:W1021 00:15:51.918000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:15:51 TP4] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1021 00:15:51.972000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:51.983000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:52.026000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:52.052000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:52.102000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:52.166000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:52.225000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:52.229000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:52.270000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:52.303000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:52.353000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:52.396000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:52.410000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:52.475000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:52.482000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:52.515000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:52.551000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:52.599000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:52.647000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:52.655000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:52.719000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:52.731000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:52.759000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:52.795000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:52.843000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:52.892000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:52.898000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:52.959000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:52.990000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:53.002000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:53.039000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:53.087000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:53.140000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:53.145000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:53.198000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:53.240000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:53.246000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:53.285000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:53.330000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:53.384000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:53.390000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:53.438000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:53.487000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:53.492000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:53.527000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:53.583000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:53.631000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:53.648000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:53.691000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/65_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:53.697000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:53.735000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:53.741000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:53.767000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:53.875000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:53.895000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:53.940000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/66_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:53.944000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:53.954000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:53.983000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:53.992000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:54.008000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:54.120000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:54.138000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:54.183000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/67_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:54.194000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:54.210000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:54.240000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:54.244000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:54.253000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:54.364000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:54.382000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:54.431000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/68_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:54.436000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:54.454000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:54.489000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:54.493000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:54.500000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:54.608000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:54.626000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:54.675000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/69_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:54.680000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:54.698000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:54.740000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:54.744000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:54.751000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:54.852000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:54.870000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:54.920000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/70_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:54.924000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:54.942000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:54.990000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:54.997000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:55.096000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:55.115000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:55.140000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:55.164000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/71_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:55.169000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:55.191000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:55.248000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:55.339000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:55.366000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:55.371000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:55.392000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:55.407000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/72_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:55.414000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:55.438000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:55.492000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:55.587000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:55.614000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:55.619000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:55.644000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:55.651000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/73_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:55.658000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:55.683000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:55.736000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:55.831000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:55.870000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:55.878000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:55.896000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:55.901000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/74_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:55.910000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:55.930000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:55.980000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:56.075000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:56.114000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:56.126000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:56.148000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/75_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:56.155000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:56.159000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:56.174000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:56.224000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:56.320000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:56.378000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:56.392000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/76_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:56.402000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:56.410000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:56.426000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:56.496000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:56.567000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:56.607000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:56.635000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/77_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:56.643000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:56.663000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:56.687000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:56.748000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:56.792000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:56.811000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:56.851000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:56.879000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/78_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:56.895000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:56.915000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:56.935000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:56.995000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:57.035000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:57.060000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:57.096000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:57.124000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/79_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:57.138000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:57.168000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:57.183000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:57.238000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:57.278000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:57.304000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:57.339000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:57.367000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/80_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:57.386000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:57.420000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:57.430000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:57.486000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:57.522000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:57.552000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:57.583000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:57.611000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/81_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:57.635000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:57.671000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:57.679000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:57.736000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:57.768000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:57.831000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:57.859000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/82_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:57.887000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:57.923000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:57.928000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:57.947000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:57.984000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:58.012000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:58.079000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:58.103000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/83_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:58.135000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:58.175000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:58.181000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:58.195000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:58.240000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:58.270000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:58.327000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:58.347000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/84_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:58.387000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:58.431000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:58.436000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:58.444000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:58.496000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:58.524000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:58.575000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:58.591000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/85_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:58.643000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:58.684000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:58.689000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:58.696000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:58.740000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:58.768000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:58.823000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:58.839000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/86_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:58.891000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:58.935000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:58.941000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:58.947000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:58.984000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:59.012000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:59.071000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:59.083000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/87_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:59.139000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:59.188000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:59.193000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:59.200000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:59.228000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:59.256000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:59.319000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:59.387000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:59.440000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:59.446000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:59.451000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:59.471000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:59.479000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/88_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:59.499000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:59.567000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:59.634000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:59.683000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:59.700000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:59.706000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:59.714000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:59.728000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/89_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:59.742000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:15:59.812000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:15:59.882000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:15:59.930000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:15:59.948000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:15:59.960000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:15:59.966000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:15:59.978000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/90_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:15:59.987000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:00.059000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:00.143000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:00.191000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:00.197000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:00.211000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:00.223000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:00.230000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/91_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:00.236000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:00.303000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:00.396000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:00.447000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:00.463000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:00.477000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:00.482000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/92_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:00.488000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:00.551000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:00.587000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:00.647000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:00.695000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:00.715000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:00.728000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:00.734000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/93_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:00.740000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:00.795000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:00.839000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:00.895000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:00.943000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:00.967000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:00.977000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:00.983000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:00.989000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/94_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:01.043000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:01.087000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:01.147000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:01.195000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:01.223000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:01.230000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:01.237000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:01.243000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/95_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:01.291000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:01.335000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:01.395000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:01.443000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:01.479000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:01.485000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:01.492000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:01.498000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/96_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:01.539000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:01.595000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:01.643000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:01.695000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:01.732000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:01.739000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:01.747000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/97_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:01.787000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:01.848000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:01.911000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:01.947000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:01.994000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/98_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:02.001000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:02.007000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:02.035000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:02.051000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:02.112000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:02.167000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:02.195000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:02.246000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/99_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:02.254000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:02.261000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:02.283000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:02.299000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:02.359000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:02.423000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:02.443000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:02.494000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/100_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:02.500000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:02.508000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:02.531000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:02.547000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:02.608000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:02.679000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:02.691000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:02.742000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/101_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:02.749000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:02.756000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:02.779000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:02.795000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:02.856000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:02.936000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:02.941000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:02.991000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/102_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:03.000000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:03.047000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:03.111000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:03.156000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:03.188000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:03.195000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:03.201000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:03.240000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/103_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:03.302000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:03.366000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:03.395000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:03.411000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:03.440000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:03.447000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:03.456000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:03.492000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/104_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:03.554000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:03.618000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:03.642000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:03.663000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:03.691000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:03.697000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:03.716000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:03.740000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/105_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:03.806000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:03.899000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:03.923000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:03.944000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:03.951000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:03.972000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:03.988000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/106_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:04.058000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:04.147000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:04.176000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:04.199000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:04.207000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:04.236000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/107_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:04.315000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:04.407000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:04.439000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:04.457000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:04.493000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/108_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:04.571000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:04.672000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:04.677000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:04.707000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:04.714000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:04.744000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/109_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:04.830000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:04.924000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:04.931000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:04.964000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:04.969000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:04.996000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/110_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:05.086000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:05.176000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:05.180000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:05.211000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:05.222000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:05.243000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/111_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:05.338000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:05.426000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:05.439000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:05.470000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:05.491000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/112_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:05.674000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:05.692000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:05.723000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:05.740000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/113_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:05.949000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:05.986000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:06.207000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:06.234000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/114_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:06.464000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:06.487000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/115_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:06.720000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:06.739000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/116_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:06.991000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/117_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:07.243000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/118_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:07.501000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/119_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:07.755000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/120_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:08.013000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/121_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_185 0.1024 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1053 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1104 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1134 ms 53.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1140 ms 53.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1144 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1182 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_194 0.1551 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_203 0.1557 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6027 seconds and 0.3780 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_185 0.1023 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1053 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1104 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1133 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1140 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1148 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1189 ms 51.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_194 0.1552 ms 39.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_203 0.1557 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6163 seconds and 0.3746 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_185 0.1026 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1053 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1098 ms 55.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1136 ms 53.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1144 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1146 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1175 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_194 0.1551 ms 39.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_203 0.1556 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6009 seconds and 0.3839 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_185 0.1023 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1057 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1080 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1135 ms 53.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1144 ms 53.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1146 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1161 ms 52.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_194 0.1529 ms 39.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_203 0.1568 ms 38.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5853 seconds and 0.3541 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0598 ms 100.0% 
  triton_mm_185 0.0993 ms 60.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1024 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1073 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1094 ms 54.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1099 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1104 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1149 ms 52.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1453 ms 41.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_202 0.1455 ms 41.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6112 seconds and 0.3872 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_185 0.1024 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1054 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1100 ms 54.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1136 ms 53.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1143 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1149 ms 52.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1186 ms 50.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_194 0.1552 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_203 0.1556 ms 38.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.6663 seconds and 0.3500 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_185 0.1025 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1055 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1087 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_205 0.1142 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1145 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1148 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_196 0.1171 ms 51.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_194 0.1537 ms 39.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_mm_203 0.1569 ms 38.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 5.5548 seconds and 0.3458 seconds precompiling for 27 choices
AUTOTUNE mm(24x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_185 0.1025 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_197 0.1054 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_184 0.1100 ms 55.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_195 0.1148 ms 52.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_mm_205 0.1152 ms 52.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_204 0.1156 ms 52.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_196 0.1183 ms 51.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_203 0.1559 ms 39.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_194 0.1561 ms 38.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=32, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.6306 seconds and 0.3522 seconds precompiling for 27 choices
Capturing batches (bs=24 avail_mem=41.50 GB):  88%| | 46/52 [03:39<04:21, 43.66s/it]Capturing batches (bs=16 avail_mem=40.89 GB):  88%| | 46/52 [03:39<04:21, 43.66s/it][rank1]:W1021 00:16:22.099000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:22.176000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:22.186000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:22.254000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:22.293000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:22.359000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:22.363000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:22.436000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:22.542000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:22.743000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:22.820000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:22.928000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:22.937000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:22.985000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:23.014000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:23.058000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:23.064000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:23.120000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:23.134000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:23.175000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:23.241000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:23.286000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:23.364000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/10_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:23.471000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/10] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:23.870000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:23.960000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:24.121000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:24.512000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:24.709000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:24.781000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:24.823000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:25.042000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:25.488000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:25.511000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:25.566000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:25.601000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:25.669000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:25.705000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:25 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:25 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1021 00:16:25.904000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:25.980000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:26.080000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:26.133000 292 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:26 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1021 00:16:26.168000 291 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank6]:W1021 00:16:26.311000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:26.398000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:26.477000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:26.495000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:26.508000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:26.557000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:26 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1021 00:16:26.566000 289 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank2]:W1021 00:16:26.570000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:26.665000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:26.674000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:26 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:26 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1021 00:16:26.993000 294 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank7]:W1021 00:16:27.137000 295 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank2]:W1021 00:16:27.137000 290 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank5]:W1021 00:16:27.158000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:27.242000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:27.267000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:27.352000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:27.353000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/11_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:27 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1021 00:16:27.458000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/11] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:27 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank5]:W1021 00:16:27.817000 293 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
[rank0]:W1021 00:16:27.916000 288 torch/_inductor/utils.py:1349] [28/2] Please pip install Composable Kernel package
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_213 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_219 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_227 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_231 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_221 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_229 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_223 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_225 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_220 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_226 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8451 seconds and 0.1588 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_219 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_212 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_227 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_221 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 98.7% 
  triton_bmm_229 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_214 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_222 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_209 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_218 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8074 seconds and 0.1583 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_228 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_226 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_227 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0064 ms 99.4% 
  triton_bmm_224 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_231 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_213 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_225 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_211 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_212 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8309 seconds and 0.1559 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_215 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_209 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_219 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_221 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_229 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_231 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_213 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_211 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_220 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_225 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8084 seconds and 0.1568 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_225 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_211 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_222 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_224 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_226 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_231 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_214 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_215 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_212 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8411 seconds and 0.1616 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_228 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_213 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_225 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_226 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_230 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_211 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_212 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_214 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_215 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8486 seconds and 0.1614 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  triton_bmm_227 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_220 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_213 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_214 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_219 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_230 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_231 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0064 ms 97.5% 
  triton_bmm_211 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_212 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7804 seconds and 0.1549 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_208 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_211 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_215 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_219 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_209 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_213 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_218 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_210 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_212 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8329 seconds and 0.1535 seconds precompiling for 25 choices
[rank4]:W1021 00:16:36.460000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:36.965000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:37.122000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:37.184000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:37.522000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:37.631000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:37.685000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:37.702000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:38.029000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:38.195000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:38.740000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:38.988000 292 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank2]:W1021 00:16:39.243000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:39.262000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:39.633000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/2_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:39.765000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:40.003000 294 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank0]:W1021 00:16:40.149000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/122_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:40.257000 291 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank1]:W1021 00:16:40.479000 289 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank7]:W1021 00:16:40.929000 295 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank2]:W1021 00:16:41.731000 290 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank5]:W1021 00:16:42.057000 293 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
[rank0]:W1021 00:16:42.667000 288 torch/_inductor/utils.py:1349] [40/2_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_234 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_245 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0071 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0072 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_239 0.0079 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0080 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_254 0.0083 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0505 seconds and 0.4337 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_234 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_245 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0069 ms 93.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_242 0.0073 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0074 ms 88.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_239 0.0081 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0081 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_255 0.0085 ms 76.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7937 seconds and 0.4151 seconds precompiling for 25 choices
[rank4]:W1021 00:16:45.434000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:45.511000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:45.614000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:45 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_234 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_245 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_242 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_254 0.0083 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7778 seconds and 0.4241 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_234 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_245 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_242 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0071 ms 91.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_255 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0310 seconds and 0.4109 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  triton_bmm_234 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_245 0.0067 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_242 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0070 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0079 ms 82.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0079 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_236 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8488 seconds and 0.4133 seconds precompiling for 25 choices
[rank6]:W1021 00:16:46.298000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:46.378000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:46.482000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:46 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1021 00:16:46.638000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:46.716000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:46.817000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:46 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_234 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_245 0.0068 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_244 0.0069 ms 94.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0077 ms 84.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0078 ms 84.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_238 0.0082 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0082 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_255 0.0083 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7936 seconds and 0.4050 seconds precompiling for 25 choices
[rank3]:W1021 00:16:47.090000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:47.168000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:47.269000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:47.269000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:47 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1021 00:16:47.349000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:47.452000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:47 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_234 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_244 0.0071 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0071 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0078 ms 82.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_243 0.0078 ms 81.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_238 0.0079 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_239 0.0079 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_255 0.0086 ms 74.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.0636 seconds and 0.4194 seconds precompiling for 25 choices
AUTOTUNE bmm(16x16x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_234 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_235 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_244 0.0068 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_245 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_243 0.0072 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_242 0.0072 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_239 0.0082 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_238 0.0082 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_255 0.0084 ms 76.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8494 seconds and 0.4136 seconds precompiling for 25 choices
[rank2]:W1021 00:16:48.476000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:48.491000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:48.553000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:48.570000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:48.653000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:48.674000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:48 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:48 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1021 00:16:49.337000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:49.416000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/12_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:49.520000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/12] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:49 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:50 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:51 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:51 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1021 00:16:51.499000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:51.575000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:51.626000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:51 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:51 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:52 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1021 00:16:52.373000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:52.453000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:52.506000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:52 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1021 00:16:52.712000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:52.787000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:52.837000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:52 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:53 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1021 00:16:53.292000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:53 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1021 00:16:53.369000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:53.419000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:53 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1021 00:16:53.919000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:53.995000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:54.047000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:54 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:54 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1021 00:16:54.520000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:54.552000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:54.595000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:54.627000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:54.644000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:54.677000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:54 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:54 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1021 00:16:55.559000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:55.571000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:55.648000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/13_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:16:55.699000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/13] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:55 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank4]:W1021 00:16:55.812000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:56.063000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:56 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1021 00:16:57.309000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:57.494000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:57.572000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:57.573000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:57.587000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:16:57.674000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:57 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1021 00:16:57.828000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:57.842000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:57 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1021 00:16:58.100000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:58.144000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:58 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1021 00:16:58.404000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:16:58.660000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:58.691000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:58.699000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:58 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank7]:W1021 00:16:58.943000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:58.951000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:16:59.197000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:16:59.202000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:59.280000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:59 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank6]:W1021 00:16:59.359000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:59 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1021 00:16:59.383000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:16:59.460000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:59 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank1]:W1021 00:16:59.550000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:59.627000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:16:59.635000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:16:59.728000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:16:59 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1021 00:16:59.887000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:17:00 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1021 00:17:00.562000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/123_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:00.627000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:00.653000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:00.702000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:00.729000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:00.802000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:00.818000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/124_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:00.831000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:17:00 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:17:00 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1021 00:17:01.074000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/125_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:17:01 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank3]:W1021 00:17:01.288000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:01.385000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:01.468000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:01.486000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:17:01 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank2]:W1021 00:17:01.544000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:01.643000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:17:01 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[rank0]:W1021 00:17:02.564000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:02.639000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/14_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:02.741000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/14] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:17:02 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_259 0.0301 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_269 0.0307 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0310 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_258 0.0316 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0461 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0462 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0503 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0504 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_262 0.0544 ms 17.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9891 seconds and 0.2151 seconds precompiling for 25 choices
[rank4]:W1021 00:17:04.548000 292 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5ad0>
[rank4]:W1021 00:17:04.570000 292 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5680>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:17:04 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_259 0.0305 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_268 0.0307 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0308 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_258 0.0319 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0461 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0462 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_276 0.0505 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0507 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0542 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0119 seconds and 0.2189 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_268 0.0309 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0311 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_259 0.0317 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0328 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_266 0.0464 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0471 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0516 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0517 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0543 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0113 seconds and 0.2211 seconds precompiling for 25 choices
[rank4]:W1021 00:17:05.914000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_259 0.0298 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_269 0.0306 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0307 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_258 0.0314 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0462 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0462 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0515 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0515 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_262 0.0541 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9169 seconds and 0.2468 seconds precompiling for 25 choices
[rank6]:W1021 00:17:06.375000 294 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d9b0>
[rank4]:W1021 00:17:06.383000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_259 0.0329 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_269 0.0332 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0332 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_258 0.0337 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0489 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0499 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_266 0.0501 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0501 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_262 0.0587 ms 16.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0816 seconds and 0.2535 seconds precompiling for 25 choices
[rank1]:W1021 00:17:06.512000 289 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38790717a0>
[rank1]:W1021 00:17:06.534000 289 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3879071da0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:17:06 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1021 00:17:06.635000 294 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d950>
[rank4]:W1021 00:17:06.642000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:17:06 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1021 00:17:06.896000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_259 0.0299 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_268 0.0305 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0307 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_258 0.0310 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_266 0.0462 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0463 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_277 0.0515 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0515 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_263 0.0541 ms 17.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0375 seconds and 0.2251 seconds precompiling for 25 choices
[rank4]:W1021 00:17:07.152000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_269 0.0307 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_268 0.0323 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_259 0.0344 ms 27.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0345 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_267 0.0494 ms 19.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_266 0.0502 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0503 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0509 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_263 0.0601 ms 15.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0115 seconds and 0.2242 seconds precompiling for 25 choices
[rank4]:W1021 00:17:07.404000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:07.656000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:07.760000 295 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b9542b80>
[rank7]:W1021 00:17:07.784000 295 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b95414a0>
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:17:07 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1021 00:17:07.912000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:07.937000 293 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568dbf0>
[rank5]:W1021 00:17:07.960000 293 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568db90>
[rank6]:W1021 00:17:07.988000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:17:08 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1021 00:17:08.091000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:08.164000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:08.244000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_268 0.0309 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_269 0.0309 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_259 0.0311 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_258 0.0324 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_266 0.0464 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_267 0.0465 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_276 0.0516 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_277 0.0516 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_263 0.0548 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9756 seconds and 0.2182 seconds precompiling for 25 choices
[rank1]:W1021 00:17:08.355000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:08.416000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:08.433000 291 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4639884ed0>
[rank3]:W1021 00:17:08.456000 291 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f46398855c0>
[rank6]:W1021 00:17:08.501000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:17:08 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1021 00:17:08.609000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:08.674000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:08.757000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:08.840000 290 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d560>
[rank2]:W1021 00:17:08.863000 290 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d4d0>
[rank1]:W1021 00:17:08.871000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1021 00:17:08.928000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[2025-10-21 00:17:08 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1021 00:17:09.012000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:09.123000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:09.153000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:09.180000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:09.273000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:09.379000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:09.412000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:09.432000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:09.529000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:09.536000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:09.636000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:09.664000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:09.683000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:09.783000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:09.791000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:09.836000 288 torch/_dynamo/variables/builtin.py:1091] [72/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26ec40>
[rank0]:W1021 00:17:09.859000 288 torch/_dynamo/variables/builtin.py:1091] [73/2] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26de00>
[rank1]:W1021 00:17:09.892000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:09.916000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:17:09 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1021 00:17:09.935000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:10.033000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:10.039000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:10.048000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:10.144000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:10.168000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:10.187000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:10.287000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:10.296000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:10.306000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:10.395000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:10.421000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:10.440000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:10.552000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:10.558000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:10.565000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:10.656000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:10.672000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:10.692000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:10.807000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:10.816000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:10.823000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:10.911000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:10.928000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:10.944000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:11.035000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:11.060000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:11.072000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:11.080000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:11.167000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:11.181000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:11.196000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:11.287000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:11.312000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:11.327000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:11.337000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:11.419000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:11.432000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:11.448000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:11.540000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:11.564000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:11.579000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:11.593000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:11.675000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:11.690000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:11.700000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:11.792000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:11.815000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:11.832000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:11.847000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:11.933000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:11.940000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:11.951000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:12.044000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:12.067000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:12.073000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/126_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:12.084000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:12.103000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:12.188000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:12.197000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:12.203000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:12.304000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:12.321000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:12.335000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/127_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:12.343000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:12.361000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:12.439000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:12.450000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:12.456000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:12.555000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:12.576000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:12.586000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/128_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:12.595000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:12.617000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:12.695000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:12.703000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:12.712000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:12.808000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:12.827000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:12.841000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/129_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:12.848000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:12.872000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:12.952000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:12.958000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:12.965000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:13.056000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:13.079000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:13.091000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/130_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:13.100000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:13.132000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:13.204000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:13.213000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:13.219000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:13.304000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:13.335000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:13.345000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/131_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:13.352000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:13.388000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:13.457000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:13.463000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:13.475000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:13.552000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:13.588000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:13.595000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/132_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:13.603000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:13.648000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:13.709000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:13.716000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:13.727000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:13.804000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:13.844000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:13.850000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/133_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:13.857000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:13.904000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:13.961000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:13.968000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:13.983000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:14.064000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:14.099000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:14.111000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/134_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:14.120000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:14.159000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:14.213000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:14.219000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:14.235000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:14.312000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:14.359000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:14.365000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/135_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:14.373000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:14.415000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:14.465000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:14.471000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:14.491000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:14.560000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:14.616000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/136_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:14.622000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:14.628000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:14.671000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:14.717000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:14.723000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:14.747000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:14.807000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:14.867000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/137_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:14.876000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:14.882000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:14.929000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:14.968000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:14.977000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:15.004000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:15.055000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:15.118000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/138_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:15.132000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:15.137000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:15.189000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:15.223000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:15.233000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:15.307000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:15.370000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/139_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:15.388000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:15.393000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:15.448000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:15.479000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:15.488000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:15.508000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:15.559000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:15.622000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/140_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:15.644000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:15.649000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:15.708000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:15.735000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:15.742000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:15.764000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:15.811000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:15.874000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/141_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:15.900000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:15.905000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:15.968000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:15.991000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:15.999000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:16.020000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:16.067000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:16.126000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/142_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:16.156000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:16.162000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:16.247000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:16.257000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:16.280000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:16.319000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:16.379000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/143_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:16.413000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:16.419000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:16.489000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:16.513000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:16.540000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:16.575000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:16.638000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/144_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:16.669000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:16.679000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:16.735000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:16.748000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:16.765000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:16.796000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:16.827000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:16.894000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/145_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:16.924000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:16.935000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:16.991000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:17.009000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:17.056000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:17.079000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:17.146000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/146_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:17.180000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:17.191000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:17.247000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:17.272000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:17.279000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:17.316000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:17.331000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:17.398000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/147_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:17.436000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:17.447000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:17.503000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:17.532000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:17.539000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:17.572000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:17.583000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:17.650000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/148_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:17.692000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:17.703000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:17.759000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:17.789000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:17.800000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:17.828000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:17.835000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:17.904000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/149_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:17.947000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:17.960000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:18.016000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:18.044000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:18.060000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:18.083000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:18.090000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:18.163000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/150_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:18.216000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:18.272000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:18.303000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:18.323000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:18.340000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:18.347000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:18.415000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/151_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:18.468000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:18.475000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:18.528000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:18.559000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:18.583000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:18.592000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:18.603000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:18.668000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/152_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:18.731000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:18.792000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:18.815000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:18.843000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:18.849000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:18.859000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:18.924000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/153_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:18.944000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:18.988000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:19.056000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:19.071000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:19.103000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:19.109000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:19.116000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:19.175000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/154_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:19.200000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:19.243000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:19.316000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:19.327000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:19.364000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:19.370000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:19.376000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:19.427000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/155_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:19.452000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:19.504000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:19.572000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:19.583000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:19.624000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:19.631000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:19.637000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:19.680000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/156_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:19.705000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:19.763000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:19.838000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:19.844000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:19.892000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:19.897000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:19.969000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:20.023000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:20.100000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:20.106000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:20.151000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:20.157000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:20.164000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:20.179000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/157_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:20.228000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:20.283000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:20.360000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:20.366000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:20.411000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:20.416000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:20.423000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:20.431000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/158_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:20.484000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:20.545000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:20.615000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:20.624000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:20.672000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:20.679000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:20.684000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:20.691000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/159_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:20.739000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:20.804000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:20.875000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:20.882000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:20.932000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:20.938000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:20.946000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/160_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:20.995000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:21.064000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:21.135000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:21.142000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:21.193000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:21.198000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:21.205000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/161_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:21.252000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:21.325000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:21.395000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:21.403000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:21.463000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:21.471000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:21.477000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/162_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:21.515000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:21.584000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:21.655000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:21.662000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:21.719000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:21.733000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:21.738000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/163_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:21.771000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:21.840000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:21.915000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:21.922000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:21.988000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:21.994000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:22.007000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/164_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:22.040000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:22.095000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:22.176000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:22.182000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:22.245000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:22.255000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:22.263000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/165_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:22.300000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:22.351000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:22.439000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:22.445000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:22.500000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:22.515000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/166_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:22.522000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:22.552000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:22.607000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:22.695000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:22.701000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:22.751000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:22.767000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/167_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:22.783000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:22.808000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:22.863000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:22.951000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:23.003000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:23.018000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/168_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:23.063000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:23.121000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:23.209000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:23.259000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:23.275000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/169_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:23.320000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:23.381000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:23.473000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:23.519000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:23.540000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/170_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:23.583000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:23.641000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:23.732000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:23.774000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:23.798000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/171_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:23.843000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:23.900000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:24.038000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:24.062000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/172_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:24.117000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:24.156000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:24.295000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:24.322000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/173_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:24.379000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:24.555000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:24.582000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/174_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:24.652000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:24.811000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:24.843000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/175_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:25.070000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:25.102000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/176_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:25.339000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:25.363000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/177_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:25.594000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:25.624000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/178_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:25.878000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/179_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:26.135000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/180_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:26.401000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/181_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_282 0.1033 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1034 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1043 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1043 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1091 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1091 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1124 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1124 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_287 0.1474 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0704 seconds and 0.1870 seconds precompiling for 25 choices
[rank0]:W1021 00:17:26.670000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/182_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_282 0.1034 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1034 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1045 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1045 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_301 0.1091 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1092 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_290 0.1126 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1127 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_286 0.1476 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0725 seconds and 0.1917 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_282 0.1033 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1033 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_293 0.1044 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_292 0.1045 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1093 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1094 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_290 0.1127 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1128 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_286 0.1474 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0056 seconds and 0.1880 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_283 0.1035 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1036 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_293 0.1047 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_292 0.1048 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_300 0.1093 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1095 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1125 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1126 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_286 0.1482 ms 41.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0395 seconds and 0.1892 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_282 0.1035 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1035 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1045 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1046 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_301 0.1092 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1094 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_290 0.1125 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1127 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_286 0.1474 ms 41.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0906 seconds and 0.1913 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_283 0.1033 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_282 0.1034 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_293 0.1046 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_292 0.1047 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_300 0.1092 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1092 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1127 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_291 0.1128 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_286 0.1478 ms 41.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0538 seconds and 0.1898 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_282 0.1003 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1003 ms 60.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_292 0.1014 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_293 0.1015 ms 59.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_300 0.1055 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1057 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_291 0.1083 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1083 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_302 0.1388 ms 43.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9718 seconds and 0.1858 seconds precompiling for 25 choices
AUTOTUNE mm(16x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_282 0.1033 ms 59.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_283 0.1034 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_293 0.1047 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_292 0.1048 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_300 0.1095 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_301 0.1096 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_291 0.1128 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_290 0.1128 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_287 0.1483 ms 41.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 7.0089 seconds and 0.0001 seconds precompiling for 25 choices
Capturing batches (bs=16 avail_mem=40.89 GB):  90%| | 47/52 [04:58<04:30, 54.16s/it]Capturing batches (bs=12 avail_mem=40.25 GB):  90%| | 47/52 [04:58<04:30, 54.16s/it][rank0]:W1021 00:17:40.815000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:40.817000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:40.893000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:40.895000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:40.989000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:41.001000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:41.003000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:41.065000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:41.172000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:41.225000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:41.303000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:41.413000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:41.539000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:41.585000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:41.618000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:41.663000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:41.730000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:41.771000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:41.963000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:42.041000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:42.150000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:42.161000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:42.239000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/15_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:42.348000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/15] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:42.531000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:42.549000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:42.702000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:42.982000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:43.315000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:43.326000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:43.704000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:43.923000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:44.104000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:44.182000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:44.284000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:44.488000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:44.566000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:44.670000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:44.750000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:44.832000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:44.940000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:44.966000 291 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank7]:W1021 00:17:45.111000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:45.122000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:45.136000 289 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank0]:W1021 00:17:45.170000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:45.192000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:45.204000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:45.254000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:45.305000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:45.314000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:45.366000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:45.427000 294 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank2]:W1021 00:17:45.738000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:45.779000 295 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank4]:W1021 00:17:45.787000 292 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank2]:W1021 00:17:45.814000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:45.859000 288 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank2]:W1021 00:17:45.925000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:46.394000 290 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
[rank5]:W1021 00:17:47.059000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:47.136000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/16_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:47.238000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/16] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:47.700000 293 torch/_inductor/utils.py:1349] [28/3] Please pip install Composable Kernel package
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_323 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_318 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_314 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_308 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_327 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0063 ms 98.1% 
  triton_bmm_321 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_310 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_316 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_322 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8202 seconds and 0.1526 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_314 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0064 ms 99.4% 
  triton_bmm_319 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_325 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_307 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_311 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_316 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_320 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_321 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_306 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.7857 seconds and 0.1507 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_315 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_309 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_311 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_317 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_320 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_322 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_327 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_305 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_316 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_318 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7978 seconds and 0.1511 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_310 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.1% 
  triton_bmm_309 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_314 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_315 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_322 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_316 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_306 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_307 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_308 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8303 seconds and 0.1562 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_309 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_327 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_307 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_311 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_315 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_317 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_319 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_321 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_323 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_325 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8492 seconds and 0.1366 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_309 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_325 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_304 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_305 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_306 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_307 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_308 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_310 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_314 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8327 seconds and 0.1340 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_307 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_306 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_319 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_320 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_322 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_318 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_305 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_308 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_309 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8480 seconds and 0.1534 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x128, 16x128x512)
  triton_bmm_306 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_307 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_310 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_325 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_309 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_324 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_304 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_322 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_315 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_317 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8324 seconds and 0.1494 seconds precompiling for 25 choices
[rank3]:W1021 00:17:55.436000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:55.939000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:56.051000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:56.261000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:56.424000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:17:56.554000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:56.606000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:56.777000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:17:56.942000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:56.947000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:17:57.118000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:57.235000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:17:57.458000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:17:57.764000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:17:57.863000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/3_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:17:58.185000 291 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank5]:W1021 00:17:58.370000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/183_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:17:58.873000 292 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank1]:W1021 00:17:58.922000 289 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank2]:W1021 00:17:59.507000 290 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank7]:W1021 00:17:59.548000 295 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank0]:W1021 00:17:59.822000 288 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank5]:W1021 00:18:00.995000 293 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
[rank6]:W1021 00:18:02.084000 294 torch/_inductor/utils.py:1349] [40/3_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_331 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_330 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.7% 
  triton_bmm_340 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_339 0.0070 ms 90.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0070 ms 90.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_334 0.0080 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0080 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0082 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7765 seconds and 0.3994 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0069 ms 93.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0070 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0083 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8273 seconds and 0.4078 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0069 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_339 0.0072 ms 90.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0072 ms 90.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_334 0.0079 ms 81.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0080 ms 81.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_351 0.0083 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7855 seconds and 0.4040 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_330 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_341 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_340 0.0067 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0071 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0075 ms 87.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_335 0.0081 ms 79.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0082 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_351 0.0082 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8204 seconds and 0.4260 seconds precompiling for 25 choices
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_330 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.8% 
  triton_bmm_340 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_339 0.0070 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_334 0.0079 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0079 ms 81.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_350 0.0081 ms 80.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8556 seconds and 0.4093 seconds precompiling for 25 choices
[rank4]:W1021 00:18:05.057000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_331 0.0066 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_340 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0069 ms 93.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0072 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0073 ms 89.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_335 0.0082 ms 78.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0082 ms 78.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_350 0.0083 ms 77.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7762 seconds and 0.3955 seconds precompiling for 25 choices
[rank1]:W1021 00:18:05.074000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:05.135000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:05.152000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:05.168000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:05.185000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:05.201000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:05.244000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:05.293000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:05.894000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:05.972000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:06.022000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:06.090000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:06.167000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:06.217000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:06.280000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_331 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 100.0% 
  triton_bmm_330 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_340 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0077 ms 83.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0078 ms 82.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_334 0.0079 ms 81.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_335 0.0079 ms 81.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_351 0.0084 ms 76.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8254 seconds and 0.4124 seconds precompiling for 25 choices
[rank2]:W1021 00:18:06.357000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:06.406000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:07.305000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:07.382000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x12x512, 16x512x128)
  triton_bmm_331 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_330 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.1% 
  triton_bmm_340 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_341 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_338 0.0071 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_339 0.0071 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_335 0.0080 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_334 0.0080 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_350 0.0082 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8332 seconds and 0.4207 seconds precompiling for 25 choices
[rank5]:W1021 00:18:07.432000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:08.863000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:08.942000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/17_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:08.994000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/17] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:11.271000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:11.350000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:11.462000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:11.589000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:11.668000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:11.778000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:11.950000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:12.029000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:12.140000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:12.280000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:12.359000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:12.473000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:12.563000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:12.592000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:12.641000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:12.670000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:12.748000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:12.780000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:13.469000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:13.547000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:13.657000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:15.543000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:15.623000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/18_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:15.734000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/18] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:16.279000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:16.287000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:16.550000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:16.559000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:16.581000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:16.814000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:16.824000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:16.845000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:16.927000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:17.104000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:17.187000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:17.456000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:17.497000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:17.709000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:17.763000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:17.975000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:18.027000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:18.234000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:18.255000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:18.286000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:18.333000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:18.364000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:18.398000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:18.401000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:18.436000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:18.467000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:18.476000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:18.578000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:18.669000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:18.903000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:18.930000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:18.980000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:19.095000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:19.696000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:19.771000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:19.774000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:19.846000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:19.878000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:19.947000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:20.217000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:20.296000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:20.407000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:20.549000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/184_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:20.822000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/185_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:21.085000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/186_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:22.524000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:22.602000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/19_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:22.705000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/19] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_355 0.0303 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_365 0.0305 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0306 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_354 0.0324 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0460 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0460 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_373 0.0512 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0514 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_359 0.0539 ms 17.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0860 seconds and 0.2587 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0097 ms 100.0% 
  triton_mm_355 0.0296 ms 32.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_364 0.0303 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.0303 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_354 0.0320 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0459 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0459 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_373 0.0512 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0513 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_359 0.0539 ms 17.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1061 seconds and 0.2456 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_355 0.0299 ms 31.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_364 0.0306 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.0306 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_354 0.0324 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0461 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0461 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0506 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0507 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0540 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1115 seconds and 0.2540 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_364 0.0305 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.0306 ms 30.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_355 0.0315 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0325 ms 29.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0461 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0464 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0510 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0512 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0541 ms 17.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0747 seconds and 0.2323 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0096 ms 100.0% 
  triton_mm_355 0.0299 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_364 0.0307 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.0309 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_354 0.0317 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0463 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0464 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_373 0.0518 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0518 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_358 0.0546 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0076 seconds and 0.2231 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0098 ms 100.0% 
  triton_mm_364 0.0305 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_365 0.0305 ms 32.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_355 0.0333 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0347 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0461 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0462 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_372 0.0512 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0512 ms 19.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0568 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1142 seconds and 0.2345 seconds precompiling for 25 choices
[rank4]:W1021 00:18:25.692000 292 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5ad0>
[rank4]:W1021 00:18:25.714000 292 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5680>
[rank7]:W1021 00:18:25.737000 295 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b9542b80>
[rank7]:W1021 00:18:25.760000 295 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b95414a0>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:18:25 TP4] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:18:25 TP7] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1021 00:18:26.060000 291 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4639884ed0>
[rank3]:W1021 00:18:26.082000 291 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f46398855c0>
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_364 0.0327 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_355 0.0331 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_365 0.0333 ms 28.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_354 0.0338 ms 27.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_363 0.0497 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_362 0.0499 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0501 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0503 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_359 0.0591 ms 15.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0814 seconds and 0.2252 seconds precompiling for 25 choices
[rank1]:W1021 00:18:26.127000 289 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38790717a0>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:18:26 TP3] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1021 00:18:26.150000 289 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3879071da0>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:18:26 TP1] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1021 00:18:26.944000 288 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26ec40>
[rank0]:W1021 00:18:26.966000 288 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26de00>
[rank2]:W1021 00:18:26.984000 290 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d560>
[rank2]:W1021 00:18:27.007000 290 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d4d0>
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:18:27 TP0] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:18:27 TP2] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1021 00:18:27.113000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:27.217000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:27.376000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:27.435000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:27.476000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:27.508000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:27.557000 293 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568dbf0>
[rank5]:W1021 00:18:27.579000 293 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568db90>
[rank7]:W1021 00:18:27.636000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:18:27 TP5] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1021 00:18:27.696000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:27.735000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:27.769000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:27.894000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:27.952000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:27.995000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:28.028000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:28.152000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:28.212000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:28.255000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:28.290000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:28.304000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:28.356000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_365 0.0305 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_364 0.0305 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_355 0.0309 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_354 0.0321 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_362 0.0461 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_363 0.0468 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_372 0.0513 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_373 0.0514 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_359 0.0542 ms 17.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0618 seconds and 0.2271 seconds precompiling for 25 choices
[rank7]:W1021 00:18:28.416000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:28.484000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:28.517000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:28.557000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:28.572000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:28.613000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:28.672000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:28.744000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:28.779000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:28.821000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:28.828000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:28.868000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:28.932000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:29.005000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:29.039000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:29.059000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:29.076000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:29.084000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:29.124000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:29.192000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:29.265000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:29.299000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:29.315000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:29.332000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:29.340000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:29.380000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:29.452000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:29.525000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:29.561000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:29.573000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:29.587000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:29.599000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:29.635000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:29.671000 294 torch/_dynamo/variables/builtin.py:1091] [72/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d9b0>
[rank6]:W1021 00:18:29.693000 294 torch/_dynamo/variables/builtin.py:1091] [73/3] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d950>
[rank7]:W1021 00:18:29.713000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:18:29 TP6] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1021 00:18:29.787000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:29.824000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:29.831000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:29.847000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:29.858000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:29.891000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:29.973000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:30.051000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:30.088000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:30.095000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:30.107000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:30.118000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:30.147000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:30.233000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:30.315000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:30.352000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:30.359000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:30.367000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:30.378000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:30.403000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:30.493000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:30.581000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:30.612000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:30.619000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:30.633000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:30.641000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:30.660000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:30.756000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:30.844000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:30.871000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:30.879000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:30.892000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:30.901000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:30.918000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:31.020000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:31.048000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/187_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:31.116000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:31.132000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:31.139000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:31.161000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:31.168000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:31.180000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:31.280000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:31.311000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/188_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:31.376000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:31.392000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:31.399000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:31.421000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:31.428000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:31.438000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:31.540000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:31.575000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/189_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:31.636000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:31.651000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:31.659000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:31.681000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:31.690000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:31.698000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:31.804000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:31.836000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/190_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:31.896000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:31.911000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:31.919000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:31.941000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:31.949000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:31.958000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:32.064000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:32.100000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/191_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:32.171000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:32.179000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:32.210000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:32.217000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:32.225000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:32.324000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:32.372000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/192_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:32.431000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:32.476000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:32.485000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:32.492000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:32.500000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:32.588000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:32.636000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/193_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:32.691000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:32.740000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:32.748000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:32.757000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:32.765000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:32.816000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:32.904000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/194_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:32.951000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:33.009000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:33.020000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:33.028000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:33.080000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:33.168000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/195_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:33.212000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:33.220000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:33.281000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:33.293000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:33.321000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:33.344000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:33.432000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/196_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:33.472000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:33.484000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:33.565000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:33.601000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:33.612000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:33.618000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:33.698000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/197_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:33.754000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:33.839000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:33.881000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:33.886000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:33.894000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:33.900000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:33.965000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/198_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:34.021000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:34.088000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:34.107000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:34.148000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:34.155000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:34.161000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:34.168000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:34.229000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/199_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:34.285000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:34.352000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:34.375000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:34.412000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:34.419000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:34.426000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:34.433000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:34.493000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/200_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:34.549000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:34.616000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:34.643000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:34.675000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:34.688000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:34.696000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:34.704000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:34.756000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/201_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:34.812000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:34.879000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:34.908000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:34.940000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:34.952000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:34.962000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:34.969000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:35.020000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/202_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:35.076000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:35.143000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:35.177000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:35.211000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:35.218000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:35.229000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:35.236000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:35.286000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/203_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:35.341000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:35.408000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:35.451000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:35.476000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:35.487000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:35.496000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:35.501000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:35.557000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/204_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:35.609000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:35.672000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:35.719000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:35.741000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:35.747000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:35.760000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:35.766000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:35.881000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:35.940000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:35.987000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:36.007000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:36.015000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:36.023000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:36.030000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:36.149000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:36.188000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/205_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:36.204000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:36.251000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:36.267000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:36.281000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:36.288000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:36.294000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:36.417000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:36.452000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/206_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:36.468000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:36.515000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:36.527000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:36.546000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:36.552000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:36.558000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:36.685000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:36.719000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/207_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:36.731000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:36.780000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:36.788000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:36.808000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:36.816000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:36.824000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:36.948000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:36.987000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/208_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:36.992000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:37.052000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:37.059000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:37.071000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:37.088000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:37.097000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:37.212000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:37.255000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:37.261000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/209_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:37.316000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:37.324000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:37.335000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:37.360000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:37.369000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:37.480000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:37.515000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:37.527000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/210_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:37.577000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:37.584000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:37.599000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:37.620000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:37.629000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:37.748000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:37.775000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:37.795000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/211_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:37.836000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:37.848000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:37.863000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:37.880000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:37.889000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:38.011000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:38.035000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:38.063000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/212_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:38.096000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:38.108000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:38.127000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:38.140000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:38.149000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:38.276000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:38.295000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:38.331000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/213_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:38.356000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:38.372000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:38.392000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:38.400000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:38.409000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:38.540000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:38.555000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:38.599000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/214_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:38.616000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:38.632000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:38.655000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:38.662000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:38.671000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:38.803000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:38.815000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:38.868000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/215_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:38.876000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:38.892000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:38.919000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:38.929000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:38.937000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:39.073000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:39.079000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:39.135000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/216_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:39.148000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:39.168000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:39.183000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:39.200000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:39.207000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:39.340000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:39.345000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:39.400000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/217_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:39.408000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:39.432000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:39.447000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:39.465000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:39.472000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:39.603000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:39.609000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:39.668000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:39.675000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/218_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:39.692000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:39.711000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:39.730000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:39.738000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:39.868000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:39.874000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:39.940000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:39.947000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/219_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:39.964000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:39.975000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:39.997000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:40.004000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:40.135000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:40.141000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:40.200000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:40.211000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/220_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:40.228000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:40.239000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:40.261000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:40.269000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:40.399000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:40.408000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:40.460000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:40.475000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/221_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:40.492000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:40.503000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:40.524000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:40.532000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:40.663000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:40.671000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:40.720000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:40.739000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/222_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:40.756000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:40.767000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:40.788000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:40.797000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:40.931000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:40.937000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:40.980000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:41.007000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/223_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:41.020000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:41.031000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:41.053000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:41.061000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:41.199000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:41.205000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:41.240000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:41.275000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/224_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:41.284000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:41.295000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:41.317000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:41.325000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:41.469000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:41.476000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:41.499000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:41.545000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/225_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:41.550000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:41.560000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:41.580000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:41.586000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:41.737000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:41.744000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:41.763000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:41.813000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/226_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:41.819000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:41.830000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:41.847000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:41.853000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:42.005000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:42.012000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:42.039000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:42.081000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/227_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:42.096000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:42.102000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:42.115000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:42.121000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:42.273000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:42.280000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:42.303000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:42.348000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/228_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:42.361000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:42.367000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:42.380000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:42.386000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:42.535000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:42.577000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:42.616000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/229_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:42.644000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:42.657000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:42.664000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:42.803000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:42.840000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:42.888000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/230_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:42.928000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:43.072000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:43.105000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:43.161000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/231_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:43.201000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:43.348000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:18:43.382000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:43.440000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/232_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:18:43.479000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:43.620000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:43.709000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/233_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:43.888000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:43.982000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/234_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:18:44.161000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:44.257000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/235_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:44.524000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/236_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:44.792000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/237_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:45.064000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/238_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:45.331000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/239_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:45.596000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/240_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:45.863000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/241_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:46.131000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/242_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:46.404000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/243_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_378 0.1042 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1043 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_389 0.1046 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_388 0.1046 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_396 0.1088 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1089 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_387 0.1120 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1121 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_383 0.1448 ms 42.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0978 seconds and 0.2123 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_378 0.1042 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1042 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1043 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1086 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1087 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1120 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1120 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_383 0.1441 ms 42.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1246 seconds and 0.2114 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_378 0.1042 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1042 ms 58.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_389 0.1047 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_388 0.1047 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_396 0.1086 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1087 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1121 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1122 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_382 0.1443 ms 42.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0763 seconds and 0.1914 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0610 ms 100.0% 
  triton_mm_379 0.1042 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1043 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_389 0.1044 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_388 0.1045 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1088 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1089 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_386 0.1122 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1122 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_382 0.1445 ms 42.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0823 seconds and 0.2075 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_378 0.1011 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1011 ms 60.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1015 ms 59.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1016 ms 59.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1051 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1052 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1081 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1081 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_383 0.1362 ms 44.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0066 seconds and 0.2234 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0609 ms 100.0% 
  triton_mm_379 0.1044 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1045 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_388 0.1046 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1046 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1090 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_397 0.1092 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1120 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1121 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_383 0.1449 ms 42.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0647 seconds and 0.2034 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0611 ms 100.0% 
  triton_mm_378 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_379 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_388 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1043 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_397 0.1092 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1094 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1121 ms 54.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_386 0.1122 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_382 0.1441 ms 42.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0656 seconds and 0.2049 seconds precompiling for 25 choices
AUTOTUNE mm(12x7168, 7168x16160)
  mm 0.0613 ms 100.0% 
  triton_mm_379 0.1042 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_378 0.1042 ms 58.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_388 0.1044 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_389 0.1044 ms 58.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_397 0.1096 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_396 0.1098 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_386 0.1129 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_387 0.1134 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_382 0.1445 ms 42.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0460 seconds and 0.1948 seconds precompiling for 25 choices
Capturing batches (bs=12 avail_mem=40.25 GB):  92%|| 48/52 [06:16<04:05, 61.49s/it]Capturing batches (bs=8 avail_mem=39.64 GB):  92%|| 48/52 [06:16<04:05, 61.49s/it] [rank7]:W1021 00:18:59.583000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:59.592000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:59.662000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:59.671000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:59.687000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:59.769000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:18:59.773000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:18:59.782000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:59.867000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:18:59.882000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:59.901000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:18:59.946000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:18:59.980000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:00.056000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:00.090000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:00.147000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:00.226000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:00.337000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:00.448000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:00.528000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:00.638000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:00.915000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:00.995000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/20_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:01.104000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/20] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:01.364000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:01.380000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:01.493000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:01.654000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:01.659000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:01.890000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:02.210000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:02.694000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:02.910000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:02.990000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:03.053000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:03.093000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:03.135000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:03.243000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:03.447000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:03.526000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:03.562000 295 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank2]:W1021 00:19:03.615000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:03.633000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:03.699000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:03.718000 294 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank2]:W1021 00:19:03.812000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:03.845000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:03.905000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:03.929000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:03.987000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:03.989000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:04.035000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:04.068000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:04.095000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:04.115000 292 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank3]:W1021 00:19:04.174000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:04.286000 290 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank5]:W1021 00:19:04.508000 293 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank1]:W1021 00:19:04.583000 289 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank3]:W1021 00:19:04.663000 291 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
[rank0]:W1021 00:19:04.780000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:04.868000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/21_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:04.975000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/21] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:05.451000 288 torch/_inductor/utils.py:1349] [28/4] Please pip install Composable Kernel package
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_403 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_422 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_413 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_400 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_418 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_406 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_410 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_421 0.0063 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_423 0.0064 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8372 seconds and 0.1541 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_403 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_405 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_421 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_401 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_407 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_411 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_413 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_415 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0063 ms 99.4% 
SingleProcess AUTOTUNE benchmarking takes 4.8645 seconds and 0.1532 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_403 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_411 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_407 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_413 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_419 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_402 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_404 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_406 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_420 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_405 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8494 seconds and 0.1551 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_418 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_419 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_417 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_420 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_421 0.0061 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_415 0.0061 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_422 0.0061 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_416 0.0061 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_423 0.0062 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_403 0.0062 ms 97.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9054 seconds and 0.1505 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_406 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_412 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_402 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_405 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_411 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_418 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_400 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_413 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_403 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_407 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8900 seconds and 0.1534 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_412 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_413 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_411 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_410 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_418 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0064 ms 96.9% 
  triton_bmm_403 0.0064 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_419 0.0064 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_416 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_404 0.0065 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8286 seconds and 0.1547 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  triton_bmm_418 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_410 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_411 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_421 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_412 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_416 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_413 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_419 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_420 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_417 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9106 seconds and 0.1504 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_400 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_412 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_414 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_420 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_401 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_402 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_403 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_404 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_405 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9064 seconds and 0.1555 seconds precompiling for 25 choices
[rank4]:W1021 00:19:14.324000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:14.404000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:14.550000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:14.838000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:14.917000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:14.933000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:15.074000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:15.095000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:15.209000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:15.436000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:15.451000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:15.624000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:15.728000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:15.951000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:16.815000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/4_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:16.847000 292 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank6]:W1021 00:19:17.102000 294 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank7]:W1021 00:19:17.245000 295 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank0]:W1021 00:19:17.344000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/244_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:17.598000 289 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank3]:W1021 00:19:17.697000 291 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank5]:W1021 00:19:18.336000 293 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank2]:W1021 00:19:18.805000 290 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
[rank0]:W1021 00:19:20.169000 288 torch/_inductor/utils.py:1349] [40/4_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_427 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.7% 
  triton_bmm_436 0.0066 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0066 ms 95.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0070 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0070 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0077 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0077 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_428 0.0081 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8311 seconds and 0.4319 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_426 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_427 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_437 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0069 ms 93.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_434 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0072 ms 88.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_431 0.0079 ms 80.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0080 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_447 0.0083 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8347 seconds and 0.4209 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_426 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0072 ms 89.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0077 ms 83.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0077 ms 82.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_446 0.0081 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8370 seconds and 0.4159 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_427 0.0065 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_426 0.0065 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_437 0.0068 ms 95.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_436 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_434 0.0071 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0073 ms 88.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0079 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_428 0.0083 ms 77.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8125 seconds and 0.4088 seconds precompiling for 25 choices
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_426 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.2% 
  triton_bmm_427 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0070 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0070 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0079 ms 80.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0079 ms 80.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_446 0.0082 ms 78.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7829 seconds and 0.4445 seconds precompiling for 25 choices
[rank4]:W1021 00:19:23.177000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:23.257000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:23.361000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:23.435000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:23.450000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:23.517000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:23.529000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  triton_bmm_426 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.1% 
  triton_bmm_427 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0069 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0069 ms 91.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0077 ms 81.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0078 ms 80.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0079 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0079 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_447 0.0083 ms 75.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7649 seconds and 0.4185 seconds precompiling for 25 choices
[rank6]:W1021 00:19:23.622000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:23.632000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:23.888000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:23.966000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:23.978000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:24.056000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:24.067000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:24.158000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_427 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_426 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_436 0.0067 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0075 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0075 ms 85.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_430 0.0080 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_431 0.0081 ms 79.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_446 0.0081 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8742 seconds and 0.4099 seconds precompiling for 25 choices
[rank5]:W1021 00:19:24.737000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:24.816000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:24.918000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:25.043000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:25.132000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:25.232000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x8x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_426 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_427 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_436 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_437 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_434 0.0071 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_435 0.0071 ms 89.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_431 0.0080 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_430 0.0080 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_428 0.0082 ms 77.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.7897 seconds and 0.4061 seconds precompiling for 25 choices
[rank0]:W1021 00:19:26.473000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:26.553000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/22_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:26.656000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/22] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:29.041000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:29.119000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:29.169000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:29.373000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:29.382000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:29.455000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:29.461000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:29.508000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:29.514000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:29.832000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:29.838000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:29.909000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:29.916000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:29.960000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:29.967000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:30.559000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:30.638000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:30.689000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:31.127000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:31.204000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:31.254000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:32.685000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:32.762000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/23_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:32.813000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/23] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:33.076000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:33.348000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:33.529000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:33.616000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:33.801000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:34.069000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:34.174000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:34.450000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:34.724000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:34.739000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:34.760000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:35.009000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:35.033000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:35.285000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:35.299000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:35.354000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:35.379000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:35.405000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:35.434000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:35.538000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:35.678000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:35.922000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:35.946000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:35.988000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:36.031000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:36.264000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:36.536000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:36.656000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:36.737000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:36.852000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:37.193000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:37.274000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:37.354000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:37.385000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:37.436000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:37.519000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/245_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:37.541000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:37.793000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/246_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:37.862000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:37.945000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:38.060000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:38.069000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/247_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:38.281000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:38.371000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:38.483000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:40.011000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:40.090000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/24_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:40.192000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/24] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_451 0.0297 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_460 0.0301 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_461 0.0303 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_450 0.0311 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0456 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0457 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_468 0.0501 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0504 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_455 0.0538 ms 17.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1125 seconds and 0.2260 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_451 0.0292 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_461 0.0297 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0300 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_450 0.0309 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0457 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0459 ms 19.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0505 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0508 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_454 0.0539 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0214 seconds and 0.2178 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_460 0.0299 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_451 0.0301 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_461 0.0301 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_450 0.0315 ms 28.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0456 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0456 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_469 0.0501 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0505 ms 18.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_455 0.0540 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0651 seconds and 0.2209 seconds precompiling for 25 choices
[rank3]:W1021 00:19:42.614000 291 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4639884ed0>
[rank3]:W1021 00:19:42.637000 291 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f46398855c0>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:19:42 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1021 00:19:42.835000 292 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5ad0>
[rank4]:W1021 00:19:42.858000 292 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5680>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:19:42 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_451 0.0290 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0298 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_460 0.0300 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_461 0.0300 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0456 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0456 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0504 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0505 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_455 0.0538 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0265 seconds and 0.2252 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_460 0.0302 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_461 0.0303 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_451 0.0312 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0321 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0456 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0456 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0497 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0499 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_455 0.0542 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0213 seconds and 0.2215 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_451 0.0315 ms 29.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0321 ms 28.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_461 0.0329 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0329 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0499 ms 18.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0501 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_468 0.0505 ms 18.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0505 ms 18.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_455 0.0604 ms 15.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1440 seconds and 0.2159 seconds precompiling for 25 choices
[rank6]:W1021 00:19:43.878000 294 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d9b0>
[rank6]:W1021 00:19:43.901000 294 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d950>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:19:43 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1021 00:19:44.137000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_460 0.0298 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_461 0.0298 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_451 0.0330 ms 28.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_450 0.0332 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_459 0.0455 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_458 0.0456 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_468 0.0505 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0506 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_455 0.0543 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1545 seconds and 0.2193 seconds precompiling for 25 choices
[rank4]:W1021 00:19:44.341000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:44.347000 289 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38790717a0>
[rank7]:W1021 00:19:44.357000 295 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b9542b80>
[rank1]:W1021 00:19:44.370000 289 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3879071da0>
[rank7]:W1021 00:19:44.381000 295 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b95414a0>
[rank3]:W1021 00:19:44.404000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:19:44 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:19:44 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1021 00:19:44.613000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:44.671000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:44.881000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:44.947000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:45.008000 293 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568dbf0>
[rank5]:W1021 00:19:45.035000 293 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568db90>
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:19:45 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1021 00:19:45.155000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:45.213000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:45.409000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:45.425000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:45.491000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:45.681000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:45.693000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:45.741000 290 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d560>
[rank3]:W1021 00:19:45.764000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:45.774000 290 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d4d0>
AUTOTUNE mm(8x7168, 7168x256)
  mm 0.0094 ms 100.0% 
  triton_mm_451 0.0292 ms 32.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_461 0.0302 ms 31.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_460 0.0305 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_450 0.0310 ms 30.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_458 0.0461 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_459 0.0461 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_468 0.0509 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_469 0.0511 ms 18.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_454 0.0542 ms 17.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0047 seconds and 0.2322 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:19:45 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1021 00:19:45.952000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:45.960000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:46.021000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:46.041000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:46.195000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:46.227000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:46.233000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:46.288000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:46.313000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:46.463000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:46.495000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:46.503000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:46.549000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:46.577000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:46.732000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:46.763000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:46.775000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:46.817000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:46.845000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:46.976000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:47.000000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:47.032000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:47.048000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:47.085000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:47.113000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:47.161000 288 torch/_dynamo/variables/builtin.py:1091] [72/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26ec40>
[rank0]:W1021 00:19:47.184000 288 torch/_dynamo/variables/builtin.py:1091] [73/4] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26de00>
[rank5]:W1021 00:19:47.248000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:19:47 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1021 00:19:47.268000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:47.299000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:47.320000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:47.355000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:47.381000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:47.516000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:47.536000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:47.567000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:47.592000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:47.629000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:47.649000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:47.737000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:47.784000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:47.804000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:47.836000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:47.864000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:47.897000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:47.917000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:48.004000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:48.054000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:48.074000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:48.105000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:48.137000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:48.163000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:48.196000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:48.279000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:48.325000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:48.341000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:48.373000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:48.408000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:48.433000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:48.469000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:48.556000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:48.592000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:48.608000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:48.640000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:48.680000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:48.705000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:48.741000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:48.821000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:48.828000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/248_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:48.860000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:48.876000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:48.908000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:48.952000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:48.973000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:49.008000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:49.084000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:49.095000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/249_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:49.130000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:49.145000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:49.177000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:49.225000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:49.240000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:49.275000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:49.347000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:49.363000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/250_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:49.397000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:49.413000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:49.445000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:49.501000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:49.511000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:49.543000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:49.611000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:49.631000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/251_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:49.665000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:49.681000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:49.713000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:49.774000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:49.783000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:49.815000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:49.887000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:49.907000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/252_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:49.933000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:49.949000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:49.981000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:50.044000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:50.057000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:50.089000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:50.153000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:50.176000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/253_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:50.204000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:50.216000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:50.248000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:50.317000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:50.329000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:50.361000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:50.417000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:50.444000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/254_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:50.472000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:50.484000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:50.516000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:50.588000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:50.597000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:50.629000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:50.681000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:50.708000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/255_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:50.744000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:50.752000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:50.784000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:50.860000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:50.867000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:50.897000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:50.945000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:50.972000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/256_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:51.016000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:51.024000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:51.052000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:51.132000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:51.139000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:51.165000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:51.209000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:51.240000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/257_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:51.292000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:51.299000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:51.320000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:51.409000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:51.416000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:51.437000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:51.473000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:51.503000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/258_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:51.566000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:51.575000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:51.593000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:51.687000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:51.696000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:51.716000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:51.743000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:51.776000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/259_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:51.841000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:51.850000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:51.865000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:51.971000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:51.980000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:51.999000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:52.019000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:52.047000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/260_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:52.112000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:52.121000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:52.136000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:52.245000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:52.253000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:52.277000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:52.286000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:52.316000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/261_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:52.384000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:52.391000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:52.408000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:52.516000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:52.530000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:52.552000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:52.558000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:52.583000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/262_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:52.657000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:52.666000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:52.689000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:52.787000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:52.805000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:52.823000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:52.829000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:52.851000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/263_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:52.929000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:52.937000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:52.961000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:53.063000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:53.082000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:53.095000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:53.102000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:53.119000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/264_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:53.201000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:53.211000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:53.233000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:53.339000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:53.361000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:53.367000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:53.375000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:53.387000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/265_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:53.477000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:53.485000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:53.509000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:53.611000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:53.635000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:53.644000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:53.652000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:53.657000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/266_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:53.753000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:53.762000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:53.785000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:53.883000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:53.903000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:53.922000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:53.928000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:53.934000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/267_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:54.025000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:54.034000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:54.057000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:54.161000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:54.173000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:54.196000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:54.204000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/268_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:54.212000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:54.297000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:54.304000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:54.328000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:54.436000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:54.445000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:54.472000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:54.479000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/269_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:54.487000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:54.570000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:54.582000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:54.601000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:54.707000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:54.714000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:54.749000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:54.755000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/270_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:54.762000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:54.841000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:54.854000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:54.873000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:54.980000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:54.986000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:55.025000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:55.030000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/271_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:55.038000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:55.113000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:55.125000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:55.144000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:55.257000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:55.265000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:55.300000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:55.306000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/272_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:55.315000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:55.384000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:55.397000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:55.416000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:55.529000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:55.537000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:55.578000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:55.584000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/273_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:55.590000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:55.657000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:55.670000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:55.689000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:55.799000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:55.806000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:55.853000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:55.859000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/274_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:55.865000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:55.929000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:55.942000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:55.961000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:56.076000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:56.084000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:56.129000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:56.135000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/275_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:56.202000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:56.214000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:56.233000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:56.355000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:56.362000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:56.406000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:56.411000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/276_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:56.474000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:56.486000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:56.617000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:56.625000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:56.636000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:56.684000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:56.690000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/277_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:56.752000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:56.760000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:56.893000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:56.901000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:56.911000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:56.964000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/278_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:57.020000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:57.028000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:57.035000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:57.161000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:57.181000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:57.190000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:57.244000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/279_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:57.296000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:57.304000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:57.311000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:57.429000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:57.457000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:57.464000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:57.471000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:57.512000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/280_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:57.572000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:57.580000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:57.587000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:57.701000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:57.739000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:57.745000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:57.789000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/281_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:57.848000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:57.855000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:57.863000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:57.989000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:58.024000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:58.031000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:58.064000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/282_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:58.124000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:58.131000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:58.138000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:58.185000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:58.261000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:58.304000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:58.312000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:58.336000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/283_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:58.400000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:58.407000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:58.457000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:58.533000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:58.589000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:58.596000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:58.608000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/284_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:58.676000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:58.683000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:58.729000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:58.801000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:58.865000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:58.876000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:58.882000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/285_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:58.920000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:58.952000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:59.005000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:59.073000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:59.141000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:59.152000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/286_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:59.160000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:59.196000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:59.228000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:59.281000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:59.426000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:59.432000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/287_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:59.440000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:59.468000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:59.474000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:59.500000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:59.561000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:19:59.721000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:19:59.728000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:19:59.734000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/288_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:19:59.742000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:19:59.749000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:19:59.772000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:19:59.801000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:19:59.837000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:00.008000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:00.017000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:00.023000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:00.030000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:00.052000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:00.085000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:00.125000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:00.299000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:00.305000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:00.313000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:00.361000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:00.401000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:00.461000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/289_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:00.581000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:00.587000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:00.594000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:00.634000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:00.676000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:00.740000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/290_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:00.874000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:00.881000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:00.890000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:00.911000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:00.955000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:01.019000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/291_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:01.153000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:01.160000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:01.168000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:01.187000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:01.231000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:01.297000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/292_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:01.432000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:01.438000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:01.445000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:01.461000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:01.516000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:01.584000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/293_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:01.713000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:01.720000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:01.728000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:01.789000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:01.856000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/294_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:01.988000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:02.001000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:02.010000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:02.133000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/295_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:02.272000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:02.289000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:02.417000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/296_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:02.548000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:02.570000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:02.701000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/297_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:02.828000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:02.859000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:02.980000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/298_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:03.134000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:03.252000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/299_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:03.404000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:03.524000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/300_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:03.810000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/301_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:04.100000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/302_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:04.379000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/303_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:04.650000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/304_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0606 ms 100.0% 
  triton_mm_474 0.1041 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1042 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1044 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1045 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_493 0.1083 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1084 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_482 0.1119 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1119 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1397 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0694 seconds and 0.1863 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_475 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_484 0.1042 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1043 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1083 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1083 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1117 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1118 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1397 ms 43.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0469 seconds and 0.1892 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_474 0.1040 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1041 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1043 ms 58.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1086 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1086 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_483 0.1120 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1120 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_479 0.1399 ms 43.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1062 seconds and 0.1857 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0608 ms 100.0% 
  triton_mm_474 0.1041 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1041 ms 58.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1043 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1043 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1085 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1085 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_483 0.1119 ms 54.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1120 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_479 0.1398 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.9970 seconds and 0.1893 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_475 0.1041 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1042 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1044 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1045 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1084 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1086 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_482 0.1116 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1118 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_479 0.1401 ms 43.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1022 seconds and 0.2061 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0607 ms 100.0% 
  triton_mm_474 0.1039 ms 58.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1041 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_484 0.1042 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_485 0.1042 ms 58.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1086 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1087 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_483 0.1118 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_482 0.1119 ms 54.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_479 0.1396 ms 43.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0760 seconds and 0.1916 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_475 0.1010 ms 59.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_474 0.1011 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_485 0.1012 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1013 ms 59.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1050 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1051 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_482 0.1078 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1080 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_479 0.1320 ms 45.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0634 seconds and 0.2078 seconds precompiling for 25 choices
AUTOTUNE mm(8x7168, 7168x16160)
  mm 0.0605 ms 100.0% 
  triton_mm_474 0.1043 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_475 0.1043 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_485 0.1045 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_484 0.1046 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_493 0.1090 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_492 0.1092 ms 55.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_482 0.1120 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_483 0.1124 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_478 0.1402 ms 43.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0971 seconds and 0.1887 seconds precompiling for 25 choices
Capturing batches (bs=8 avail_mem=39.64 GB):  94%|| 49/52 [07:34<03:19, 66.42s/it]Capturing batches (bs=4 avail_mem=39.02 GB):  94%|| 49/52 [07:34<03:19, 66.42s/it][rank0]:W1021 00:20:17.617000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:17.643000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:17.697000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:17.725000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:17.806000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:17.813000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:17.837000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:17.893000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:18.004000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:18.043000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:18.122000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:18.148000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:18.227000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:18.232000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:18.337000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:18.387000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:18.467000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:18.576000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:18.614000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:18.693000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:18.803000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:18.826000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:18.905000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/25_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:19.014000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/25] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:19.358000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:19.387000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:19.559000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:19.832000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:19.874000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:20.188000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:20.371000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:20.588000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:21.295000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:21.375000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:21.416000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:21.427000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:21.496000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:21.547000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:21.796000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:21.876000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:21.929000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:21.939000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:22.019000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:22.071000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:22.113000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:22.131000 295 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank0]:W1021 00:20:22.136000 288 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank3]:W1021 00:20:22.196000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:22.256000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:22.406000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:22.407000 292 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank1]:W1021 00:20:22.490000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:22.547000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:22.547000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:22.560000 294 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank2]:W1021 00:20:22.633000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:22.685000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:22.703000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:22.767000 291 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank5]:W1021 00:20:22.768000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/26_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:22.821000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/26] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:23.056000 289 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank2]:W1021 00:20:23.183000 290 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
[rank5]:W1021 00:20:23.297000 293 torch/_inductor/utils.py:1349] [28/5] Please pip install Composable Kernel package
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_500 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  bmm 0.0064 ms 99.4% 
  triton_bmm_497 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_498 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_501 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_499 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_508 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_496 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_502 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_503 0.0065 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8836 seconds and 0.1556 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_503 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_515 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 99.4% 
  triton_bmm_513 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_519 0.0064 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_501 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_507 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_512 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_502 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_506 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9132 seconds and 0.1557 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_513 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_517 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_501 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_511 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_515 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_509 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_499 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_503 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_514 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8911 seconds and 0.1561 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_512 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_500 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_508 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_514 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_516 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_518 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_502 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_506 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_517 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 98.7% 
SingleProcess AUTOTUNE benchmarking takes 4.7715 seconds and 0.1554 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_497 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_519 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_499 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_509 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_510 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_512 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  bmm 0.0063 ms 99.3% 
  triton_bmm_500 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_511 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_513 0.0063 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8401 seconds and 0.1665 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  bmm 0.0064 ms 100.0% 
  triton_bmm_516 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_513 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_515 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_496 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_501 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_502 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_503 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_509 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_517 0.0065 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8105 seconds and 0.1526 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_500 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_501 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_513 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_502 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_514 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_503 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_506 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_512 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  bmm 0.0063 ms 98.1% 
  triton_bmm_498 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8552 seconds and 0.1531 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x128, 16x128x512)
  triton_bmm_517 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_508 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_496 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_498 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_499 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_501 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_502 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_507 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_519 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_506 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8992 seconds and 0.1506 seconds precompiling for 25 choices
[rank4]:W1021 00:20:32.677000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:32.764000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:33.159000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:33.195000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:33.279000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:33.576000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:33.599000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:33.683000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:33.761000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:33.948000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:34.090000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:34.114000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:34.286000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:34.474000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:34.564000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/5_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:35.085000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/305_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:35.732000 292 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank7]:W1021 00:20:35.737000 295 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank5]:W1021 00:20:36.434000 293 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank0]:W1021 00:20:36.587000 288 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank6]:W1021 00:20:37.002000 294 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank3]:W1021 00:20:37.366000 291 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank1]:W1021 00:20:37.375000 289 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
[rank2]:W1021 00:20:37.964000 290 torch/_inductor/utils.py:1349] [40/5_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_522 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.8% 
  triton_bmm_523 0.0065 ms 97.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_533 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0067 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_530 0.0069 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0069 ms 91.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_527 0.0078 ms 81.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_526 0.0079 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_543 0.0081 ms 78.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8181 seconds and 0.4215 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_522 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_533 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_530 0.0071 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 90.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0078 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0078 ms 82.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_524 0.0083 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8298 seconds and 0.4209 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_523 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_522 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 96.9% 
  triton_bmm_532 0.0069 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0069 ms 89.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_531 0.0077 ms 80.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0078 ms 79.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0078 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0078 ms 79.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_543 0.0084 ms 73.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8826 seconds and 0.4260 seconds precompiling for 25 choices
[rank7]:W1021 00:20:41.954000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0065 ms 100.0% 
  triton_bmm_522 0.0066 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_523 0.0066 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_532 0.0067 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0068 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0071 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 92.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_527 0.0081 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_526 0.0081 ms 79.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_542 0.0083 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9258 seconds and 0.4120 seconds precompiling for 25 choices
[rank7]:W1021 00:20:42.033000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:42.042000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:42.084000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:42.122000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:42.173000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_523 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_522 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.1% 
  triton_bmm_532 0.0068 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0068 ms 92.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0069 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0071 ms 87.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_526 0.0079 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0079 ms 79.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_542 0.0083 ms 75.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8597 seconds and 0.4415 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_523 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_522 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_532 0.0066 ms 97.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_531 0.0069 ms 92.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0070 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_526 0.0078 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_527 0.0078 ms 82.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_524 0.0083 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8398 seconds and 0.4275 seconds precompiling for 25 choices
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_523 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 97.5% 
  triton_bmm_522 0.0064 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_533 0.0066 ms 94.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_532 0.0066 ms 94.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_530 0.0069 ms 90.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_531 0.0070 ms 89.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_527 0.0078 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_526 0.0078 ms 80.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_524 0.0081 ms 77.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8763 seconds and 0.4149 seconds precompiling for 25 choices
[rank5]:W1021 00:20:42.814000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:42.894000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:42.945000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:43.027000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:43.105000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:43.156000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x4x512, 16x512x128)
  triton_bmm_523 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_522 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.8% 
  triton_bmm_532 0.0066 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_533 0.0066 ms 95.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_531 0.0072 ms 88.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_530 0.0072 ms 87.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_527 0.0079 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_526 0.0079 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_542 0.0081 ms 78.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8979 seconds and 0.4238 seconds precompiling for 25 choices
[rank6]:W1021 00:20:43.394000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:43.474000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:43.525000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:43.758000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:43.837000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:43.887000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:44.020000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:44.098000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:44.148000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:44.793000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:44.871000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/27_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:44.922000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/27] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:47.670000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:47.751000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:47.865000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:47.920000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:48.000000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:48.113000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:48.794000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:48.843000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:48.873000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:48.923000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:48.995000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:49.034000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:49.152000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:49.232000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:49.346000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:49.625000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:49.705000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:49.818000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:49.969000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:50.047000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:50.156000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:50.553000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:50.632000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/28_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:50.741000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/28] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:52.133000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:52.409000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:52.607000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:52.680000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:52.887000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:53.168000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:53.713000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:53.792000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:53.989000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:54.080000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:54.108000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:54.138000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:54.219000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:54.261000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:20:54.325000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:54.352000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:54.397000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:54.404000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:54.677000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:54.685000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:54.765000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:54.950000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:55.049000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:55.096000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:55.177000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:20:55.282000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:55.333000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/306_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:55.340000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:55.610000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/307_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:55.706000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:55.793000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:55.888000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/308_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:20:55.901000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:55.961000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:56.039000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:20:56.142000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:56.174000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:56.254000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:20:56.369000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:56.415000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:56.496000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:20:56.609000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:56.675000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:56.755000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:20:56.868000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:57.385000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:57.466000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/29_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:20:57.570000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/29] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_547 0.0290 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_556 0.0296 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0306 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_546 0.0308 ms 29.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0452 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0453 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0480 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0480 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0536 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0554 seconds and 0.2273 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_547 0.0283 ms 31.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_557 0.0294 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0295 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_546 0.0301 ms 30.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0450 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0459 ms 19.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0478 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0478 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_551 0.0536 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1183 seconds and 0.2254 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_547 0.0285 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_556 0.0298 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0299 ms 30.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_546 0.0319 ms 28.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0452 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0453 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_564 0.0484 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0485 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0539 ms 17.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0077 seconds and 0.2367 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_547 0.0318 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_557 0.0325 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0326 ms 27.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_546 0.0330 ms 27.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_565 0.0481 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0482 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_554 0.0484 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0489 ms 18.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_550 0.0585 ms 15.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1875 seconds and 0.2241 seconds precompiling for 25 choices
[rank4]:W1021 00:21:02.062000 292 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5ad0>
[rank4]:W1021 00:21:02.085000 292 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5680>
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_547 0.0296 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_556 0.0297 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0305 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_546 0.0307 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0449 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0449 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0481 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0481 ms 18.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_551 0.0537 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1315 seconds and 0.2269 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:21:02 TP4] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0093 ms 100.0% 
  triton_mm_547 0.0289 ms 32.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_557 0.0294 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0294 ms 31.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_546 0.0312 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0449 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0449 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0478 ms 19.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0479 ms 19.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_551 0.0535 ms 17.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0514 seconds and 0.2427 seconds precompiling for 25 choices
[rank7]:W1021 00:21:02.375000 295 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b9542b80>
[rank7]:W1021 00:21:02.398000 295 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b95414a0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:21:02 TP7] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0095 ms 100.0% 
  triton_mm_547 0.0284 ms 33.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_557 0.0295 ms 32.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_556 0.0295 ms 32.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_546 0.0300 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_554 0.0450 ms 21.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_555 0.0455 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0477 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0477 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_550 0.0537 ms 17.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0942 seconds and 0.2273 seconds precompiling for 25 choices
[rank0]:W1021 00:21:02.837000 288 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26ec40>
[rank0]:W1021 00:21:02.860000 288 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26de00>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:21:02 TP0] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1021 00:21:03.248000 293 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568dbf0>
[rank5]:W1021 00:21:03.271000 293 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568db90>
AUTOTUNE mm(4x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_556 0.0297 ms 30.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_557 0.0310 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_547 0.0345 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_546 0.0346 ms 26.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_555 0.0449 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_554 0.0451 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_565 0.0480 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_564 0.0481 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_566 0.0613 ms 14.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 5.1123 seconds and 0.2323 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:21:03 TP5] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1021 00:21:03.616000 291 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4639884ed0>
[rank3]:W1021 00:21:03.640000 291 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f46398855c0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:21:03 TP3] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1021 00:21:03.909000 289 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38790717a0>
[rank1]:W1021 00:21:03.932000 289 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3879071da0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:21:03 TP1] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank7]:W1021 00:21:04.210000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:04.345000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:04.354000 294 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d9b0>
[rank7]:W1021 00:21:04.487000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:04.607000 290 torch/_dynamo/variables/builtin.py:1091] [72/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d560>
[rank4]:W1021 00:21:04.628000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:04.630000 290 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d4d0>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:21:04 TP2] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1021 00:21:04.914000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:04.964000 294 torch/_dynamo/variables/builtin.py:1091] [73/5] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d950>
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:21:05 TP6] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1021 00:21:05.064000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:05.194000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:05.348000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:05.357000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:05.365000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:05.474000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:05.564000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:05.631000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:05.640000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:05.647000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:05.761000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:05.845000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:05.909000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:05.918000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:05.931000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:06.029000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:06.040000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:06.122000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:06.185000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:06.196000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:06.210000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:06.308000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:06.322000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:06.400000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:06.463000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:06.474000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:06.490000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:06.499000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:06.588000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:06.605000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:06.680000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:06.686000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/309_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:06.751000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:06.761000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:06.770000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:06.782000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:06.878000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:06.885000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:06.961000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:06.969000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/310_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:07.033000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:07.040000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:07.049000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:07.061000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:07.158000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:07.165000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:07.242000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:07.250000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/311_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:07.313000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:07.322000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:07.328000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:07.340000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:07.438000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:07.445000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:07.521000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:07.529000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/312_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:07.594000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:07.602000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:07.609000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:07.620000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:07.717000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:07.724000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:07.802000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:07.810000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/313_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:07.872000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:07.882000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:07.892000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:07.902000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:08.001000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:08.008000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:08.080000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:08.087000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/314_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:08.152000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:08.162000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:08.171000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:08.182000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:08.277000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:08.292000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:08.372000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:08.378000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/315_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:08.442000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:08.449000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:08.457000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:08.467000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:08.554000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:08.572000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:08.664000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:08.670000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/316_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:08.722000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:08.738000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:08.745000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:08.753000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:08.833000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:08.856000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:08.948000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:08.954000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/317_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:09.002000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:09.018000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:09.024000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:09.035000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:09.114000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:09.136000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:09.228000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:09.234000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/318_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:09.282000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:09.298000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:09.305000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:09.318000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:09.394000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:09.416000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:09.508000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:09.514000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/319_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:09.561000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:09.574000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:09.583000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:09.597000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:09.669000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:09.697000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:09.789000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/320_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:09.797000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:09.840000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:09.852000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:09.861000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:09.876000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:09.950000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:09.976000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:10.064000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/321_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:10.080000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:10.119000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:10.130000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:10.137000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:10.158000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:10.230000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:10.264000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:10.353000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/322_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:10.373000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:10.396000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:10.404000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:10.425000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:10.437000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:10.508000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:10.557000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:10.629000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/323_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:10.654000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:10.673000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:10.680000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:10.701000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:10.716000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:10.784000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:10.833000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:10.909000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/324_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:10.929000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:10.952000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:10.959000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:10.977000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:10.997000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:11.060000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:11.113000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:11.185000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/325_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:11.205000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:11.229000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:11.236000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:11.252000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:11.277000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:11.338000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:11.392000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:11.460000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/326_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:11.480000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:11.510000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:11.518000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:11.531000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:11.558000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:11.618000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:11.672000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:11.740000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/327_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:11.760000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:11.790000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:11.799000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:11.811000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:11.838000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:11.894000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:11.953000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:12.025000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/328_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:12.049000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:12.068000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:12.076000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:12.101000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:12.117000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:12.168000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:12.233000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:12.301000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/329_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:12.325000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:12.348000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:12.355000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:12.377000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:12.397000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:12.444000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:12.513000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:12.577000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/330_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:12.602000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:12.624000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:12.632000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:12.653000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:12.677000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:12.720000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:12.793000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:12.853000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/331_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:12.878000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:12.904000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:12.911000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:12.929000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:12.957000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:12.996000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:13.073000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:13.129000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/332_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:13.153000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:13.180000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:13.188000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:13.205000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:13.237000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:13.272000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:13.354000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:13.406000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/333_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:13.434000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:13.461000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:13.468000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:13.481000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:13.517000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:13.548000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:13.645000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:13.693000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/334_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:13.721000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:13.740000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:13.748000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:13.768000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:13.797000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:13.824000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:13.926000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:13.969000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/335_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:13.998000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:14.024000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:14.031000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:14.049000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:14.077000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:14.100000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:14.205000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:14.257000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/336_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:14.285000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:14.304000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:14.310000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:14.337000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:14.357000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:14.376000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:14.486000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:14.534000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/337_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:14.561000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:14.584000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:14.590000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:14.612000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:14.637000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:14.652000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:14.766000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:14.809000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/338_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:14.838000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:14.864000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:14.871000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:14.889000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:14.917000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:14.928000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:15.046000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:15.085000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/339_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:15.113000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:15.144000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:15.151000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:15.165000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:15.201000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:15.209000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:15.325000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:15.361000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/340_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:15.390000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:15.424000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:15.430000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:15.445000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:15.485000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:15.491000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:15.605000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:15.637000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/341_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:15.669000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:15.704000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:15.710000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:15.721000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:15.768000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:15.776000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:15.885000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:15.914000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/342_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:15.950000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:15.984000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:15.990000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:15.999000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:16.048000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:16.057000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:16.177000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:16.193000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/343_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:16.229000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:16.264000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:16.270000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:16.279000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:16.328000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:16.337000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:16.469000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:16.481000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/344_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:16.521000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:16.544000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:16.550000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:16.564000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:16.608000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:16.625000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:16.750000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:16.758000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/345_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:16.801000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:16.825000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:16.831000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:16.841000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:16.893000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:16.905000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:17.033000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/346_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:17.042000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:17.081000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:17.106000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:17.116000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:17.122000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:17.174000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:17.194000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:17.312000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/347_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:17.320000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:17.360000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:17.390000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:17.400000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:17.405000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:17.458000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:17.479000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:17.592000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/348_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:17.599000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:17.640000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:17.674000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:17.682000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:17.689000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:17.738000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:17.766000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:17.872000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/349_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:17.883000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:17.920000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:17.958000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:17.966000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:17.973000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:18.019000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:18.054000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:18.161000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/350_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:18.177000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:18.214000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:18.236000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:18.244000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:18.265000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:18.300000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:18.337000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:18.441000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/351_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:18.457000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:18.494000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:18.517000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:18.524000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:18.545000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:18.580000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:18.621000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:18.722000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/352_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:18.737000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:18.773000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:18.800000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:18.807000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:18.833000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:18.860000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:18.905000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:19.002000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/353_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:19.017000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:19.053000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:19.080000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:19.087000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:19.113000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:19.140000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:19.188000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:19.281000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/354_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:19.297000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:19.334000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:19.360000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:19.367000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:19.393000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:19.420000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:19.472000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:19.560000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/355_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:19.576000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:19.612000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:19.642000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:19.650000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:19.671000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:19.702000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:19.758000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:19.840000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/356_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:19.855000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:19.892000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:19.926000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:19.934000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:19.951000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:19.986000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:20.042000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:20.120000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/357_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:20.140000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:20.176000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:20.210000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:20.218000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:20.234000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:20.326000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:20.400000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/358_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:20.432000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:20.468000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:20.494000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:20.502000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:20.527000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:20.610000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:20.680000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/359_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:20.715000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:20.753000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:20.782000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:20.812000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:20.899000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:20.976000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/360_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:21.008000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:21.041000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:21.070000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:21.190000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:21.260000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/361_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:21.296000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:21.333000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:21.479000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:21.544000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/362_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:21.587000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:21.770000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:21.828000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/363_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:21.878000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:22.056000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:22.113000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/364_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:22.350000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:22.396000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/365_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_570 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_581 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_571 0.1041 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_580 0.1041 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_588 0.1079 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1080 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1112 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1112 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_574 0.1323 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1184 seconds and 0.1901 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_570 0.1041 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1042 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_581 0.1043 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_580 0.1044 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_588 0.1080 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1080 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1112 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1112 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_575 0.1326 ms 45.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1347 seconds and 0.2096 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0598 ms 100.0% 
  triton_mm_570 0.1043 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1044 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_580 0.1045 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1046 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_589 0.1080 ms 55.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1080 ms 55.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_578 0.1112 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1113 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_574 0.1324 ms 45.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1092 seconds and 0.2031 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0603 ms 100.0% 
  triton_mm_570 0.1040 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_581 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_571 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_580 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_588 0.1079 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1080 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1112 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1114 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_574 0.1321 ms 45.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1255 seconds and 0.1941 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_580 0.1041 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_571 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_570 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_589 0.1078 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1078 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_578 0.1114 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1116 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_575 0.1324 ms 45.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1128 seconds and 0.1896 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_570 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_580 0.1042 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1042 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_589 0.1082 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1083 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1114 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1114 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_574 0.1325 ms 45.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1408 seconds and 0.1941 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0604 ms 100.0% 
  triton_mm_580 0.1040 ms 58.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_570 0.1040 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_581 0.1041 ms 58.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1084 ms 55.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1086 ms 55.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1114 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1114 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_574 0.1328 ms 45.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1205 seconds and 0.1926 seconds precompiling for 25 choices
AUTOTUNE mm(4x7168, 7168x16160)
  mm 0.0599 ms 100.0% 
  triton_mm_570 0.1010 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_571 0.1010 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_580 0.1011 ms 59.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_581 0.1011 ms 59.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_588 0.1045 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_589 0.1047 ms 57.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_578 0.1069 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_579 0.1070 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_574 0.1255 ms 47.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0795 seconds and 0.1950 seconds precompiling for 25 choices
Capturing batches (bs=4 avail_mem=39.02 GB):  96%|| 50/52 [08:52<02:19, 69.86s/it]Capturing batches (bs=2 avail_mem=38.42 GB):  96%|| 50/52 [08:52<02:19, 69.86s/it][rank2]:W1021 00:21:35.450000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:35.530000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:35.639000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:35.796000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:35.875000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:35.984000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:36.027000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:36.108000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:36.220000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:36.265000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:36.294000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:36.347000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:36.374000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:36.389000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:36.413000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:36.417000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:36.461000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:36.472000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:36.485000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:36.495000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:36.498000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/30_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:36.583000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:36.611000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:36.614000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/30] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:37.239000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:37.589000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:37.821000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:38.032000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:38.049000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:38.139000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:38.175000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:38.210000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:38.781000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:38.861000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:38.913000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:39.363000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:39.447000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:39.501000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:39.624000 290 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank0]:W1021 00:21:39.694000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:39.728000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:39.785000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:39.816000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:39.824000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:39.842000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:39.857000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:39.873000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:39.908000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:39.939000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:39.960000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:39.990000 292 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank3]:W1021 00:21:39.991000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:40.147000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:40.180000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:40.231000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:40.264000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/31_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:40.284000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:40.316000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/31] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:40.333000 288 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank6]:W1021 00:21:40.367000 294 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank1]:W1021 00:21:40.447000 289 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank3]:W1021 00:21:40.481000 291 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank5]:W1021 00:21:40.775000 293 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
[rank7]:W1021 00:21:40.813000 295 torch/_inductor/utils.py:1349] [28/6] Please pip install Composable Kernel package
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_595 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0063 ms 99.4% 
  triton_bmm_604 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_605 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_613 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_614 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_596 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_615 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_602 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_606 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8676 seconds and 0.1546 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_597 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_599 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_605 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_607 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_609 0.0062 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  bmm 0.0062 ms 99.4% 
  triton_bmm_595 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_611 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_613 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_598 0.0062 ms 99.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8903 seconds and 0.1467 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  bmm 0.0063 ms 100.0% 
  triton_bmm_611 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_599 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_605 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_609 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_610 0.0065 ms 97.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_595 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_596 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_598 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_602 0.0065 ms 96.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8853 seconds and 0.1574 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_605 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_595 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_597 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_603 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_607 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_599 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_609 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_611 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_613 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_615 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8628 seconds and 0.1554 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_611 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 100.0% 
  triton_bmm_607 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_608 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
  triton_bmm_609 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_613 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_594 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_598 0.0064 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_593 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_595 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8799 seconds and 0.1522 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_610 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_609 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_611 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  bmm 0.0063 ms 98.8% 
  triton_bmm_613 0.0063 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_602 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_604 0.0063 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_593 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_596 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_597 0.0064 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8626 seconds and 0.1585 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_595 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_592 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_594 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_603 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_609 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_610 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_615 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_604 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_596 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_598 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 4.8955 seconds and 0.1543 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x128, 16x128x512)
  triton_bmm_595 0.0061 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_594 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_615 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_592 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=16, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_613 0.0062 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_605 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_603 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_609 0.0062 ms 98.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
  triton_bmm_604 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_610 0.0063 ms 98.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9070 seconds and 0.1524 seconds precompiling for 25 choices
[rank2]:W1021 00:21:49.900000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:50.290000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:50.410000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:50.668000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:50.803000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:50.834000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:50.940000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:21:51.196000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:21:51.370000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:21:51.451000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:51.775000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:51.975000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:52.009000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/6_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:21:52.299000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:21:52.489000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:21:52.532000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/366_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:52.550000 290 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank4]:W1021 00:21:53.111000 292 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank1]:W1021 00:21:53.684000 289 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank0]:W1021 00:21:53.735000 288 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank6]:W1021 00:21:53.873000 294 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank5]:W1021 00:21:54.278000 293 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank3]:W1021 00:21:55.057000 291 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
[rank7]:W1021 00:21:55.152000 295 torch/_inductor/utils.py:1349] [40/6_1] Please pip install Composable Kernel package
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0063 ms 100.0% 
  triton_bmm_619 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0064 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_629 0.0066 ms 96.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0066 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0073 ms 86.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0074 ms 85.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_622 0.0079 ms 80.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0079 ms 80.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0080 ms 79.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8099 seconds and 0.4261 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_619 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0063 ms 99.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.1% 
  triton_bmm_628 0.0067 ms 94.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0067 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0070 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0070 ms 90.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0077 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0077 ms 81.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_620 0.0081 ms 77.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8827 seconds and 0.4152 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_618 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0065 ms 98.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_628 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0070 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0070 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0079 ms 81.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0079 ms 81.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_639 0.0081 ms 78.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.8525 seconds and 0.4146 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 99.4% 
  triton_bmm_619 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_629 0.0068 ms 94.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0068 ms 94.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0071 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0071 ms 89.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_622 0.0079 ms 81.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0079 ms 81.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0083 ms 77.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.7474 seconds and 0.4197 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_618 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_628 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0069 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0070 ms 90.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_623 0.0078 ms 81.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_622 0.0079 ms 81.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_620 0.0081 ms 78.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.9374 seconds and 0.4228 seconds precompiling for 25 choices
[rank2]:W1021 00:21:59.197000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:59.276000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:21:59.326000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:59.492000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:59.573000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:21:59.624000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_618 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_619 0.0063 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  bmm 0.0064 ms 98.1% 
  triton_bmm_628 0.0067 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0067 ms 93.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0076 ms 83.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0077 ms 82.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_622 0.0078 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0078 ms 80.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_639 0.0083 ms 76.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9002 seconds and 0.4311 seconds precompiling for 25 choices
[rank1]:W1021 00:22:00.009000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:00.089000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:00.111000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:00.131000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:00.140000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:00.190000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:00.213000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:00.241000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:00.265000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE bmm(16x2x512, 16x512x128)
  triton_bmm_619 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0065 ms 98.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  bmm 0.0065 ms 98.2% 
  triton_bmm_628 0.0066 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_629 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_627 0.0069 ms 92.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_626 0.0069 ms 91.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_622 0.0078 ms 82.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0078 ms 82.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_638 0.0081 ms 78.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 4.9281 seconds and 0.4238 seconds precompiling for 25 choices
AUTOTUNE bmm(16x2x512, 16x512x128)
  bmm 0.0064 ms 100.0% 
  triton_bmm_619 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_618 0.0064 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_629 0.0067 ms 95.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_628 0.0067 ms 95.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_626 0.0070 ms 91.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_bmm_627 0.0072 ms 88.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_bmm_622 0.0076 ms 83.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_bmm_623 0.0077 ms 83.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_bmm_620 0.0082 ms 77.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 4.8664 seconds and 0.4189 seconds precompiling for 25 choices
[rank7]:W1021 00:22:01.382000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:01.428000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:01.463000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:01.497000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:01.509000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:01.515000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:01.561000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:01.578000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/32_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:01.629000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/32] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:05.026000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:05.104000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:05.214000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:05.372000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:05.453000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:05.567000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:06.079000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:06.152000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:06.162000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:06.183000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:06.232000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:06.263000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:06.279000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:06.343000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:06.374000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:07.395000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:07.420000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:07.480000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:07.485000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:07.501000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:07.562000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/33_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:07.596000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:07.614000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:07.675000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/33] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:09.828000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:10.119000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:10.336000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:10.401000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:10.624000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:10.921000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:10.990000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:10.998000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:11.165000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:11.290000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:11.297000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:11.459000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:11.580000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:11.588000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:11.730000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:11.746000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:11.808000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:11.913000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:12.249000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:12.329000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:12.345000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:12.358000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:12.409000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/367_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:12.433000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:12.634000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:12.649000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:12.702000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/368_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:12.918000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:12.937000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:12.954000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:12.992000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/369_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:13.037000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:13.068000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:13.082000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:13.141000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:13.147000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:13.164000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:13.250000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:13.272000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:14.246000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:14.320000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:14.326000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:14.400000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:14.430000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:14.505000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:14.576000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:14.656000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/34_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:14.759000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/34] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_653 0.0289 ms 31.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0308 ms 29.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0349 ms 26.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0350 ms 26.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_661 0.0446 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_651 0.0447 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0447 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_650 0.0451 ms 20.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_647 0.0583 ms 15.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0647 seconds and 0.2207 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0089 ms 100.0% 
  triton_mm_653 0.0292 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0293 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0301 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0311 ms 28.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_650 0.0441 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0441 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0444 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0444 ms 20.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_647 0.0539 ms 16.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0534 seconds and 0.2403 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_643 0.0287 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_653 0.0292 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0293 ms 31.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_642 0.0306 ms 29.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_651 0.0439 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0440 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_660 0.0444 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0447 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_647 0.0542 ms 16.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0732 seconds and 0.2244 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0092 ms 100.0% 
  triton_mm_653 0.0293 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0295 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0303 ms 30.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0323 ms 28.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_651 0.0442 ms 20.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0443 ms 20.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_660 0.0449 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0450 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_647 0.0539 ms 17.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0900 seconds and 0.2284 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_652 0.0294 ms 30.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0294 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_643 0.0303 ms 29.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0316 ms 28.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_650 0.0439 ms 20.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0439 ms 20.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_661 0.0447 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0447 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_647 0.0537 ms 16.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1259 seconds and 0.2244 seconds precompiling for 25 choices
[rank2]:W1021 00:22:19.291000 290 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d560>
[rank2]:W1021 00:22:19.314000 290 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d4d0>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:22:19 TP2] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1021 00:22:19.583000 292 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5ad0>
[rank4]:W1021 00:22:19.606000 292 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5680>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:22:19 TP4] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1021 00:22:19.983000 289 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38790717a0>
[rank1]:W1021 00:22:20.006000 289 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3879071da0>
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:22:20 TP1] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank0]:W1021 00:22:20.143000 288 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26ec40>
[rank0]:W1021 00:22:20.166000 288 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26de00>
[rank6]:W1021 00:22:20.181000 294 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d9b0>
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0090 ms 100.0% 
  triton_mm_652 0.0311 ms 29.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0321 ms 28.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_653 0.0322 ms 28.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_642 0.0329 ms 27.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_661 0.0475 ms 19.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0477 ms 18.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0482 ms 18.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0497 ms 18.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_647 0.0602 ms 15.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1455 seconds and 0.2320 seconds precompiling for 25 choices
[rank6]:W1021 00:22:20.204000 294 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d950>
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0089 ms 100.0% 
  triton_mm_653 0.0293 ms 30.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_652 0.0295 ms 30.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_643 0.0297 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_642 0.0303 ms 29.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_660 0.0443 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0443 ms 20.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_650 0.0444 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0445 ms 20.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_647 0.0544 ms 16.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1140 seconds and 0.2167 seconds precompiling for 25 choices
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:22:20 TP0] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:22:20 TP6] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
AUTOTUNE mm(2x7168, 7168x256)
  mm 0.0091 ms 100.0% 
  triton_mm_643 0.0285 ms 31.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_652 0.0290 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_653 0.0290 ms 31.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_642 0.0303 ms 29.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_650 0.0438 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_661 0.0439 ms 20.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_660 0.0439 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_651 0.0440 ms 20.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_647 0.0538 ms 16.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0406 seconds and 0.2294 seconds precompiling for 25 choices
[rank2]:W1021 00:22:20.962000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:21.242000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:21.336000 293 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568dbf0>
[rank5]:W1021 00:22:21.359000 293 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568db90>
[rank4]:W1021 00:22:21.424000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:22:21 TP5] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1021 00:22:21.522000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:21.549000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:21.684000 295 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b9542b80>
[rank7]:W1021 00:22:21.707000 295 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b95414a0>
[rank4]:W1021 00:22:21.708000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:22:21 TP7] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1021 00:22:21.802000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:21.811000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:21.833000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:21.877000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:21.922000 291 torch/_dynamo/variables/builtin.py:1091] [72/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4639884ed0>
[rank3]:W1021 00:22:21.945000 291 torch/_dynamo/variables/builtin.py:1091] [73/6] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f46398855c0>
[rank4]:W1021 00:22:21.994000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:22:22 TP3] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1021 00:22:22.080000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:22.092000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:22.112000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:22.165000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:22.282000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:22.360000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:22.376000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:22.402000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:22.450000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:22.566000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:22.650000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:22.674000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:22.697000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:22.738000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:22.744000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:22.852000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:22.940000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:22.957000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:22.981000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:23.031000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:23.039000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:23.138000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:23.220000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:23.240000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:23.264000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:23.319000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:23.327000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:23.422000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:23.508000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:23.524000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:23.552000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:23.607000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:23.616000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:23.707000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:23.815000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:23.822000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:23.840000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:23.899000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:23.908000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:23.929000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/370_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:24.107000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:24.115000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:24.195000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:24.204000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:24.221000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/371_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:24.395000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:24.487000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:24.495000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:24.503000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:24.521000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/372_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:24.683000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:24.692000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:24.783000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:24.790000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:24.812000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/373_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:24.832000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:24.975000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:24.985000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:25.072000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:25.079000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:25.100000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/374_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:25.119000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:25.270000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:25.279000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:25.360000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:25.368000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:25.388000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/375_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:25.407000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:25.495000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:25.562000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:25.571000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:25.644000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:25.656000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:25.672000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/376_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:25.695000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:25.766000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:25.786000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:25.850000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:25.859000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:25.940000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:25.952000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:25.968000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/377_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:25.991000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:26.054000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:26.079000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:26.142000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:26.228000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:26.240000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:26.256000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:26.279000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:26.346000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:26.375000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:26.434000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:26.516000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:26.528000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:26.544000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:26.568000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:26.634000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:26.671000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:26.726000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:26.805000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:26.822000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:26.831000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/378_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:26.860000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:26.922000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:26.967000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:27.018000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:27.096000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:27.118000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/379_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:27.124000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:27.152000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:27.210000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:27.263000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:27.306000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:27.384000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:27.406000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:27.416000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:27.447000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:27.498000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:27.517000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/380_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:27.559000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:27.598000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:27.676000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:27.695000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:27.710000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:27.741000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:27.789000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:27.806000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/381_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:27.857000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:27.888000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:27.962000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:27.981000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:28.006000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:28.033000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:28.076000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:28.094000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/382_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:28.153000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:28.177000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:28.246000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:28.269000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:28.294000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:28.321000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:28.365000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:28.382000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/383_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:28.449000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:28.464000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:28.529000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:28.557000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:28.582000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:28.609000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:28.652000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:28.682000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/384_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:28.745000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:28.753000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:28.826000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:28.845000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:28.870000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:28.897000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:28.941000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:28.970000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/385_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:29.041000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:29.047000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:29.110000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:29.133000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:29.157000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:29.185000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:29.228000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:29.262000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/386_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:29.333000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:29.339000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:29.393000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:29.421000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:29.446000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:29.481000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:29.517000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:29.562000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/387_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:29.625000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:29.631000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:29.677000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:29.709000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:29.730000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:29.768000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:29.807000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:29.853000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/388_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:29.918000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:29.927000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:29.960000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:29.998000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:30.016000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:30.056000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:30.094000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:30.144000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/389_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:30.213000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:30.220000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:30.246000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:30.285000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:30.306000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:30.345000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:30.381000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:30.438000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/390_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:30.509000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:30.515000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:30.530000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:30.573000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:30.594000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:30.633000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:30.669000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:30.730000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/391_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:30.807000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:30.816000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:30.822000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:30.863000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:30.880000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:30.920000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:30.958000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:31.021000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/392_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:31.106000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:31.115000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:31.122000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:31.150000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:31.168000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:31.210000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:31.245000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:31.314000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/393_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:31.400000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:31.408000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:31.416000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:31.437000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:31.458000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:31.497000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:31.533000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:31.606000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/394_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:31.688000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:31.701000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:31.709000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:31.725000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:31.746000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:31.785000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:31.823000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:31.897000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/395_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:31.978000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:31.996000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:32.005000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:32.015000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:32.044000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:32.084000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:32.110000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:32.188000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/396_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:32.268000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:32.282000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:32.297000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:32.304000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:32.334000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:32.373000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:32.397000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:32.482000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/397_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:32.560000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:32.568000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:32.588000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:32.595000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:32.621000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:32.661000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:32.685000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:32.774000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/398_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:32.850000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:32.857000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:32.878000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:32.890000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:32.908000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:32.948000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:32.974000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:33.065000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/399_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:33.142000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:33.149000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:33.166000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:33.182000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:33.196000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:33.236000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:33.261000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:33.358000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/400_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:33.432000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:33.440000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:33.453000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:33.473000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:33.485000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:33.525000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:33.549000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:33.650000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/401_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:33.720000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:33.728000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:33.741000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:33.765000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:33.774000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:33.813000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:33.837000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:33.949000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/402_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:34.010000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:34.024000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:34.033000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:34.058000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:34.065000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:34.100000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:34.126000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:34.241000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/403_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:34.296000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:34.314000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:34.321000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:34.349000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:34.358000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:34.389000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:34.413000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:34.534000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/404_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:34.584000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:34.602000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:34.609000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:34.641000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:34.648000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:34.677000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:34.701000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:34.826000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/405_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:34.872000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:34.885000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:34.896000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:34.933000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:34.941000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:34.965000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:34.989000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:35.118000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/406_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:35.164000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:35.173000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:35.184000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:35.225000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:35.233000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:35.253000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:35.277000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:35.410000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/407_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:35.456000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:35.464000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:35.472000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:35.517000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:35.525000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:35.541000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:35.565000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:35.702000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/408_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:35.748000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:35.756000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:35.764000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:35.809000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:35.816000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:35.829000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:35.853000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:35.993000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/409_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:36.042000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:36.048000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:36.058000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:36.102000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:36.109000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:36.117000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:36.146000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:36.285000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/410_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:36.338000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:36.345000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:36.353000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:36.401000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:36.410000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:36.418000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:36.437000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:36.578000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/411_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:36.630000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:36.638000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:36.645000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:36.693000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:36.701000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:36.710000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:36.729000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:36.870000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/412_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:36.914000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:36.928000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:36.935000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:36.990000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:36.998000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:37.006000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:37.017000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:37.170000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/413_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:37.210000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:37.220000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:37.227000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:37.290000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:37.298000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:37.308000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:37.316000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:37.461000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/414_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:37.496000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:37.514000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:37.523000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:37.576000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:37.594000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:37.601000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:37.612000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:37.752000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/415_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:37.784000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:37.806000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:37.815000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:37.868000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:37.891000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:37.897000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:37.908000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:38.046000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/416_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:38.096000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:38.103000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:38.162000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:38.185000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:38.194000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:38.202000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:38.338000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/417_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:38.390000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:38.399000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:38.452000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:38.478000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:38.486000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:38.496000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:38.631000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/418_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:38.691000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:38.750000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:38.777000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:38.789000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:38.931000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/419_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:38.981000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:39.079000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:39.086000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:39.235000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/420_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:39.283000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:39.381000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:39.534000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/421_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:39.581000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:39.679000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:39.834000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/422_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:39.873000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:40.130000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/423_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:40.161000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:40.418000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/424_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:40.454000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:40.716000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/425_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:40.750000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:41.004000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/426_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0596 ms 100.0% 
  triton_mm_677 0.1010 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_667 0.1011 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_676 0.1011 ms 59.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_666 0.1011 ms 58.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_684 0.1030 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1032 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1069 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1069 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1217 ms 49.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0887 seconds and 0.2130 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0599 ms 100.0% 
  triton_mm_676 0.1039 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1039 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_666 0.1040 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1040 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_685 0.1063 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1063 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1108 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1108 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_671 0.1279 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0882 seconds and 0.2021 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0599 ms 100.0% 
  triton_mm_666 0.1042 ms 57.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1043 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_677 0.1043 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_676 0.1043 ms 57.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_684 0.1066 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1066 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1108 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1109 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_670 0.1282 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0600 seconds and 0.2091 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0601 ms 100.0% 
  triton_mm_666 0.1039 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1039 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_677 0.1039 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_676 0.1040 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_684 0.1063 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1064 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1108 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1111 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1284 ms 46.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0491 seconds and 0.2142 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0602 ms 100.0% 
  triton_mm_666 0.1040 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_676 0.1040 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1041 ms 57.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_667 0.1041 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_684 0.1066 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1066 ms 56.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1112 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1112 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_671 0.1284 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1022 seconds and 0.2053 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0600 ms 100.0% 
  triton_mm_677 0.1038 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_676 0.1038 ms 57.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_666 0.1039 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1040 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_684 0.1067 ms 56.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1067 ms 56.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1109 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1110 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_670 0.1278 ms 47.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1468 seconds and 0.2098 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0597 ms 100.0% 
  triton_mm_666 0.1041 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_676 0.1041 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_677 0.1041 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_667 0.1042 ms 57.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_684 0.1069 ms 55.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1069 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_675 0.1108 ms 53.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1110 ms 53.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_671 0.1281 ms 46.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.0997 seconds and 0.2144 seconds precompiling for 25 choices
AUTOTUNE mm(2x7168, 7168x16160)
  mm 0.0600 ms 100.0% 
  triton_mm_676 0.1041 ms 57.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_666 0.1042 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
  triton_mm_667 0.1042 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=2
  triton_mm_677 0.1042 ms 57.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_684 0.1070 ms 56.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_685 0.1072 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=16, BLOCK_N=128, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_674 0.1108 ms 54.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=4
  triton_mm_675 0.1112 ms 54.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=16, BLOCK_N=64, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=16, num_stages=2, num_warps=4
  triton_mm_670 0.1281 ms 46.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=16, BLOCK_N=32, B_PROLOGUE_CAST_TYPE=None, EVEN_K=True, GROUP_M=8, matrix_instr_nonkdim=0, num_stages=2, num_warps=2
SingleProcess AUTOTUNE benchmarking takes 5.1226 seconds and 0.2031 seconds precompiling for 25 choices
Capturing batches (bs=2 avail_mem=38.42 GB):  98%|| 51/52 [10:11<01:12, 72.53s/it]Capturing batches (bs=1 avail_mem=37.83 GB):  98%|| 51/52 [10:11<01:12, 72.53s/it][rank3]:W1021 00:22:53.642000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:53.725000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:53.841000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:53.856000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:53.937000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:54.049000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:54.081000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:54.101000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:54.145000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:54.148000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:54.161000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:54.180000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:54.229000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:54.229000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:54.237000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:54.267000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:54.271000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:54.291000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:54.319000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:54.341000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:54.344000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:54.350000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/35_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:54.432000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:54.462000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/35] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:55.334000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:55.557000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:55.790000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:55.802000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:55.849000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:55.856000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:55.944000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:55.951000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [25/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:56.728000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:56.811000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:22:56.920000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:57.083000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:57.166000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:57.246000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:22:57.273000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:57.331000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:57.331000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:57.350000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:57.414000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:57.433000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:22:57.443000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:57.505000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:22:57.525000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:22:57.546000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:57.589000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:57.641000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:22:57.696000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:57.724000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:22:57.833000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:57.864000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:57.949000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/36_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:22:58.058000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/36] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:02.406000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:02.934000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:03.237000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:03.243000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:03.494000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:03.648000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:03.755000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:03.764000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:03.941000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:04.009000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:04.018000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:04.162000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:04.455000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:04.525000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:05.074000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [33/7_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:05.590000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/427_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:06.305000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:06.390000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:06.442000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:06.683000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:06.764000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:06.816000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:06.943000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:06.951000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:07.024000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:07.034000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:07.077000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:07.088000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:08.036000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:08.052000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:08.117000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:08.132000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:08.170000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:08.184000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:08.473000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:08.556000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:08.610000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:09.252000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:09.334000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/37_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:09.387000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/37] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:11.698000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:11.780000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:11.800000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:11.842000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:11.879000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:11.888000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:11.893000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:11.923000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:11.968000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:11.990000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:12.035000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:12.080000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:13.126000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:13.182000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:13.207000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:13.262000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:13.320000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:13.374000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:13.442000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:13.523000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:13.635000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:14.290000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:14.371000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/38_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:14.481000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/38] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:15.290000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:15.345000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:15.421000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:15.582000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:15.642000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:15.650000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:15.717000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:15.870000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:15.930000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:15.941000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:16.013000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:16.215000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:16.661000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:16.957000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:16.998000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:17.198000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:17.249000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:17.278000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:17.290000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:17.382000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:17.440000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:17.518000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:17.582000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:17.602000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:17.713000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:17.733000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:17.754000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/428_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:17.875000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:17.955000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:18.024000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:18.050000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/429_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:18.060000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:18.172000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:18.250000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:18.345000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/430_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:18.353000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:18.590000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:18.671000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:18.777000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:18.779000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:18.861000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:18.967000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:19.511000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:19.530000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [12/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:19.593000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:19.610000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [14/39_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:19.701000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:19.714000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [18/39] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:21.002000 291 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4639884ed0>
[rank3]:W1021 00:23:21.025000 291 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f46398855c0>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:23:21 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1021 00:23:21.529000 294 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d9b0>
[rank6]:W1021 00:23:21.553000 294 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f5a6229d950>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:23:21 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank5]:W1021 00:23:22.004000 293 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568dbf0>
[rank5]:W1021 00:23:22.028000 293 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f55a568db90>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:23:22 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank1]:W1021 00:23:22.429000 289 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38790717a0>
[rank1]:W1021 00:23:22.452000 289 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f3879071da0>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:23:22 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1021 00:23:22.576000 290 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d560>
[rank2]:W1021 00:23:22.599000 290 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f4fd5d6d4d0>
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:23:22 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank3]:W1021 00:23:22.692000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:22.962000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:22.989000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:23.257000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:23.282000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:23.396000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:23.499000 288 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26ec40>
[rank0]:W1021 00:23:23.522000 288 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f2acb26de00>
[rank6]:W1021 00:23:23.554000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:23.579000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:23:23 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank4]:W1021 00:23:23.632000 292 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5ad0>
[rank4]:W1021 00:23:23.655000 292 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f9e59df5680>
[rank5]:W1021 00:23:23.698000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:23:23 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank6]:W1021 00:23:23.851000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:23.870000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:23.989000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:24.002000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:24.130000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:24.149000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:24.178000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:24.281000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:24.298000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:24.418000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:24.445000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:24.472000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:24.566000 295 torch/_dynamo/variables/builtin.py:1091] [72/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b9542b80>
[rank5]:W1021 00:23:24.576000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:24.589000 295 torch/_dynamo/variables/builtin.py:1091] [73/7] Failed to create UserFunctionVariable: expected FunctionType found builtin_function_or_method <built-in method __str__ of PyCapsule object at 0x7f38b95414a0>
[rank1]:W1021 00:23:24.590000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:23:24 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[rank2]:W1021 00:23:24.706000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:24.745000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:24.766000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:24.869000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:24.882000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:24.994000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:25.052000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:25.063000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:25.171000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:25.182000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:25.189000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:25.201000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:25.286000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:25.349000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:25.358000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:25.465000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:25.478000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:25.486000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:25.496000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:25.574000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:25.649000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:25.658000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:25.756000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:25.770000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:25.778000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:25.787000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:25.862000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:25.949000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:25.957000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:26.048000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:26.062000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:26.071000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:26.081000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:26.151000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:26.245000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:26.262000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:26.340000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:26.364000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:26.373000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:26.383000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:26.438000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:26.545000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:26.558000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:26.577000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/431_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:26.632000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:26.657000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:26.666000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:26.677000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:26.726000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:26.845000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:26.866000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:26.874000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/432_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:26.924000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:26.948000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:26.970000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:26.978000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:27.014000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:27.147000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:27.161000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:27.170000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/433_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:27.218000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:27.242000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:27.260000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:27.268000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:27.301000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:27.447000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:27.462000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/434_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:27.469000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:27.510000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:27.534000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:27.556000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:27.565000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:27.588000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:27.747000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:27.756000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/435_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:27.765000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:27.806000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:27.826000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:27.852000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:27.861000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:27.876000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:28.047000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:28.055000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/436_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:28.063000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:28.098000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:28.118000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:28.148000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:28.157000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:28.168000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:28.346000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/437_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:28.355000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:28.362000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:28.390000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:28.410000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:28.444000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:28.453000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:28.461000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:28.643000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/438_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:28.659000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:28.665000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:28.690000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:28.702000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:28.752000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:28.762000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:28.769000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:28.939000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/439_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:28.958000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:28.965000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:28.990000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:29.000000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:29.049000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:29.058000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:29.065000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:29.235000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/440_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:29.258000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:29.268000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:29.282000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:29.294000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:29.356000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:29.366000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:29.374000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:29.530000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/441_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:29.559000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:29.568000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:29.578000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:29.590000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:29.660000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:29.667000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:29.676000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:29.826000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/442_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:29.863000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:29.869000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:29.878000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:29.887000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:29.956000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:29.966000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:29.973000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:30.122000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/443_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:30.162000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:30.170000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:30.179000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:30.188000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:30.252000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:30.264000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:30.272000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:30.418000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/444_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:30.462000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:30.469000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:30.479000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:30.489000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:30.549000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:30.556000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:30.568000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:30.718000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/445_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:30.762000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:30.772000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:30.781000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:30.791000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:30.848000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:30.855000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:30.871000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:31.019000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/446_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:31.062000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:31.078000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:31.086000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:31.095000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:31.152000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:31.159000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:31.172000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:31.314000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/447_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:31.367000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:31.375000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:31.390000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:31.397000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:31.456000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:31.465000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:31.480000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:31.611000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/448_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:31.666000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:31.675000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:31.686000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:31.696000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:31.752000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:31.759000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:31.776000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:31.907000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/449_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:31.966000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:31.974000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:31.983000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:31.996000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:32.048000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:32.055000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:32.072000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:32.203000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/450_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:32.264000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:32.271000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:32.278000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:32.298000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:32.342000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:32.353000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:32.374000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:32.501000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/451_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:32.568000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:32.576000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:32.582000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:32.594000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:32.630000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:32.645000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:32.670000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:32.797000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/452_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:32.881000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:32.888000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:32.935000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:32.954000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:32.983000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:33.102000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/453_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:33.185000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:33.242000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:33.269000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:33.294000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:33.405000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/454_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:33.489000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:33.538000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:33.570000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:33.590000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:33.671000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:33.701000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/455_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:33.708000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:33.785000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:33.878000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:33.895000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:33.975000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:33.998000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/456_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:34.005000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:34.025000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:34.081000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:34.182000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:34.286000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:34.300000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/457_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:34.309000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:34.327000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:34.379000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:34.490000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:34.599000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/458_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:34.606000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:34.616000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:34.631000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:34.649000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:34.675000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:34.901000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/459_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:34.915000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:34.922000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:34.933000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:34.951000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:35.027000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:35.205000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/460_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:35.218000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:35.229000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:35.241000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:35.251000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:35.325000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:35.507000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/461_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:35.521000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:35.530000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:35.551000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:35.558000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:35.576000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:35.624000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:35.805000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/462_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:35.816000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:35.826000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:35.834000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:35.854000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:35.862000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:35.875000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:35.926000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:36.121000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:36.130000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:36.138000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:36.150000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:36.169000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:36.177000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:36.226000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:36.430000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:36.437000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:36.448000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:36.455000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:36.476000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:36.487000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:36.525000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:36.734000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:36.750000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:36.757000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:36.765000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:36.780000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:36.795000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:36.824000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:36.939000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/463_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:37.034000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:37.050000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:37.064000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:37.076000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:37.085000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:37.103000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:37.124000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:37.239000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/464_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:37.334000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:37.350000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:37.380000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:37.388000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:37.396000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:37.411000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:37.424000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:37.539000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/465_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:37.634000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:37.650000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:37.680000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:37.688000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:37.696000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:37.719000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:37.727000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:37.842000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/466_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:37.932000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:37.953000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:37.982000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:37.995000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:38.003000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:38.025000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:38.033000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:38.142000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/467_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:38.232000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:38.257000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:38.278000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:38.294000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:38.309000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:38.334000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:38.343000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:38.444000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/468_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:38.538000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:38.562000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:38.580000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:38.592000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:38.608000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:38.632000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:38.651000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:38.743000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/469_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:38.840000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:38.868000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:38.886000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:38.898000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:38.914000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:38.938000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:38.958000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:39.042000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/470_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:39.144000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:39.172000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:39.198000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:39.210000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:39.225000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:39.250000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:39.266000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:39.341000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/471_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:39.451000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:39.478000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:39.500000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:39.509000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:39.528000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:39.549000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:39.575000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:39.647000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/472_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:39.754000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:39.785000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:39.800000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:39.809000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:39.832000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:39.852000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:39.883000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:39.955000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/473_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:40.058000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:40.090000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:40.104000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:40.112000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:40.136000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:40.156000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:40.191000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:40.259000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/474_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:40.362000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:40.390000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:40.404000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank3]:W1021 00:23:40.413000 291 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:40.440000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:40.456000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:40.495000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:40.563000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/475_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:40.666000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:40.690000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:40.704000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:40.744000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:40.756000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank6]:W1021 00:23:40.799000 294 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:40.866000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/476_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:40.969000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank5]:W1021 00:23:40.989000 293 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:41.002000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:41.046000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:41.066000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:41.169000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/477_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:41.272000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:41.302000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:41.350000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:41.370000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:41.477000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/478_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:41.577000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank2]:W1021 00:23:41.602000 290 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:41.650000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank1]:W1021 00:23:41.670000 289 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:41.781000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/479_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:41.876000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:41.952000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:42.085000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/480_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:42.177000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:42.250000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:42.384000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/481_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:42.491000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:42.552000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:42.690000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/482_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank4]:W1021 00:23:42.790000 292 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank0]:W1021 00:23:42.852000 288 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:42.989000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/483_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:43.297000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/484_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:43.603000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/485_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:43.905000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/486_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
[rank7]:W1021 00:23:44.205000 295 torch/_functorch/_aot_autograd/autograd_cache.py:852] [35/487_1] Bypassing autograd cache due to: Cannot cache a graph with functional tensor
Capturing batches (bs=1 avail_mem=37.83 GB): 100%|| 52/52 [11:11<00:00, 68.75s/it]Capturing batches (bs=1 avail_mem=37.83 GB): 100%|| 52/52 [11:11<00:00, 12.91s/it]
[2025-10-21 00:23:50 TP0] Registering 6396 cuda graph addresses
[2025-10-21 00:23:51 TP2] Capture cuda graph end. Time elapsed: 673.44 s. mem usage=6.20 GB. avail mem=36.99 GB.
[2025-10-21 00:23:51 TP5] Capture cuda graph end. Time elapsed: 673.47 s. mem usage=6.18 GB. avail mem=37.15 GB.
[2025-10-21 00:23:51 TP7] Capture cuda graph end. Time elapsed: 672.28 s. mem usage=6.18 GB. avail mem=37.14 GB.
[2025-10-21 00:23:51 TP4] Capture cuda graph end. Time elapsed: 672.28 s. mem usage=6.13 GB. avail mem=37.11 GB.
[2025-10-21 00:23:51 TP1] Capture cuda graph end. Time elapsed: 673.53 s. mem usage=6.16 GB. avail mem=37.04 GB.
[2025-10-21 00:23:51 TP3] Capture cuda graph end. Time elapsed: 673.54 s. mem usage=6.15 GB. avail mem=37.04 GB.
[2025-10-21 00:23:51 TP0] Capture cuda graph end. Time elapsed: 673.58 s. mem usage=6.22 GB. avail mem=37.39 GB.
[2025-10-21 00:23:51 TP6] Capture cuda graph end. Time elapsed: 672.20 s. mem usage=6.15 GB. avail mem=37.17 GB.
[2025-10-21 00:23:51 TP0] max_total_num_tokens=971748, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=37.39 GB
[2025-10-21 00:23:52] INFO:     Started server process [43]
[2025-10-21 00:23:52] INFO:     Waiting for application startup.
[2025-10-21 00:23:52] INFO:     Application startup complete.
[2025-10-21 00:23:52] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-10-21 00:23:53] INFO:     127.0.0.1:43860 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-21 00:23:53 TP0] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-21 00:23:54] INFO:     127.0.0.1:54596 - "GET /get_model_info HTTP/1.1" 200 OK
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-10-21 00:23:55 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-10-21 00:23:55 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-10-21 00:23:55 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-10-21 00:23:55 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-10-21 00:23:55 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-10-21 00:23:55 TP5] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-10-21 00:23:55 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
[2025-10-21 00:23:56 TP7] start build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
[2025-10-21 00:24:03] INFO:     127.0.0.1:54600 - "GET /get_model_info HTTP/1.1" 200 OK
1 warning generated when compiling for host.
[92mSuccessfully preprocessed all matching files.[0m
[aiter] finish build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip], cost 56.66653457s
[2025-10-21 00:24:51 TP7] finish build [mha_varlen_fwd_bf16_nlogits_nbias_mask_nlse_ndropout_nskip], cost 56.66653457s
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:51 TP7] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:51 TP2] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:51 TP0] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:51 TP1] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:51 TP4] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:51 TP6] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:51 TP5] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:51 TP3] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:52 TP0] Prefill batch. #new-seq: 1, #new-token: 667, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:52 TP0] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:52 TP2] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:52 TP5] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:52 TP7] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:52 TP4] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:52 TP1] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:52 TP3] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:52 TP6] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:54] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:24:54 TP0] Prefill batch. #new-seq: 106, #new-token: 6686, #cached-token: 70702, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:54 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:54 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:54 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:54 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:54 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:54 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:54 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:54 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:24:54 TP0] Prefill batch. #new-seq: 68, #new-token: 4344, #cached-token: 45356, token usage: 0.01, #running-req: 107, #queue-req: 0, 
[2025-10-21 00:24:55 TP0] Prefill batch. #new-seq: 248, #new-token: 14585, #cached-token: 166028, token usage: 0.01, #running-req: 175, #queue-req: 0, 
[2025-10-21 00:24:55 TP0] Prefill batch. #new-seq: 177, #new-token: 10340, #cached-token: 118493, token usage: 0.03, #running-req: 423, #queue-req: 0, 
[2025-10-21 00:24:57 TP0] Prefill batch. #new-seq: 276, #new-token: 16354, #cached-token: 184861, token usage: 0.04, #running-req: 600, #queue-req: 290, 
[2025-10-21 00:24:58 TP0] Prefill batch. #new-seq: 148, #new-token: 9358, #cached-token: 99136, token usage: 0.05, #running-req: 876, #queue-req: 296, 
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP7] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP7] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP4] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP4] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP1] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP1] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP5] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP5] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP3] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP3] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP0] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP0] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP2] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP2] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP6] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP6] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-10-21 00:25:00 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:00 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-10-21 00:25:01] INFO:     127.0.0.1:43872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:01] The server is fired up and ready to roll!
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:01 TP3] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:01 TP1] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:01 TP5] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:01 TP7] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:01 TP4] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:01 TP0] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:01 TP2] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:01 TP6] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:01 TP0] Prefill batch. #new-seq: 1, #new-token: 40, #cached-token: 670, token usage: 0.07, #running-req: 1023, #queue-req: 295, 
[2025-10-21 00:25:03] INFO:     127.0.0.1:49296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:03 TP0] Prefill batch. #new-seq: 1, #new-token: 43, #cached-token: 669, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:03 TP1] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:03 TP3] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:03 TP5] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:03 TP4] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:03 TP7] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:03 TP0] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:03 TP2] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:03 TP6] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:04] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:05 TP0] Prefill batch. #new-seq: 1, #new-token: 41, #cached-token: 670, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP1] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP3] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP5] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP4] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP7] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP0] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP2] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP6] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP0] Decode batch. #running-req: 1024, #token: 101597, token usage: 0.10, cuda graph: False, gen throughput (token/s): 528.66, #queue-req: 293, 
[2025-10-21 00:25:05] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:05] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:05] INFO:     127.0.0.1:50078 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP3] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP1] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP4] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP7] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP5] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP0] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP2] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP6] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05] INFO:     127.0.0.1:58064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:05] INFO:     127.0.0.1:50978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:05] INFO:     127.0.0.1:51154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:05 TP0] Prefill batch. #new-seq: 3, #new-token: 195, #cached-token: 2009, token usage: 0.11, #running-req: 1021, #queue-req: 290, 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP1] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP3] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP7] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP4] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP5] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP2] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP0] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP6] [fused_moe] using default for (195, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05] INFO:     127.0.0.1:58958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:05] INFO:     127.0.0.1:59636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:05] INFO:     127.0.0.1:48822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:05] INFO:     127.0.0.1:49158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:05] INFO:     127.0.0.1:52072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:05] INFO:     127.0.0.1:55010 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP3] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP1] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP0] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP4] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP7] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP5] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP2] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP6] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP0] Prefill batch. #new-seq: 9, #new-token: 674, #cached-token: 6031, token usage: 0.11, #running-req: 1015, #queue-req: 281, 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP3] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP1] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP0] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP4] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP7] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP5] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP2] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:05 TP6] [fused_moe] using default for (674, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06] INFO:     127.0.0.1:58304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:52106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:54876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06 TP0] Prefill batch. #new-seq: 3, #new-token: 193, #cached-token: 2011, token usage: 0.11, #running-req: 1021, #queue-req: 278, 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP3] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP1] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP7] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP4] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP5] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP2] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP0] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP6] [fused_moe] using default for (193, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06] INFO:     127.0.0.1:58292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:50714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:51276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP3] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP7] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP0] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP4] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP1] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP5] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP2] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP6] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP0] Prefill batch. #new-seq: 8, #new-token: 404, #cached-token: 5365, token usage: 0.11, #running-req: 1016, #queue-req: 270, 
[aiter] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP3] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP0] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP1] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP7] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP4] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP5] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP2] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP6] [fused_moe] using default for (404, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:58682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:49750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:54974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP3] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP0] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP7] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP4] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP1] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP5] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP2] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP6] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP0] Prefill batch. #new-seq: 6, #new-token: 367, #cached-token: 4017, token usage: 0.11, #running-req: 1018, #queue-req: 264, 
[aiter] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP3] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP0] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP7] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP4] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP1] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP2] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP5] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP6] [fused_moe] using default for (367, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06] INFO:     127.0.0.1:59542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:49626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:50240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:06] INFO:     127.0.0.1:54352 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP0] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP3] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP4] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP7] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP1] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP2] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP5] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP6] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP0] Prefill batch. #new-seq: 5, #new-token: 253, #cached-token: 3350, token usage: 0.11, #running-req: 1019, #queue-req: 259, 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP3] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP0] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP1] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP7] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP4] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP2] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP5] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06 TP6] [fused_moe] using default for (253, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:06] INFO:     127.0.0.1:52342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07 TP0] Prefill batch. #new-seq: 1, #new-token: 36, #cached-token: 669, token usage: 0.11, #running-req: 1023, #queue-req: 258, 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP0] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP3] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP1] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP2] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP7] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP4] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP5] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP6] [fused_moe] using default for (36, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07] INFO:     127.0.0.1:58098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:58420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:58588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:59562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:50904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:51250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:52424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:53928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:54152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:54312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:55066 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP1] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP3] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP7] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP5] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP4] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP6] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP2] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP0] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP0] Prefill batch. #new-seq: 12, #new-token: 743, #cached-token: 8040, token usage: 0.11, #running-req: 1012, #queue-req: 246, 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP3] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP0] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP1] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP7] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP4] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP5] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP2] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP6] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:48428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:48582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:54004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:55440 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP3] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP7] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP0] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP4] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP1] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP5] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP2] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP6] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP0] Prefill batch. #new-seq: 7, #new-token: 513, #cached-token: 4690, token usage: 0.11, #running-req: 1017, #queue-req: 239, 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP4] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP7] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP5] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP1] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP3] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP6] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP2] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP0] [fused_moe] using default for (513, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07] INFO:     127.0.0.1:58850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:51200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:53988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07 TP0] Prefill batch. #new-seq: 6, #new-token: 310, #cached-token: 4023, token usage: 0.11, #running-req: 1018, #queue-req: 233, 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP7] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP0] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP3] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP4] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP1] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP5] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP2] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP6] [fused_moe] using default for (310, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07] INFO:     127.0.0.1:58638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:50822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:51976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:54596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:07] INFO:     127.0.0.1:55368 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP1] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP3] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP7] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP4] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP5] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP2] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP0] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP6] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP0] Prefill batch. #new-seq: 10, #new-token: 669, #cached-token: 6699, token usage: 0.11, #running-req: 1014, #queue-req: 223, 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP1] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP3] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP0] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP7] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP4] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP5] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP2] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:07 TP6] [fused_moe] using default for (669, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08] INFO:     127.0.0.1:48942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:50622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:51514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08 TP0] Prefill batch. #new-seq: 6, #new-token: 372, #cached-token: 4019, token usage: 0.12, #running-req: 1018, #queue-req: 217, 
[aiter] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP0] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP1] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP3] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP7] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP4] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP5] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP2] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP6] [fused_moe] using default for (372, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:58852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:49114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:49496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:51602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:52626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:52790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:54450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:54848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08 TP0] Prefill batch. #new-seq: 12, #new-token: 560, #cached-token: 8036, token usage: 0.12, #running-req: 1012, #queue-req: 205, 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP1] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP3] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP0] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP4] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP7] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP5] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP2] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP6] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:59254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:59450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:49768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:52238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:52702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:54384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08 TP0] Prefill batch. #new-seq: 9, #new-token: 543, #cached-token: 6030, token usage: 0.12, #running-req: 1015, #queue-req: 196, 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP0] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP3] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP7] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP4] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP1] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP5] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP2] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP6] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:59658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:50940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:52514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:53142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08 TP0] Prefill batch. #new-seq: 5, #new-token: 302, #cached-token: 3349, token usage: 0.12, #running-req: 1019, #queue-req: 191, 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP1] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP3] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP0] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP7] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP4] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP5] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP2] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08 TP6] [fused_moe] using default for (302, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:08] INFO:     127.0.0.1:57840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:48434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:52844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:53338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:08] INFO:     127.0.0.1:55162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09 TP0] Prefill batch. #new-seq: 7, #new-token: 408, #cached-token: 4687, token usage: 0.12, #running-req: 1017, #queue-req: 184, 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP1] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP3] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP0] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP7] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP4] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP5] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP2] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP6] [fused_moe] using default for (408, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09] INFO:     127.0.0.1:58830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:59764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:48966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:49822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:50930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:52828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP3] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP7] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP0] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP4] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP1] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP5] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP2] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP6] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP0] Prefill batch. #new-seq: 11, #new-token: 607, #cached-token: 7370, token usage: 0.12, #running-req: 1013, #queue-req: 173, 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP1] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP0] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP3] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP7] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP4] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP5] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP2] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP6] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:48418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:49376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:50888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:52302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09 TP0] Prefill batch. #new-seq: 8, #new-token: 401, #cached-token: 5359, token usage: 0.12, #running-req: 1016, #queue-req: 165, 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP1] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP3] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP0] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP7] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP5] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP4] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP2] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP6] [fused_moe] using default for (401, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09] INFO:     127.0.0.1:48606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:51142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:51312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:52224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:52946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:53650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:54128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:54538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09 TP0] Prefill batch. #new-seq: 8, #new-token: 449, #cached-token: 5360, token usage: 0.12, #running-req: 1016, #queue-req: 157, 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP3] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP1] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP7] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP5] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP4] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP2] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP0] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09 TP6] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:09] INFO:     127.0.0.1:48884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09] INFO:     127.0.0.1:55246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:09 TP0] Prefill batch. #new-seq: 5, #new-token: 310, #cached-token: 3348, token usage: 0.12, #running-req: 1019, #queue-req: 152, 
[2025-10-21 00:25:10] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:58262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:51434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:52056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:54620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10 TP0] Prefill batch. #new-seq: 11, #new-token: 727, #cached-token: 7371, token usage: 0.12, #running-req: 1013, #queue-req: 141, 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP3] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP1] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP0] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP7] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP4] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP5] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP2] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP6] [fused_moe] using default for (727, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10] INFO:     127.0.0.1:59128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:48702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:51420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:51912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:52116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:52620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:54666 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP3] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP0] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP7] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP4] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP1] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP5] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP2] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP6] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP0] Prefill batch. #new-seq: 14, #new-token: 757, #cached-token: 9381, token usage: 0.12, #running-req: 1010, #queue-req: 127, 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP1] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP3] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP0] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP7] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP4] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP5] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP2] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP6] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:51720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:53238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:53960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10 TP0] Prefill batch. #new-seq: 6, #new-token: 411, #cached-token: 4021, token usage: 0.12, #running-req: 1018, #queue-req: 121, 
[aiter] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP1] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP3] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP0] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP7] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP4] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP5] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP2] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP6] [fused_moe] using default for (411, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10] INFO:     127.0.0.1:59294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:48690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:50634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:51786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:52366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:53666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:54742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10 TP0] Prefill batch. #new-seq: 12, #new-token: 893, #cached-token: 8041, token usage: 0.12, #running-req: 1012, #queue-req: 109, 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP3] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP1] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP0] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP7] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP4] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP5] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP2] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10 TP6] [fused_moe] using default for (893, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:10] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:58904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:49312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:49442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:50506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:51704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:10] INFO:     127.0.0.1:54438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11 TP0] Prefill batch. #new-seq: 10, #new-token: 689, #cached-token: 6700, token usage: 0.12, #running-req: 1014, #queue-req: 99, 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP3] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP1] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP0] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP7] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP4] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP5] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP2] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP6] [fused_moe] using default for (689, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:58518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:59572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:49288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:49428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:50970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:55214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11 TP0] Prefill batch. #new-seq: 12, #new-token: 635, #cached-token: 8044, token usage: 0.12, #running-req: 1012, #queue-req: 87, 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP3] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP1] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP7] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP4] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP5] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP2] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP0] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP6] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11] INFO:     127.0.0.1:59140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:48316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:48456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:51888 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP3] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP7] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP0] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP4] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP1] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP5] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP2] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP6] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP0] Prefill batch. #new-seq: 4, #new-token: 195, #cached-token: 2679, token usage: 0.12, #running-req: 1020, #queue-req: 83, 
[2025-10-21 00:25:11] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:49034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:49120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:49446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:49962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:51346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:52044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:52336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:52678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:53746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP3] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP7] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP0] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP1] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP4] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP5] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP2] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP6] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP0] Prefill batch. #new-seq: 17, #new-token: 1106, #cached-token: 11386, token usage: 0.12, #running-req: 1007, #queue-req: 66, 
[2025-10-21 00:25:11] INFO:     127.0.0.1:59170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:49830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:51962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:52090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:52304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:53196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:53380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:11] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP3] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP0] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP7] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP1] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP4] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP5] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP2] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP6] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP0] Prefill batch. #new-seq: 13, #new-token: 958, #cached-token: 8706, token usage: 0.12, #running-req: 1011, #queue-req: 53, 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP3] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP1] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP0] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP5] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP7] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP4] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP2] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:11 TP6] [fused_moe] using default for (958, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:48832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:50376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:51028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:51288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:51408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:51990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:52194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:53636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12 TP0] Prefill batch. #new-seq: 14, #new-token: 782, #cached-token: 9379, token usage: 0.12, #running-req: 1010, #queue-req: 39, 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP3] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP0] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP1] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP5] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP7] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP4] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP2] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP6] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12] INFO:     127.0.0.1:48462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:49614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:49646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:50204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:51732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:53444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:53610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:53816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP3] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP7] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP1] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP0] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP5] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP4] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP2] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP6] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP0] Prefill batch. #new-seq: 15, #new-token: 878, #cached-token: 10053, token usage: 0.12, #running-req: 1009, #queue-req: 24, 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP0] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP1] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP3] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP7] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP5] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP4] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP2] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP6] [fused_moe] using default for (878, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:49524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:49806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:52290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:52872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:53116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:53222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12 TP0] Prefill batch. #new-seq: 11, #new-token: 697, #cached-token: 7366, token usage: 0.13, #running-req: 1013, #queue-req: 13, 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP0] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP3] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP1] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP7] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP5] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP4] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP2] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP6] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12] INFO:     127.0.0.1:59706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:48648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:49944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:50254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:50418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:51238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:52522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:53066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:54272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12] INFO:     127.0.0.1:55256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:12 TP0] Prefill batch. #new-seq: 12, #new-token: 710, #cached-token: 8041, token usage: 0.13, #running-req: 1012, #queue-req: 1, 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP3] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP0] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP1] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP7] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP5] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP4] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP2] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:12 TP6] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:59544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:49910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:52438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:54174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13 TP0] Prefill batch. #new-seq: 1, #new-token: 34, #cached-token: 673, token usage: 0.13, #running-req: 1014, #queue-req: 0, 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP0] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP3] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP1] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP7] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP4] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP5] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP2] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP6] [fused_moe] using default for (34, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:58220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:58728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:50716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:50758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:52166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:52960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:54094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP3] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP7] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP0] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP1] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP4] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP5] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP2] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP6] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:55084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP3] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP1] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP7] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP5] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP4] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP0] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP2] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP6] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13] INFO:     127.0.0.1:58926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:48810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:50690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:52382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:53296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:53748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP7] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP5] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP4] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP3] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP1] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP6] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP0] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP2] [fused_moe] using default for (989, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13] INFO:     127.0.0.1:58320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:59366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:52184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:53020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:54844 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP3] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP1] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP0] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP2] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP7] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP5] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP4] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP6] [fused_moe] using default for (978, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP0] Decode batch. #running-req: 989, #token: 120882, token usage: 0.12, cuda graph: False, gen throughput (token/s): 4852.27, #queue-req: 0, 
[2025-10-21 00:25:13] INFO:     127.0.0.1:58024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:58172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:59024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:49074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:49562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:50318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:51090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:51374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:51456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP3] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP7] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP0] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP1] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP4] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP5] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP2] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP6] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13] INFO:     127.0.0.1:58526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:59108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:59230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:48888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:50778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:53972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:54124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:54576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:54610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:13] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP3] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP7] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP1] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP0] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP5] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP4] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP2] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:13 TP6] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14] INFO:     127.0.0.1:58002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:59182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:48360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:48716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:51294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:51698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:54550 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP3] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP7] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP1] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP5] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP0] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP4] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP2] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP6] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14] INFO:     127.0.0.1:58144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:51642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:52806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:53862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:54782 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP3] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP7] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP1] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP5] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP4] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP0] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP2] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP6] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:59732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:50382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:53082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:53750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:54072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:54226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP3] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP1] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP5] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP7] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP4] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP2] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP0] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP6] [fused_moe] using default for (919, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:59812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:52712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP5] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP7] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP0] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP3] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP1] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP4] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP6] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP2] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14] INFO:     127.0.0.1:58124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:58476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:51922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:53488 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP3] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP4] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP1] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP7] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP5] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP2] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP6] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP0] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:58994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:59284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:59316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:59510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:49450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:50306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:50544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:52230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:53740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:14] INFO:     127.0.0.1:54256 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP3] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP1] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP0] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP7] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP5] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP4] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP2] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:14 TP6] [fused_moe] using default for (885, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15] INFO:     127.0.0.1:48846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:49326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:49880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:50026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:50290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:51522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:51716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:51798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:52524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:53562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP3] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP7] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP1] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP5] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP0] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP4] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP2] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP6] [fused_moe] using default for (872, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:48796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:49742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:50270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:52978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:53266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:55194 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP3] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP7] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP1] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP5] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP0] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP4] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP2] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP6] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15] INFO:     127.0.0.1:58056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:58114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:59034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:49698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:49842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:49920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:50076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:51336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:51442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:53672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:55290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP3] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP7] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP1] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP5] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP0] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP4] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP2] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP6] [fused_moe] using default for (846, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:51768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:53166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP3] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP7] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP1] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP5] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP0] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP4] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP2] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP6] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15] INFO:     127.0.0.1:59482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:49856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:52448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP3] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP7] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP1] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP5] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP4] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP0] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP2] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP6] [fused_moe] using default for (828, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:48634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:51852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:53004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54364 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP3] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP1] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP5] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP7] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP4] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP0] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP2] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP6] [fused_moe] using default for (820, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:59222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:59360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:50664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:50858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:51160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:53302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP1] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP3] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP5] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP7] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP0] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP4] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP2] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP6] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15] INFO:     127.0.0.1:50468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:50916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:51722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:52146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:52428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:53148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:54898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:55402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:15] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP1] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP3] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP5] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP7] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP4] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP0] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP2] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:15 TP6] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:51438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:51556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP3] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP1] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP5] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP7] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP4] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP2] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP0] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP6] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:49710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:50012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:50432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:50452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:50874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:55588 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP3] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP1] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP5] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP7] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP4] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP0] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP2] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP6] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16] INFO:     127.0.0.1:58366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:50578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:51146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:52028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:54334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:54906 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP1] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP3] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP5] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP7] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP4] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP0] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP2] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP6] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16] INFO:     127.0.0.1:58642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:59462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:48370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:48870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:49392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:50850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:51570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:52140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:54734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:55142 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP3] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP1] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP7] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP5] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP4] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP0] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP2] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP6] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:58360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:59398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:48740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:49416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:52042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:56620 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP3] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP1] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP5] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP7] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP4] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP0] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP2] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP6] [fused_moe] using default for (745, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP3] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP1] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP4] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP5] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP7] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP0] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP2] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP6] [fused_moe] using default for (734, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16] INFO:     127.0.0.1:48658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:49212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:49332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:50814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:50998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:54234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:55380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:59196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:49834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:49986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:51236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:54798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:55554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:55638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP3] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP1] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP4] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP7] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP5] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP0] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP2] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP6] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16] INFO:     127.0.0.1:58268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:51736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:55434 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP3] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP1] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP5] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP7] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP4] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP0] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP2] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16 TP6] [fused_moe] using default for (720, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:16] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:48956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:49660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:52412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:53122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:16] INFO:     127.0.0.1:54130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:59692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:48906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:49688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:50790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:52866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:54284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:56068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:56314 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP3] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP1] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP5] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP4] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP7] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP0] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP2] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP6] [fused_moe] using default for (698, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17] INFO:     127.0.0.1:49002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:50274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:50598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:51070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:52282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:52598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:53874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:56550 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP3] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP1] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP5] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP7] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP4] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP0] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP2] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP6] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:59434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:48946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:49814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:49914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:50158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:51494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:56236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:59232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:48568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:54480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:56274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP3] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP1] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP5] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP4] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP7] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP0] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP2] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP6] [fused_moe] using default for (663, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:49176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:49462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:50196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:50298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:50906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:51104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:51230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:53460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:54038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:56792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP3] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP1] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP7] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP5] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP4] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP2] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP0] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP6] [fused_moe] using default for (648, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17] INFO:     127.0.0.1:58764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:49550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:51592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:55306 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP3] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP1] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP0] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP5] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP4] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP7] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP2] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP6] [fused_moe] using default for (643, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17] INFO:     127.0.0.1:58390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:48302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:48622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:48680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:50802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:52356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:53890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:54828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP1] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP3] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP5] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP7] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP4] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP0] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP2] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP6] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:58200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:59298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:49128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:49472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:49576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:50824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:51882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:17] INFO:     127.0.0.1:55574 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP3] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP1] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP7] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP5] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP4] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP0] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP2] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:17 TP6] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:59102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:49862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:52252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP3] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP1] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP5] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP7] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP4] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP0] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP2] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP6] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:58832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:50610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:53472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:57574 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP3] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP1] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP4] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP5] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP7] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP0] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP2] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP6] [fused_moe] using default for (602, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:48596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:49520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:50000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:50838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:52622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:53396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:53968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP3] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP1] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP4] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP5] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP7] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP0] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP2] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP6] [fused_moe] using default for (589, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:48386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:50460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:50696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:53322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP3] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP1] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP5] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP7] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP4] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP0] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP2] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP6] [fused_moe] using default for (580, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18] INFO:     127.0.0.1:58322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:48410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:52214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:52640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:53910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:53998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:54078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP1] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP3] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP4] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP7] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP5] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP0] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP2] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP6] [fused_moe] using default for (567, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:58158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:58906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:59444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:49630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:49726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:50500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:50768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:52926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:53204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:53838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:54214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:56878 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP3] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP1] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP5] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP7] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP4] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP0] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP2] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP6] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:59646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:48898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:49510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:50640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:50986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:53500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP1] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP3] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP7] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP5] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP4] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP0] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP2] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP6] [fused_moe] using default for (534, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP0] Decode batch. #running-req: 548, #token: 84003, token usage: 0.09, cuda graph: False, gen throughput (token/s): 6186.13, #queue-req: 0, 
[2025-10-21 00:25:18] INFO:     127.0.0.1:50336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:54912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:56116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP3] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP1] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP4] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP7] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP5] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP0] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP2] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18 TP6] [fused_moe] using default for (527, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:25:18] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:49342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:51176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:51426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:52208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:53466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:54614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:56284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:56380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:56704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:57364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:58980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:59750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:51626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:55186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:56350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:56546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:56750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:49024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:49262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:50556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:52760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:56066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:18] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:48742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:48818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:50648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:52360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:53848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:50100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:52656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:54726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:59386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:48850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:50522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:50536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:51036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:54108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:59608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:53724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:53992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:59050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:49302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:51182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:54062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:50212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:50984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:53952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:50176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:51774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:52474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:55622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:59726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:52172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:58846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:50484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:51812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:54584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:49210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:49584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:53108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:58126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:58378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:49780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:49796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:49896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:52142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:52484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:51896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:48572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:48974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:49060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:49716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:50126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:52088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:51402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:52558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:54646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:56980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:19] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:59664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:52010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:53086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:49228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:51662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:52530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:53570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:59792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:48782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:53786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:52998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:50592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:50662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:48928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:55624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:59394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:51112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:52004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:52570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:54750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:54960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:58108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:58212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:59734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:48754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:48860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:51320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:58026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:59372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:58790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:48412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:49674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:55150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:48662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:54882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:58206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:51322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:55458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:56804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:20] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:58340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:59578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:59780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:52616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:59616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:50444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:59640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:48478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:53642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:52276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21 TP0] Decode batch. #running-req: 251, #token: 48403, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6071.53, #queue-req: 0, 
[2025-10-21 00:25:21] INFO:     127.0.0.1:58186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:49512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:52114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:54774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:58068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:52890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:58694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:50142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:50594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:51578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:58888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:52744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:52378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:58670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:51776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:58080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:58560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:53276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:48346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:48768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:48546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:56590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:21] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:48350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:53410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:51866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:48726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:51674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:58818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:59828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:48678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:50676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:55448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:55928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:49362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:51684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:55842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:51508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:55888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:52126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:54830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:54300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:49048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:51552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:59704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:51760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:22] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:57386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23 TP0] Decode batch. #running-req: 93, #token: 23301, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3530.47, #queue-req: 0, 
[2025-10-21 00:25:23] INFO:     127.0.0.1:58050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:55520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:59210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:58468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:58878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:57082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:56860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:57626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:50292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:50478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:53770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:55882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:49086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:59438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:49252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:57638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:56494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:57522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:51646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:52740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:56200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:59354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:55970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:53556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:57690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:57370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:57320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:56324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:23] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:57180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:56998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:57560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:57674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:49400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:52860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:54744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:56920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:58038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24 TP0] Decode batch. #running-req: 32, #token: 9493, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1660.52, #queue-req: 0, 
[2025-10-21 00:25:24] INFO:     127.0.0.1:58722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:56102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:58092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:49196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:56800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:55394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:24] INFO:     127.0.0.1:56358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:53680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:56876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:56146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25 TP0] Decode batch. #running-req: 10, #token: 3648, token usage: 0.00, cuda graph: True, gen throughput (token/s): 832.29, #queue-req: 0, 
[2025-10-21 00:25:25] INFO:     127.0.0.1:57080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:55310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:53362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:25] INFO:     127.0.0.1:56858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:26] INFO:     127.0.0.1:56438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:26 TP0] Decode batch. #running-req: 1, #token: 966, token usage: 0.00, cuda graph: True, gen throughput (token/s): 222.08, #queue-req: 0, 
[2025-10-21 00:25:26] INFO:     127.0.0.1:56914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:25:40] INFO:     127.0.0.1:48192 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-21 00:25:40 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-21 00:25:41 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-21 00:25:41 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-21 00:25:41 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-21 00:25:41 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-21 00:25:41 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-21 00:25:41 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-21 00:25:41 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-10-21 00:25:42 TP5] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
[92mSuccessfully preprocessed all matching files.[0m
[aiter] finish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip], cost 55.61402310s
[2025-10-21 00:26:37 TP5] finish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip], cost 55.61402310s
[2025-10-21 00:26:37] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:37 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-21 00:26:37 TP0] Prefill batch. #new-seq: 23, #new-token: 23, #cached-token: 16706, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP0] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP2] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP1] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP3] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP4] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP7] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP5] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP6] [fused_moe] using default for (23, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4406, token usage: 0.00, #running-req: 24, #queue-req: 0, 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP7] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP5] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP4] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP2] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP6] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP3] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP1] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP0] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP0] Prefill batch. #new-seq: 53, #new-token: 53, #cached-token: 38622, token usage: 0.01, #running-req: 30, #queue-req: 0, 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP5] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP4] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP7] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP6] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP0] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP2] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP1] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP3] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP0] Prefill batch. #new-seq: 46, #new-token: 46, #cached-token: 33449, token usage: 0.01, #running-req: 83, #queue-req: 0, 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP2] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP1] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP0] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP3] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP7] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP4] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP6] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP5] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 44004, token usage: 0.01, #running-req: 129, #queue-req: 0, 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP0] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP2] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP4] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP6] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP1] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP3] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP7] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:37 TP5] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] Prefill batch. #new-seq: 50, #new-token: 50, #cached-token: 36397, token usage: 0.02, #running-req: 189, #queue-req: 0, 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP2] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP5] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP3] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP1] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP4] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP7] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP6] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 47256, token usage: 0.02, #running-req: 239, #queue-req: 0, 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP3] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP1] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP7] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP2] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP4] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP6] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP5] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] Prefill batch. #new-seq: 55, #new-token: 55, #cached-token: 39974, token usage: 0.02, #running-req: 304, #queue-req: 0, 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP2] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP3] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP1] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP5] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP7] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP4] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP6] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] Prefill batch. #new-seq: 68, #new-token: 68, #cached-token: 49402, token usage: 0.03, #running-req: 359, #queue-req: 0, 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP2] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP1] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP3] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP4] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP5] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP7] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP6] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] Prefill batch. #new-seq: 58, #new-token: 58, #cached-token: 41875, token usage: 0.03, #running-req: 427, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP2] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP3] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP1] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP5] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP7] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP4] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP6] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] Prefill batch. #new-seq: 71, #new-token: 71, #cached-token: 51669, token usage: 0.03, #running-req: 485, #queue-req: 0, 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP2] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP1] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP7] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP4] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP3] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP5] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP6] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 44208, token usage: 0.04, #running-req: 556, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP2] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP3] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP1] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP4] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP5] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP7] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP6] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] Prefill batch. #new-seq: 74, #new-token: 74, #cached-token: 54138, token usage: 0.04, #running-req: 617, #queue-req: 0, 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP2] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP3] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP4] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP0] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP7] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP1] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP6] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:38 TP5] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP0] Prefill batch. #new-seq: 63, #new-token: 63, #cached-token: 45737, token usage: 0.05, #running-req: 691, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP1] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP3] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP6] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP0] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP7] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP5] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP2] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP4] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP0] Prefill batch. #new-seq: 77, #new-token: 77, #cached-token: 56446, token usage: 0.05, #running-req: 754, #queue-req: 0, 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP2] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP4] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP5] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP0] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP3] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP1] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP7] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP6] [fused_moe] using default for (77, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 44922, token usage: 0.06, #running-req: 831, #queue-req: 0, 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP2] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP1] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP3] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP0] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP4] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP7] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP5] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP6] [fused_moe] using default for (62, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP0] Prefill batch. #new-seq: 49, #new-token: 49, #cached-token: 35804, token usage: 0.06, #running-req: 893, #queue-req: 0, 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP4] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP7] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP5] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP6] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP2] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP1] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP3] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP0] [fused_moe] using default for (49, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP0] Prefill batch. #new-seq: 22, #new-token: 22, #cached-token: 16108, token usage: 0.06, #running-req: 942, #queue-req: 0, 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP2] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP4] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP6] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP0] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP1] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP5] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP7] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:39 TP3] [fused_moe] using default for (22, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:40 TP0] Prefill batch. #new-seq: 50, #new-token: 50, #cached-token: 36840, token usage: 0.06, #running-req: 964, #queue-req: 0, 
[2025-10-21 00:26:40 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7235, token usage: 0.07, #running-req: 1014, #queue-req: 39, 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:40 TP2] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:40 TP3] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:40 TP1] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:40 TP0] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:40 TP7] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:40 TP4] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:40 TP6] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:40 TP5] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:43] INFO:     127.0.0.1:40814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:43 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 709, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-21 00:26:44] INFO:     127.0.0.1:36732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:44 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-21 00:26:44 TP0] Decode batch. #running-req: 1023, #token: 96325, token usage: 0.10, cuda graph: False, gen throughput (token/s): 444.44, #queue-req: 293, 
[2025-10-21 00:26:44] INFO:     127.0.0.1:40242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:44 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-21 00:26:44] INFO:     127.0.0.1:35668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:44 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 729, token usage: 0.10, #running-req: 1023, #queue-req: 291, 
[2025-10-21 00:26:45] INFO:     127.0.0.1:36160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:42570 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:45 TP2] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:45 TP4] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:45 TP6] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:45 TP0] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:45 TP3] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:45 TP7] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:45 TP1] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:45 TP5] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:45] INFO:     127.0.0.1:36066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:36092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:37680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:38634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:41996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1525, token usage: 0.11, #running-req: 1022, #queue-req: 289, 
[2025-10-21 00:26:45] INFO:     127.0.0.1:37290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:37940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:38984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:39156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:39220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7383, token usage: 0.11, #running-req: 1014, #queue-req: 279, 
[2025-10-21 00:26:45] INFO:     127.0.0.1:43856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 726, token usage: 0.11, #running-req: 1023, #queue-req: 278, 
[2025-10-21 00:26:45] INFO:     127.0.0.1:37722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:37856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:37990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:38246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:38672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:39134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:40496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:41084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45] INFO:     127.0.0.1:44032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:45 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6502, token usage: 0.11, #running-req: 1015, #queue-req: 269, 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP0] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP2] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP3] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP7] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP1] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP4] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP6] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP5] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46] INFO:     127.0.0.1:35654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:36218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:38680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:40648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:40834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:41352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4345, token usage: 0.11, #running-req: 1018, #queue-req: 263, 
[2025-10-21 00:26:46] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:41224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:42502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:43382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2866, token usage: 0.11, #running-req: 1020, #queue-req: 259, 
[2025-10-21 00:26:46] INFO:     127.0.0.1:37766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:43584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:44004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:44128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2890, token usage: 0.11, #running-req: 1020, #queue-req: 255, 
[2025-10-21 00:26:46] INFO:     127.0.0.1:36322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:37786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:38212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:38948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:39058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:39236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:40094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:40540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:41176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:42518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:42954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:43198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:43510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46 TP0] Prefill batch. #new-seq: 14, #new-token: 14, #cached-token: 10311, token usage: 0.11, #running-req: 1010, #queue-req: 241, 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP2] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP0] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP3] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP4] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP6] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP7] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP1] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46 TP5] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:46] INFO:     127.0.0.1:37276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:37872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:39404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:41750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:41858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:42704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:43038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:46] INFO:     127.0.0.1:43330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5772, token usage: 0.11, #running-req: 1016, #queue-req: 233, 
[2025-10-21 00:26:47 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3720, token usage: 0.11, #running-req: 1019, #queue-req: 228, 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP2] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP5] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP4] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP6] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP0] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP1] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP7] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP3] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47] INFO:     127.0.0.1:36170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:38010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:40722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:43016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:44060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:35902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:36566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:38110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:38558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:38704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:39410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:39454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:43854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:44470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6572, token usage: 0.12, #running-req: 1015, #queue-req: 219, 
[2025-10-21 00:26:47] INFO:     127.0.0.1:35640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:37520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:38166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:39380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:40368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:40794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:40952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:41592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:43250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:43396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7199, token usage: 0.12, #running-req: 1014, #queue-req: 209, 
[2025-10-21 00:26:47] INFO:     127.0.0.1:35952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:37020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:38022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:38754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:39586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:40916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:41414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:41584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:43348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:43458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47] INFO:     127.0.0.1:44392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:47 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 7977, token usage: 0.12, #running-req: 1013, #queue-req: 198, 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP0] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP2] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP6] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP4] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP1] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP5] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP3] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:47 TP7] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:48] INFO:     127.0.0.1:36554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:37176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:37228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:38118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:38428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:39296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4363, token usage: 0.12, #running-req: 1018, #queue-req: 192, 
[2025-10-21 00:26:48] INFO:     127.0.0.1:36108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:37194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:39336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:41200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:41494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:42426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:42446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5085, token usage: 0.12, #running-req: 1017, #queue-req: 185, 
[2025-10-21 00:26:48] INFO:     127.0.0.1:35984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:36568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:36686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:39674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:39944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:40572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:40606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:40786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:42192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:42370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7265, token usage: 0.12, #running-req: 1014, #queue-req: 175, 
[2025-10-21 00:26:48] INFO:     127.0.0.1:36352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:36424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:36580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:37308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:40840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:42378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:43220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5054, token usage: 0.12, #running-req: 1017, #queue-req: 168, 
[2025-10-21 00:26:48] INFO:     127.0.0.1:35774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:36128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:37154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:39026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:39072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:39184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:40582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:42632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48] INFO:     127.0.0.1:44180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:48 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6485, token usage: 0.12, #running-req: 1015, #queue-req: 159, 
[2025-10-21 00:26:49] INFO:     127.0.0.1:36684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:37784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:39544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:39812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:39972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:42472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:42660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5124, token usage: 0.12, #running-req: 1017, #queue-req: 152, 
[2025-10-21 00:26:49] INFO:     127.0.0.1:36890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:37244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:38440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:40852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2874, token usage: 0.12, #running-req: 1020, #queue-req: 148, 
[2025-10-21 00:26:49] INFO:     127.0.0.1:36200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:36426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:36514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:38048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:42280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:43152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:43342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:43614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5923, token usage: 0.12, #running-req: 1016, #queue-req: 140, 
[2025-10-21 00:26:49] INFO:     127.0.0.1:37496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:37514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:38248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:38334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:39190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:40844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:41954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:42592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:42744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:43514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:43652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:44374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8696, token usage: 0.12, #running-req: 1012, #queue-req: 128, 
[2025-10-21 00:26:49] INFO:     127.0.0.1:36206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:36698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:37898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:38062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:38382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:39902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:42974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:49] INFO:     127.0.0.1:43408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6723, token usage: 0.12, #running-req: 1015, #queue-req: 119, 
[2025-10-21 00:26:50] INFO:     127.0.0.1:35820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:38198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:41098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:42838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4347, token usage: 0.12, #running-req: 1018, #queue-req: 113, 
[2025-10-21 00:26:50] INFO:     127.0.0.1:36022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:36158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:36620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:38370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:39762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:40634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:42042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:43288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:43446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:43808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8151, token usage: 0.12, #running-req: 1013, #queue-req: 102, 
[2025-10-21 00:26:50] INFO:     127.0.0.1:35840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:36236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:36700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:37032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:37206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:37610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:38870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:38974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:39526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:39558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:40446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:42152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:42194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:43706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:43872 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:50 TP2] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:50 TP4] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:50 TP6] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:50 TP0] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:50 TP1] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:50 TP5] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:50 TP3] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:50 TP7] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:50 TP0] Prefill batch. #new-seq: 16, #new-token: 16, #cached-token: 11611, token usage: 0.12, #running-req: 1008, #queue-req: 86, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-10-21 00:26:50] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:36538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:36904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:37074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:37522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:39060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4327, token usage: 0.12, #running-req: 1018, #queue-req: 80, 
[2025-10-21 00:26:50] INFO:     127.0.0.1:35994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:37148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:37810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:38138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:38790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:38828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:39722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:39782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:40910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:41816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:42100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:42158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:42762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:43418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:43484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:50] INFO:     127.0.0.1:44410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51 TP0] Prefill batch. #new-seq: 17, #new-token: 17, #cached-token: 12509, token usage: 0.13, #running-req: 1007, #queue-req: 63, 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP0] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP2] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP6] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP4] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP1] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP3] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP5] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP7] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51] INFO:     127.0.0.1:36142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:36396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:36596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:36958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:38270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:38848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:39108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:40042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:40106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:42322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:43248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:43264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:43712 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP2] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP0] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP6] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP4] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP1] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP5] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP3] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP7] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP0] Prefill batch. #new-seq: 18, #new-token: 18, #cached-token: 13260, token usage: 0.13, #running-req: 1006, #queue-req: 45, 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP2] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP6] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP4] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP0] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP1] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP5] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP3] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP7] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51] INFO:     127.0.0.1:36674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:37356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:37482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:37750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:38132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:38408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:39854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:42024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:42110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7225, token usage: 0.13, #running-req: 1014, #queue-req: 35, 
[2025-10-21 00:26:51] INFO:     127.0.0.1:35832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:36358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:36866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:38710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:39440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:39822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:39850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:40140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:42526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:42684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:44334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51 TP0] Prefill batch. #new-seq: 17, #new-token: 17, #cached-token: 12439, token usage: 0.13, #running-req: 1007, #queue-req: 18, 
[2025-10-21 00:26:51] INFO:     127.0.0.1:36852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:37342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:38654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:39038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:39620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:40570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:41872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:42614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:42624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51] INFO:     127.0.0.1:43826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:51 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 10913, token usage: 0.13, #running-req: 1009, #queue-req: 3, 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP2] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP4] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP6] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP0] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP3] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP7] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP1] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:51 TP5] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52] INFO:     127.0.0.1:36084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:39914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:40458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:40654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:40704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:41068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:43310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2163, token usage: 0.13, #running-req: 1016, #queue-req: 0, 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP2] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP4] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP6] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP0] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP3] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP1] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP7] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP5] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52] INFO:     127.0.0.1:36986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:37378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:39050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:39260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:39374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:40486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:42076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:44000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52 TP0] Decode batch. #running-req: 1011, #token: 121760, token usage: 0.13, cuda graph: False, gen throughput (token/s): 4981.77, #queue-req: 0, 
[2025-10-21 00:26:52] INFO:     127.0.0.1:36756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:38094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:38386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:40168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:41132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:42204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:42488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:43110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:44224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:44256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:44290 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP2] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP4] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP6] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP0] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP3] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP7] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP1] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP5] [fused_moe] using default for (1000, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52] INFO:     127.0.0.1:36530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:39482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:42018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:43592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:43646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:43950 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP4] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP2] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP6] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP0] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP3] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP7] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP1] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP5] [fused_moe] using default for (994, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52] INFO:     127.0.0.1:36984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:38418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:38918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:39684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:40484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:41392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:42136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:43528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:44108 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP4] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP6] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP2] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP5] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP7] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP0] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP3] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP1] [fused_moe] using default for (985, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52] INFO:     127.0.0.1:35784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:35850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:36632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:36804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:37364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:37582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:37862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:38818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:39966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:40132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:40766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:41696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:52] INFO:     127.0.0.1:44166 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP2] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP4] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP6] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP0] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP1] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP3] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP5] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:52 TP7] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:37796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:38758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:40214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:42056 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP4] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP2] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP6] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP0] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP1] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP3] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP5] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP7] [fused_moe] using default for (967, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53] INFO:     127.0.0.1:36294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:36734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:36810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:37310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:37628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:38034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:38164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:38544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:39680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:39876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:43006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:43136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:43554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:43848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:37102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:37114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:38756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:39322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:39468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:43506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:43674 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP2] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP4] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP6] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP0] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP1] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP5] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP3] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP7] [fused_moe] using default for (945, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53] INFO:     127.0.0.1:35710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:36172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:36230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:37954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:39250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:42196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:42560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:42890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:44110 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP4] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP2] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP6] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP0] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP1] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP5] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP3] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP7] [fused_moe] using default for (936, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53] INFO:     127.0.0.1:36934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:36942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:38170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:38174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:38472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:38922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:40306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:40678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:41278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:41508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:42782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:42950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:43094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:43626 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP4] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP2] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP6] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP0] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP1] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP5] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP3] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP7] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53] INFO:     127.0.0.1:35736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:36778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:37318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:39268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:39984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:40686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:40724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:42404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:42492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP4] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP2] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP6] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP0] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP1] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP5] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP3] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP7] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53] INFO:     127.0.0.1:36150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:36298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:37192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:37498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:38252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:38902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:39306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:40896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:41294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:42216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:42374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:43822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:44452 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP2] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP4] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP6] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP0] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP3] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP1] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP7] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP5] [fused_moe] using default for (897, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53] INFO:     127.0.0.1:36266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:37714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:37922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:38518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:39726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:39752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:40798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:41420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:42868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:43050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:43296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:44350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:53] INFO:     127.0.0.1:44594 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP2] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP4] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP6] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP0] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP3] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP7] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP1] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:53 TP5] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54] INFO:     127.0.0.1:36220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:36826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:37516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:37554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:38234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:38316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:38592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:38890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:40180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:40256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:40668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:40972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:41398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:41450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:42122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:42576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:43782 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP2] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP4] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP6] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP0] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP3] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP7] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP1] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP5] [fused_moe] using default for (867, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54] INFO:     127.0.0.1:36600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:38858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:39514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:39834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:39892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:40928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:42224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP2] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP4] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP6] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP0] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP3] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP7] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP1] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP5] [fused_moe] using default for (859, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54] INFO:     127.0.0.1:35684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:36042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:36786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:37566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:38692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:39000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:39698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:40592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:40734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:42438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:42664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:43488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:43770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:44140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:44234 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP2] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP4] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP6] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP0] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP3] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP7] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP1] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP5] [fused_moe] using default for (843, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54] INFO:     127.0.0.1:35966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:37240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:40038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:42360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:43744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:44548 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP2] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP4] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP6] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP0] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP3] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP7] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP1] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP5] [fused_moe] using default for (837, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54] INFO:     127.0.0.1:36030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:36560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:38220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:38388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:42328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:43192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:43690 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP2] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP4] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP6] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP0] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP3] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP1] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP5] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP7] [fused_moe] using default for (830, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54] INFO:     127.0.0.1:35968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:36112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:36280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:36654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:37510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:39096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:40010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:40314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:40610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:41530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:41676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:42054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:42394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:43374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:43406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:44178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:44188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:44240 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP2] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP4] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP6] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP0] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP3] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP1] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP7] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP5] [fused_moe] using default for (812, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54] INFO:     127.0.0.1:37644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:38362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:39650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:41466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:44980 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP2] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP4] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP6] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP0] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP3] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP1] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP7] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP5] [fused_moe] using default for (807, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54] INFO:     127.0.0.1:35980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:39054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:42720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:42760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:43184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:43462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:43686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:43960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:54] INFO:     127.0.0.1:45208 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP2] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP4] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP6] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP0] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP3] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP1] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP5] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:54 TP7] [fused_moe] using default for (797, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55] INFO:     127.0.0.1:35806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:43900 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP2] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP4] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP6] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP0] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP3] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP1] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP7] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP5] [fused_moe] using default for (791, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55] INFO:     127.0.0.1:35864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:37884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:40550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:42070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:43898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:44362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:44620 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP2] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP4] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP6] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP0] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP3] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP1] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP7] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP5] [fused_moe] using default for (779, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55] INFO:     127.0.0.1:37242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:42390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:45254 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP2] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP4] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP6] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP0] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP3] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP1] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP7] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP5] [fused_moe] using default for (771, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55] INFO:     127.0.0.1:36498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:36874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:37368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:37448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:37686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:37994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:40986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:42306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:42444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:44094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:45644 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP2] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP4] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP6] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP0] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP1] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP3] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP5] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP7] [fused_moe] using default for (759, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55] INFO:     127.0.0.1:35740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:36010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:37172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:40384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:40694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41710 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP2] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55] INFO:     127.0.0.1:43914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:44300 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP4] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP6] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55] INFO:     127.0.0.1:43286 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP0] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP3] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP1] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP5] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP7] [fused_moe] using default for (746, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP2] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP4] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP6] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP0] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP7] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP3] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP5] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP1] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55] INFO:     127.0.0.1:36730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:37270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:37552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:37632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:42096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:42584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:42934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:43304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:44168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:45652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:37004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:40072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:40770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:44460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:44578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:45108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:36382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:42290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:43536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:43842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:44056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:44078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:44384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:45064 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP2] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP6] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP4] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP0] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP3] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP1] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP7] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP5] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55] INFO:     127.0.0.1:36106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:38916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:40398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:40530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:41748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:42794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:43168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:43504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:45386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:55] INFO:     127.0.0.1:45584 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP2] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP4] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP6] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP0] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP3] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP1] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP7] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:55 TP5] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56] INFO:     127.0.0.1:35800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:35886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:36286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:36338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:39348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:40048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:40062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:41116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:42882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:44478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:45228 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP4] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP2] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP6] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP0] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP3] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP1] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP7] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP5] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56] INFO:     127.0.0.1:39040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:39706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:39794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:40330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:40626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:43044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:43648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:44704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:45306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:45372 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP2] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP4] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP6] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP0] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP3] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP1] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP7] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP5] [fused_moe] using default for (682, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56] INFO:     127.0.0.1:37876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:38410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:39150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:40714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:41788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:42032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:42956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:43100 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP2] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP4] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP6] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP0] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP3] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP1] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP7] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP5] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56] INFO:     127.0.0.1:35758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:36594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:37380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:37546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:38004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:38262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:38498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:40958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:41048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:41968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:44760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:45354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:45626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:45834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:46832 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP2] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP4] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP6] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP0] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP3] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP1] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP5] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP7] [fused_moe] using default for (658, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56] INFO:     127.0.0.1:36966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:37780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:39394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:39612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:40120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:41926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:43060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:44530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:46004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:36650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:37460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:39974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:42688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:43758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:44456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:45374 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP2] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP4] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP6] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP0] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP3] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP1] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP7] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP5] [fused_moe] using default for (641, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56] INFO:     127.0.0.1:35698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:36320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:36838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:36970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:37592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:38464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:39946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:41544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:41906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:42462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:42918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:43934 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP2] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP4] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP6] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP0] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP3] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP1] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP7] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP5] [fused_moe] using default for (629, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56] INFO:     127.0.0.1:35756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:36078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:37668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:38580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:39204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:40084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:40878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:41446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:41628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:41970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:42064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:44626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:45782 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP2] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP4] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP6] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP0] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP3] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP1] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP7] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56 TP5] [fused_moe] using default for (614, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:56] INFO:     127.0.0.1:35742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:36144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:56] INFO:     127.0.0.1:37158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:38376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:38506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:43736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:43794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:43828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:44220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:44420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:46974 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP2] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP4] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP6] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP0] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP3] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP1] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP7] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP5] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57] INFO:     127.0.0.1:36796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:38340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:40026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP2] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP4] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP6] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP0] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP3] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP7] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP1] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP5] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57] INFO:     127.0.0.1:40948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:41520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:41890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:42256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:43024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:43636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:44050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:45240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:46186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57 TP0] Decode batch. #running-req: 598, #token: 89966, token usage: 0.09, cuda graph: False, gen throughput (token/s): 6664.13, #queue-req: 0, 
[2025-10-21 00:26:57] INFO:     127.0.0.1:35926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:36480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:36616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:36832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:37912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:38284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:39632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:39866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:40850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:41062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:43358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:43976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:46806 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP4] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP2] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP6] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP0] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP3] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP1] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP5] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP7] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57] INFO:     127.0.0.1:35962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:36488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:37720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:41028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:41382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:42410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:42938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:44276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:44504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:46206 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP2] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP4] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP6] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP0] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP3] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP7] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP1] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP5] [fused_moe] using default for (566, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57] INFO:     127.0.0.1:36090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:36448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:38416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:38494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:38642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:39256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:39298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:39572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:39928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:40514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:40820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:40864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:41740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:41758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:42852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:44346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:44822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:45022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:45934 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP2] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP4] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP6] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP0] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP3] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP7] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP1] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP5] [fused_moe] using default for (546, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57] INFO:     127.0.0.1:37362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:37502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:38310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:38962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:41920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:42004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:43568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:43598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:44670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:44762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:44842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:44906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:45190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:45448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:46448 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP2] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP4] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP6] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP0] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP3] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP7] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP1] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP5] [fused_moe] using default for (531, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57] INFO:     127.0.0.1:44262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:45442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:45690 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP2] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP4] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP6] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP0] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP3] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP1] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP5] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP7] [fused_moe] using default for (528, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57] INFO:     127.0.0.1:36070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:36258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:36412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:40278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:40414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:40472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:40560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:42188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:44054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:45568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:45768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:46372 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP2] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP4] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP6] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP0] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP3] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP7] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP1] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57 TP5] [fused_moe] using default for (516, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:26:57] INFO:     127.0.0.1:36356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:36750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:36892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:38126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:38944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:39280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:40492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:40512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:43918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:45310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:45420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:46068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:35734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:36928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:38306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:38660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:39016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:41162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:41260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:41308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:57] INFO:     127.0.0.1:43416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:38188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:38250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:39666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:39990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:42546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:42796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:43512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:45054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:39562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:41602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:43684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:44014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:44044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:35662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:41434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:42732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:42826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:43008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:45540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:44896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:45436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:36468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:39168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:40188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:40354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:44236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:45220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:45478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:45636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:45688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:40002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:40254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:40460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:42778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:43078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:43098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:36372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:38404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:39798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:42120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:44204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:44694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:45950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:38646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:39582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:41700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:43402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:44748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:45124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:39486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:39768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:43472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:44816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:45724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:47142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:42144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:44804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:36906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:38078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:40292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:40344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:40812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:42966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:44024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:44654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:38832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:44074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:45678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:38438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:41324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:37432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:41558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:58] INFO:     127.0.0.1:46322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:37924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:38286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:38322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:39598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:42008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:42618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:43232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:47130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:35802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:37392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:38180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:38928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:39088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:40866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:41488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:41714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:37306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:39638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:41562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:37086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:37346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:38226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:42816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:43242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:47026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:36340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:36768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:37402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:40266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:40882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:41292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:35996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:36074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:39196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:39326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:41024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:38040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:40680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:40194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:43120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:35880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:35914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:37374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:37734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:47200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:47248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:43888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:47108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:47154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:35724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:39466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:41638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:41706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:41800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:42268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:45960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:38292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:38934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:37138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:47192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:47232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:40750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:41664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:43750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:37110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:39362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:39732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:41774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:44872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:46940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:26:59] INFO:     127.0.0.1:47218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00 TP0] Decode batch. #running-req: 270, #token: 52035, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6098.01, #queue-req: 0, 
[2025-10-21 00:27:00] INFO:     127.0.0.1:39736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:36326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:37044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:43866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:44518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:45338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:47048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:36714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:36990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:37208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:42646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:45668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:35844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:36648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:37658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:39428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:41368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:44604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:45700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:40544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:42600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:47080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:47182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:39948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:38774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:44846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:38528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:39120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:45860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:42354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:44534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:44942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:47086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:38488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:44862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:36886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:37174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:39932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:40998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:44122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:37184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:37224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:40226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:40438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:44314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:44880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:45524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:45996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:36438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:39368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:41234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:42258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:45904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:39076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:43988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:44934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:37130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:37330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:38554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:45162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:45618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:45836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:42170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:44684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:00] INFO:     127.0.0.1:46660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:38504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:41014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:35932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:36076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:42172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:43834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:36548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:44494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:37562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:40420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:43272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:45152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:41036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:42834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:45794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:44916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:45596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:45542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:38752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:44920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:45604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:36666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:40148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:45886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:37640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:40764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:45044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:45752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:36518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:39426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:40022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:45084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:45558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:35754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:44722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:45512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:36186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:38568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:43320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:43728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:44944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:47178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:35744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:37116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:45710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:37334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:37624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:41340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01 TP0] Decode batch. #running-req: 110, #token: 25593, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3841.88, #queue-req: 0, 
[2025-10-21 00:27:01] INFO:     127.0.0.1:40160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:41878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:42344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:45850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:38128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:42240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:44516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:44596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:37472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:39534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:44568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:01] INFO:     127.0.0.1:46386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:36916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:42448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:45562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:46248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:44324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:37682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:44640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:37710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:45802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:36310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:37424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:38600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:38740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:40162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:41250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:45346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:47214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:38806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:46138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:46638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:40198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:42802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:44936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:45012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:44730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:44776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:36510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:39398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:46538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:46648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:42648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:45264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:45552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:37058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:46500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:47236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:36602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:45870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:44190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:44996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:43682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:45390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:46720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:37574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:40964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:46050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:02] INFO:     127.0.0.1:47040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:40534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:46578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:46190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:46966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:46700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:46280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:45918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:46942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03 TP0] Decode batch. #running-req: 35, #token: 10472, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1751.38, #queue-req: 0, 
[2025-10-21 00:27:03] INFO:     127.0.0.1:45140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:46150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:45198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:46532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:45740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:47100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:42534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:44320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:39964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:36058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:40434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:41372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:45818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:44450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:03] INFO:     127.0.0.1:45432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:45488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:45074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:35730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:42676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:45238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:46718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:39626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04 TP0] Decode batch. #running-req: 12, #token: 4279, token usage: 0.00, cuda graph: True, gen throughput (token/s): 918.34, #queue-req: 0, 
[2025-10-21 00:27:04] INFO:     127.0.0.1:36052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:44982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:45804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:41776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:44214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:43668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:44740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:04] INFO:     127.0.0.1:45902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:05] INFO:     127.0.0.1:45462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:05 TP0] Decode batch. #running-req: 1, #token: 962, token usage: 0.00, cuda graph: True, gen throughput (token/s): 274.43, #queue-req: 0, 
[2025-10-21 00:27:05] INFO:     127.0.0.1:45968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:19] INFO:     127.0.0.1:53822 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-21 00:27:19 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-21 00:27:19] INFO:     127.0.0.1:53838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:19 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-21 00:27:19 TP0] Prefill batch. #new-seq: 23, #new-token: 23, #cached-token: 16706, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-21 00:27:19 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4336, token usage: 0.00, #running-req: 24, #queue-req: 0, 
[2025-10-21 00:27:20 TP0] Prefill batch. #new-seq: 54, #new-token: 54, #cached-token: 39389, token usage: 0.01, #running-req: 30, #queue-req: 0, 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP0] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP2] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP1] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP3] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP4] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP6] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP5] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP7] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP0] Prefill batch. #new-seq: 48, #new-token: 48, #cached-token: 34891, token usage: 0.01, #running-req: 84, #queue-req: 0, 
[2025-10-21 00:27:20 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 45364, token usage: 0.01, #running-req: 132, #queue-req: 0, 
[2025-10-21 00:27:20 TP0] Prefill batch. #new-seq: 51, #new-token: 51, #cached-token: 37181, token usage: 0.02, #running-req: 194, #queue-req: 0, 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP5] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP3] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP2] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP7] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP0] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP4] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP6] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP1] [fused_moe] using default for (51, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP0] Prefill batch. #new-seq: 67, #new-token: 67, #cached-token: 48536, token usage: 0.02, #running-req: 245, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP2] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP7] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP5] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP3] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP1] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP6] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP0] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP4] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:20 TP0] Prefill batch. #new-seq: 54, #new-token: 54, #cached-token: 39353, token usage: 0.02, #running-req: 312, #queue-req: 0, 
[2025-10-21 00:27:20 TP0] Prefill batch. #new-seq: 65, #new-token: 65, #cached-token: 47476, token usage: 0.03, #running-req: 366, #queue-req: 0, 
[2025-10-21 00:27:20 TP0] Prefill batch. #new-seq: 53, #new-token: 53, #cached-token: 38549, token usage: 0.03, #running-req: 431, #queue-req: 0, 
[2025-10-21 00:27:21 TP0] Prefill batch. #new-seq: 75, #new-token: 75, #cached-token: 54462, token usage: 0.03, #running-req: 484, #queue-req: 0, 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP0] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP2] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP6] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP1] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP4] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP3] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP5] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP7] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP0] Prefill batch. #new-seq: 53, #new-token: 53, #cached-token: 38435, token usage: 0.04, #running-req: 559, #queue-req: 0, 
[2025-10-21 00:27:21 TP0] Prefill batch. #new-seq: 86, #new-token: 86, #cached-token: 62523, token usage: 0.04, #running-req: 612, #queue-req: 0, 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP2] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP3] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP1] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP5] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP6] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP4] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP0] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP7] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4454, token usage: 0.04, #running-req: 698, #queue-req: 0, 
[2025-10-21 00:27:21 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 7888, token usage: 0.04, #running-req: 704, #queue-req: 0, 
[2025-10-21 00:27:21 TP0] Prefill batch. #new-seq: 46, #new-token: 46, #cached-token: 33518, token usage: 0.05, #running-req: 715, #queue-req: 0, 
[2025-10-21 00:27:21 TP0] Prefill batch. #new-seq: 47, #new-token: 47, #cached-token: 34287, token usage: 0.05, #running-req: 761, #queue-req: 0, 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP2] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP3] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP1] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP5] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP7] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP6] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP0] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:21 TP4] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:22 TP0] Prefill batch. #new-seq: 55, #new-token: 55, #cached-token: 40150, token usage: 0.05, #running-req: 808, #queue-req: 0, 
[2025-10-21 00:27:22 TP0] Prefill batch. #new-seq: 52, #new-token: 52, #cached-token: 37578, token usage: 0.06, #running-req: 863, #queue-req: 0, 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:22 TP3] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:22 TP2] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:22 TP0] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:22 TP1] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:22 TP5] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:22 TP7] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:22 TP6] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:22 TP4] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:22 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 44843, token usage: 0.06, #running-req: 915, #queue-req: 0, 
[2025-10-21 00:27:22 TP0] Prefill batch. #new-seq: 48, #new-token: 48, #cached-token: 35277, token usage: 0.06, #running-req: 976, #queue-req: 6, 
[2025-10-21 00:27:25] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:25 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-21 00:27:26 TP0] Decode batch. #running-req: 1024, #token: 93244, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1440.69, #queue-req: 294, 
[2025-10-21 00:27:26] INFO:     127.0.0.1:34034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-21 00:27:26] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:26 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-21 00:27:26] INFO:     127.0.0.1:53884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:27 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 735, token usage: 0.10, #running-req: 1023, #queue-req: 291, 
[2025-10-21 00:27:27] INFO:     127.0.0.1:54276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:27] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:27] INFO:     127.0.0.1:58138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:27] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:27 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 729, token usage: 0.11, #running-req: 1023, #queue-req: 290, 
[2025-10-21 00:27:27] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:27] INFO:     127.0.0.1:56630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:27] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:27] INFO:     127.0.0.1:59484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:27] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:27] INFO:     127.0.0.1:34020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6696, token usage: 0.11, #running-req: 1015, #queue-req: 281, 
[2025-10-21 00:27:28] INFO:     127.0.0.1:54092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:55120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:59510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3673, token usage: 0.11, #running-req: 1019, #queue-req: 276, 
[2025-10-21 00:27:28] INFO:     127.0.0.1:54584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:54906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:55030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:58412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:58400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6481, token usage: 0.11, #running-req: 1015, #queue-req: 267, 
[2025-10-21 00:27:28] INFO:     127.0.0.1:53860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:57150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:34128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3627, token usage: 0.11, #running-req: 1019, #queue-req: 262, 
[2025-10-21 00:27:28] INFO:     127.0.0.1:57030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:57418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:60570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:34106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4310, token usage: 0.11, #running-req: 1018, #queue-req: 256, 
[2025-10-21 00:27:28] INFO:     127.0.0.1:59782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:28] INFO:     127.0.0.1:33502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1448, token usage: 0.11, #running-req: 1022, #queue-req: 254, 
[2025-10-21 00:27:29] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:54530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:56430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:57272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:58700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:33736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5841, token usage: 0.11, #running-req: 1016, #queue-req: 246, 
[2025-10-21 00:27:29] INFO:     127.0.0.1:56060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:33086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:33674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:34156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6650, token usage: 0.11, #running-req: 1015, #queue-req: 237, 
[2025-10-21 00:27:29] INFO:     127.0.0.1:54174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:54752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:55650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:57894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:58964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:32900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:33156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:33460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:34582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6598, token usage: 0.11, #running-req: 1015, #queue-req: 228, 
[2025-10-21 00:27:29] INFO:     127.0.0.1:55150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:58236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:58818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:59158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:59346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:60888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:32998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29] INFO:     127.0.0.1:33144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:29 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5845, token usage: 0.11, #running-req: 1016, #queue-req: 220, 
[2025-10-21 00:27:30] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:57966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:34530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3659, token usage: 0.12, #running-req: 1019, #queue-req: 215, 
[2025-10-21 00:27:30] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:54060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:55256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:56964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:57736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4287, token usage: 0.12, #running-req: 1018, #queue-req: 209, 
[2025-10-21 00:27:30] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:55092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:59208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:59570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:59684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:60214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:33446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:33618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:33990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 7930, token usage: 0.12, #running-req: 1013, #queue-req: 198, 
[2025-10-21 00:27:30] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:54982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:58992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:59954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:33536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5124, token usage: 0.12, #running-req: 1017, #queue-req: 191, 
[2025-10-21 00:27:30] INFO:     127.0.0.1:55662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:55696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:56502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:59024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:59702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30] INFO:     127.0.0.1:60544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:30 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5086, token usage: 0.12, #running-req: 1017, #queue-req: 184, 
[2025-10-21 00:27:31] INFO:     127.0.0.1:54798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:55334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:55838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:57212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:58380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:60136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:60270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:33690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:34278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 10864, token usage: 0.12, #running-req: 1009, #queue-req: 169, 
[2025-10-21 00:27:31] INFO:     127.0.0.1:53986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:55382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:56848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6461, token usage: 0.12, #running-req: 1015, #queue-req: 160, 
[2025-10-21 00:27:31] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:59672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2891, token usage: 0.12, #running-req: 1020, #queue-req: 156, 
[2025-10-21 00:27:31] INFO:     127.0.0.1:54874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:59108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:59292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:60504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4374, token usage: 0.12, #running-req: 1018, #queue-req: 150, 
[2025-10-21 00:27:31] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:58170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:60390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:31] INFO:     127.0.0.1:34554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5157, token usage: 0.12, #running-req: 1017, #queue-req: 143, 
[2025-10-21 00:27:32] INFO:     127.0.0.1:54386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:54666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:55062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:55394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:57314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:58310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:59368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:59526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:60876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:60892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:33272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:33750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32 TP0] Prefill batch. #new-seq: 16, #new-token: 16, #cached-token: 11638, token usage: 0.12, #running-req: 1008, #queue-req: 127, 
[2025-10-21 00:27:32] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:55362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:57822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:32910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:33798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4426, token usage: 0.12, #running-req: 1018, #queue-req: 121, 
[2025-10-21 00:27:32] INFO:     127.0.0.1:54016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:57194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:58956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:60078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:60160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:60656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:60714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:33096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:33558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9648, token usage: 0.12, #running-req: 1011, #queue-req: 108, 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:32 TP2] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:32 TP1] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:32 TP0] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:32 TP5] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:32 TP6] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:32 TP4] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:32 TP3] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:32 TP7] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:32] INFO:     127.0.0.1:55038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:60626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:32836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32] INFO:     127.0.0.1:33588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:32 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6648, token usage: 0.12, #running-req: 1015, #queue-req: 99, 
[2025-10-21 00:27:33] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:55086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:56170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:56224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:56870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:58772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:59076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:59186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:33404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:33582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33 TP0] Prefill batch. #new-seq: 16, #new-token: 16, #cached-token: 11542, token usage: 0.12, #running-req: 1008, #queue-req: 83, 
[2025-10-21 00:27:33] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:56012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:56694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:58884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:60236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:33384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:33858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:33864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:34350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33 TP0] Prefill batch. #new-seq: 15, #new-token: 15, #cached-token: 11021, token usage: 0.12, #running-req: 1009, #queue-req: 68, 
[2025-10-21 00:27:33] INFO:     127.0.0.1:55024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:55952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:56616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:57364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:59084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:59340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:59562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:33948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8072, token usage: 0.12, #running-req: 1013, #queue-req: 57, 
[2025-10-21 00:27:33] INFO:     127.0.0.1:54960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:55594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:57340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:60770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:32938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:33326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:33568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:58940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:59462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:59752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:59766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33 TP0] Prefill batch. #new-seq: 16, #new-token: 16, #cached-token: 11743, token usage: 0.12, #running-req: 1008, #queue-req: 41, 
[2025-10-21 00:27:33] INFO:     127.0.0.1:54050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:55982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:57720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:58092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:59360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:59626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:60576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:60598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:60918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:32788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:33 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8760, token usage: 0.12, #running-req: 1012, #queue-req: 29, 
[2025-10-21 00:27:34 TP0] Decode batch. #running-req: 1012, #token: 120273, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5219.07, #queue-req: 29, 
[2025-10-21 00:27:34] INFO:     127.0.0.1:56270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:57064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:57168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:58284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:58682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:58842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:59230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:33960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6544, token usage: 0.13, #running-req: 1015, #queue-req: 20, 
[2025-10-21 00:27:34] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:55114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:55178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:55664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:57306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:60526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:60816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:32886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:34476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7316, token usage: 0.13, #running-req: 1014, #queue-req: 10, 
[2025-10-21 00:27:34] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:55988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:56854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:57294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:57744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:58328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:59796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:59956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:59958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:60286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:60640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:60988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7230, token usage: 0.13, #running-req: 1011, #queue-req: 0, 
[2025-10-21 00:27:34] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:57618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:59908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:60496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:33430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:33804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:34458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:34480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:54362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:54372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:58058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:59586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:34070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:34260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:34] INFO:     127.0.0.1:34450 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:34 TP5] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:34 TP2] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:34 TP6] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:34 TP0] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:34 TP4] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:34 TP1] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:34 TP3] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:34 TP7] [fused_moe] using default for (1002, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35] INFO:     127.0.0.1:54288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:55504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:56376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:58858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:60400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:33370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:34102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:34180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:54526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:58984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:60354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:60406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:33738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:33790 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP2] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP6] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP0] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP5] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP4] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP1] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP3] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP7] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35] INFO:     127.0.0.1:54000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:55580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:55780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:58028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:58480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:60836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:32940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:33706 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP2] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP6] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP4] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP0] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP5] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP1] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP3] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP7] [fused_moe] using default for (970, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35] INFO:     127.0.0.1:54776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:55282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:58224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:58408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:60466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:34004 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP2] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP6] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP4] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP0] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP5] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP1] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP3] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP7] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35] INFO:     127.0.0.1:54972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:57040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:58766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:33412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:33730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:56550 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:59040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:60998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:33112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:34188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:54076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:54562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:54706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:58180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:33682 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP2] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP6] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP4] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP0] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP5] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP1] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP3] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP7] [fused_moe] using default for (935, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35] INFO:     127.0.0.1:54654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:55746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:57718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:59338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:33052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:33376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:35] INFO:     127.0.0.1:33946 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP2] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP6] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP4] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP0] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:35 TP5] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP1] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP3] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP7] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:32942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:33072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:33218 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP2] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP4] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP6] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP0] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP5] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP1] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP3] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP7] [fused_moe] using default for (915, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36] INFO:     127.0.0.1:54576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:55006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:56126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:58710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:59092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:60172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:60198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:33240 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP2] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP4] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP6] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP0] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP5] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP1] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP3] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP7] [fused_moe] using default for (901, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:56290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:56900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:58476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:59656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:32920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:33966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:34708 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP2] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP4] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP6] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP0] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP5] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP1] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP3] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP7] [fused_moe] using default for (890, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:56794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:59174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:60946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:33004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:33174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:33512 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP2] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP4] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP6] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP0] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP5] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP1] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP3] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP7] [fused_moe] using default for (877, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:58222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:34232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:34444 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP2] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP6] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP4] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP0] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP5] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP1] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP3] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP7] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:55070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:58256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:59334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:59926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:60660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:60954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:33762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:53890 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP2] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP6] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP4] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP0] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP5] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP1] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP3] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP7] [fused_moe] using default for (853, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36] INFO:     127.0.0.1:54442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:59844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:60592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:33660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:33726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:33932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:55418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:60582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:33604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:34272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:36] INFO:     127.0.0.1:34316 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP2] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP6] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP4] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:36 TP0] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP5] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP1] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP3] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP7] [fused_moe] using default for (835, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:55012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:55134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:55226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:56332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:59246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:33352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:33836 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP2] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP6] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP4] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP0] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP5] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP1] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP3] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP7] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:55892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:58022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:59122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:33944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:34090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:34654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:56842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:58306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:59900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:60718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:33896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:34038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:34360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:35066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:35286 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP2] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP4] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP6] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP0] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP5] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP1] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP3] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP7] [fused_moe] using default for (801, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37] INFO:     127.0.0.1:54038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:58374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:59922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:60614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:60886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:32930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:33298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:33474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:33552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:33846 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP2] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP4] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP6] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP0] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP5] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP1] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP3] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP7] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:57754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:58620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:58914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:34548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:34724 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP2] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP4] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP6] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP0] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP5] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP1] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP3] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP7] [fused_moe] using default for (781, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:55358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:33868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:34198 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP2] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP6] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP4] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP0] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP5] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP1] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP3] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP7] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:55964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:56284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:56516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:58524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:58644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:59528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:32970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:34048 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP2] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP6] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP4] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP0] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP5] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP1] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP3] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP7] [fused_moe] using default for (762, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37] INFO:     127.0.0.1:53934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:54160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:54226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:54234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:55776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:58040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:59156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:59448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:34276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:37] INFO:     127.0.0.1:35762 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP2] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP4] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP6] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP0] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP5] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP1] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP3] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:37 TP7] [fused_moe] using default for (752, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP2] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP6] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP4] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP0] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP5] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP1] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:56990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:57712 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP3] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP7] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:60246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:33400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:34284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:35782 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP2] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP4] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP6] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38] INFO:     127.0.0.1:55192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:57222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:57368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:58206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:59412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP0] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38] INFO:     127.0.0.1:34168 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38] INFO:     127.0.0.1:34470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38 TP5] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38] INFO:     127.0.0.1:34520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:34672 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP1] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP3] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP7] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38] INFO:     127.0.0.1:56834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:60122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP2] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP6] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP4] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP0] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP5] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP1] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP3] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP7] [fused_moe] using default for (726, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:55884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:56302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:57666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:58844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:58996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:59432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:33710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:34576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP2] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP6] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP4] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP0] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP5] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP1] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP3] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP7] [fused_moe] using default for (714, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38] INFO:     127.0.0.1:54108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:54186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:54394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:57414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:60256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:60386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:60438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:60540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:33282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:34506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:35338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:35472 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP6] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP4] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP5] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP2] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP0] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP7] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP1] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP3] [fused_moe] using default for (700, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38] INFO:     127.0.0.1:55642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:57622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:58042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:60766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:33420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:33490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:34432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:34634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:34804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:35684 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP2] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP4] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP6] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP0] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP5] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP1] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP3] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP7] [fused_moe] using default for (685, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38] INFO:     127.0.0.1:54412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:60114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:60798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:33172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:33788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:34080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:34864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:35392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:37006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:53988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:54250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:55540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:56806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:58208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:58526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:34340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:35752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:38] INFO:     127.0.0.1:35902 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP0] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP2] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP4] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP6] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP1] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP5] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP3] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:38 TP7] [fused_moe] using default for (662, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:57542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:58592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:35440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:36072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP0] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP4] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP2] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP6] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP5] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP1] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP3] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP7] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39] INFO:     127.0.0.1:54118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:55420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:58538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:60820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:33180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:33970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:35172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:54792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:57390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:57962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:58568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:33028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:33916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:34386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:35468 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP2] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP4] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP6] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP0] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP5] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP1] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP3] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP7] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP0] Decode batch. #running-req: 641, #token: 93539, token usage: 0.10, cuda graph: False, gen throughput (token/s): 6396.72, #queue-req: 0, 
[2025-10-21 00:27:39] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:54716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:55048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:57026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:58000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:58082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:59264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:59770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:33044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:33968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:34534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:34692 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP2] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP6] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP4] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP0] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP5] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP1] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP3] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP7] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:57252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:58926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:59688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:34376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:35870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:36754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:37132 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP2] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP4] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP6] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP0] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP5] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP1] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP3] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP7] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39] INFO:     127.0.0.1:53892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:53940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:54466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:59716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:34146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:34934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:36324 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP2] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP4] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP6] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP0] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP5] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP1] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP3] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP7] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:55850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:34860 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP2] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP6] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP4] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP0] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP5] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP1] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP3] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP7] [fused_moe] using default for (585, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:54510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:55568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:33528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:33778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:34566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:34890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:35372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:36344 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP2] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP4] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP6] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP0] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP5] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP1] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP3] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP7] [fused_moe] using default for (573, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39] INFO:     127.0.0.1:54346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:56482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:59220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:60604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:60726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:60778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:33154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:33554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:39] INFO:     127.0.0.1:36004 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP2] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP4] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP6] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP0] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP5] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP1] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP3] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:39 TP7] [fused_moe] using default for (563, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40] INFO:     127.0.0.1:55100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:59630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:33058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:37046 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP2] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP6] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP4] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP0] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP5] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP1] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP3] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP7] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40] INFO:     127.0.0.1:54650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:56122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:58506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:33020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:33224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:33354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:36592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:59318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:33734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35822 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP2] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP4] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP6] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP0] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP5] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP1] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP3] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP7] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40] INFO:     127.0.0.1:54376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:54454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:36474 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP2] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP4] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP6] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP0] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP5] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP1] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP3] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40 TP7] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:27:40] INFO:     127.0.0.1:53920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:56998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:58788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:33338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:33744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:36166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:57910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:33222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:57696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:59832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:59928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:36636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:55348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:57888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:33812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:57178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:58554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:33256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:36310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:32904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:33128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:36540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:55992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:56696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:59022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:36104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:59134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:60094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:33090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:33194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:34774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:35598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:40] INFO:     127.0.0.1:36610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:57082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:32956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:33204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:37034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:59962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:34906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:37262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:57816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:58798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:58810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:33616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:34142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:37078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:37256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:56474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:37010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:57190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:59560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:60676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:32980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:34178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:34346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:37128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:59282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:34306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:57592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:59396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:60008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:33688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:54402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:37248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:55682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:60506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:60754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:32812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:33362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:60964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:37176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:55312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:58012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:32978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:34624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:34726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:60422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:56246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:57284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:57604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:60350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:34650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:36732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:37002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:37240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:34002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:34670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:41] INFO:     127.0.0.1:35356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:59160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:34744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:58340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:55392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:55556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:59964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:59996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:37302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:56356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:37268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:56970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42 TP0] Decode batch. #running-req: 299, #token: 56936, token usage: 0.06, cuda graph: True, gen throughput (token/s): 5995.31, #queue-req: 0, 
[2025-10-21 00:27:42] INFO:     127.0.0.1:53918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:37050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:37082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:37306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:37326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:60058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:33810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:34024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:34968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:37296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:55794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:57790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:37298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:56714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:33882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:34920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:55302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:34814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:56934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:57526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:34822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:37206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:59730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:32824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:34246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:54846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:34714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:37214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:32774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:37160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:37288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:57502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:57946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:33900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:53954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:35806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:58660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:34950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:36712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:42] INFO:     127.0.0.1:37104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:55628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:34954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:34364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:34642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:54012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:57292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:34212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:37090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:56828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:34098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:37042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:57730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:58366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:60318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:37236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:54674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:59522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:60562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:34290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:34788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:60334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:34974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:37024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:54834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:56544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:37152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:58416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:59010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:60302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:55996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:60982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:53968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:33978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:55268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:37068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:58414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:33390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:34606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:60970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:37058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:55232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:58030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:59638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:36930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:43] INFO:     127.0.0.1:35710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:34988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:36388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:36416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:36676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:37144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:35232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:35844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:36920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:35144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:60488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:34844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:36688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44 TP0] Decode batch. #running-req: 117, #token: 27368, token usage: 0.03, cuda graph: True, gen throughput (token/s): 4045.62, #queue-req: 0, 
[2025-10-21 00:27:44] INFO:     127.0.0.1:55296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:55162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:59728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:33866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:35828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:36146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:58462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:58758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:59966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:33438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:35394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:36138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:37116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:37284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:55278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:35696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:35948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:36770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:54342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:34656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:34686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:37308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:34594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:34876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:36514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:37184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:34400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:34734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:56232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:58696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:55210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:59196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:56580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:60542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:35896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:36126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:36218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:57806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:35028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:35970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:35196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:35112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:36822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:44] INFO:     127.0.0.1:54992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:60784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:32840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:36716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:35646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:34466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:35376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:35974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:36654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:37250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:37318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:36694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:36884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:58440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:60180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:35470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:36056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:36338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:36876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:60930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:32982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:33822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:36498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:36404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:34852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:35820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45 TP0] Decode batch. #running-req: 38, #token: 10852, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1891.75, #queue-req: 0, 
[2025-10-21 00:27:45] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:36036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:37196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:35246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:37096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:56684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:36264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:45] INFO:     127.0.0.1:36748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:35276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:37242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:34828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:34328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:35912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:36212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:34286 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:35496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:36908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:35584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:37228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:53904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:35092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:35188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:54764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:35348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46 TP0] Decode batch. #running-req: 6, #token: 2324, token usage: 0.00, cuda graph: True, gen throughput (token/s): 869.95, #queue-req: 0, 
[2025-10-21 00:27:46] INFO:     127.0.0.1:55490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:35884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:46] INFO:     127.0.0.1:60736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:47] INFO:     127.0.0.1:34292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:47] INFO:     127.0.0.1:35964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:27:47 TP0] Decode batch. #running-req: 1, #token: 983, token usage: 0.00, cuda graph: True, gen throughput (token/s): 138.38, #queue-req: 0, 
[2025-10-21 00:27:47] INFO:     127.0.0.1:35560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:01] INFO:     127.0.0.1:38680 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-21 00:28:01 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-21 00:28:01] INFO:     127.0.0.1:38686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:01 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-21 00:28:01 TP0] Prefill batch. #new-seq: 38, #new-token: 38, #cached-token: 27676, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:01 TP0] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:01 TP2] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:01 TP1] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:01 TP4] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:01 TP6] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:01 TP7] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:01 TP3] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:01 TP5] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP0] Prefill batch. #new-seq: 49, #new-token: 49, #cached-token: 35594, token usage: 0.01, #running-req: 39, #queue-req: 0, 
[2025-10-21 00:28:02 TP0] Prefill batch. #new-seq: 31, #new-token: 31, #cached-token: 22581, token usage: 0.01, #running-req: 88, #queue-req: 0, 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP4] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP7] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP2] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP5] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP1] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP3] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP6] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP0] [fused_moe] using default for (31, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 40948, token usage: 0.01, #running-req: 119, #queue-req: 0, 
[2025-10-21 00:28:02 TP0] Prefill batch. #new-seq: 50, #new-token: 50, #cached-token: 36393, token usage: 0.01, #running-req: 175, #queue-req: 0, 
[2025-10-21 00:28:02 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 45094, token usage: 0.02, #running-req: 225, #queue-req: 0, 
[2025-10-21 00:28:02 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 40670, token usage: 0.02, #running-req: 287, #queue-req: 0, 
[2025-10-21 00:28:02 TP0] Prefill batch. #new-seq: 66, #new-token: 66, #cached-token: 47974, token usage: 0.03, #running-req: 343, #queue-req: 0, 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP7] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP5] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP4] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP6] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP2] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP1] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP3] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP0] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:02 TP0] Prefill batch. #new-seq: 62, #new-token: 62, #cached-token: 45377, token usage: 0.03, #running-req: 409, #queue-req: 0, 
[2025-10-21 00:28:02 TP0] Prefill batch. #new-seq: 50, #new-token: 50, #cached-token: 36267, token usage: 0.03, #running-req: 471, #queue-req: 0, 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP4] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP7] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP6] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP5] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP2] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP1] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP0] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP3] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8011, token usage: 0.03, #running-req: 521, #queue-req: 0, 
[2025-10-21 00:28:03 TP0] Prefill batch. #new-seq: 47, #new-token: 47, #cached-token: 34040, token usage: 0.04, #running-req: 532, #queue-req: 0, 
[2025-10-21 00:28:03 TP0] Prefill batch. #new-seq: 47, #new-token: 47, #cached-token: 34214, token usage: 0.04, #running-req: 579, #queue-req: 0, 
[2025-10-21 00:28:03 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 40664, token usage: 0.04, #running-req: 626, #queue-req: 0, 
[2025-10-21 00:28:03 TP0] Prefill batch. #new-seq: 53, #new-token: 53, #cached-token: 38641, token usage: 0.05, #running-req: 682, #queue-req: 0, 
[2025-10-21 00:28:03 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 44440, token usage: 0.05, #running-req: 735, #queue-req: 0, 
[2025-10-21 00:28:03 TP0] Prefill batch. #new-seq: 57, #new-token: 57, #cached-token: 41642, token usage: 0.05, #running-req: 796, #queue-req: 0, 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP2] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP1] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP3] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP7] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP0] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP4] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP6] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:03 TP5] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:04 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 44140, token usage: 0.06, #running-req: 853, #queue-req: 0, 
[2025-10-21 00:28:04 TP0] Prefill batch. #new-seq: 60, #new-token: 60, #cached-token: 44114, token usage: 0.06, #running-req: 914, #queue-req: 0, 
[2025-10-21 00:28:04 TP0] Prefill batch. #new-seq: 50, #new-token: 50, #cached-token: 36773, token usage: 0.06, #running-req: 974, #queue-req: 14, 
[2025-10-21 00:28:07] INFO:     127.0.0.1:41540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:07 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-21 00:28:08] INFO:     127.0.0.1:42920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:08 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 790, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-21 00:28:08] INFO:     127.0.0.1:47044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:08 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 718, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-21 00:28:08] INFO:     127.0.0.1:41574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:08 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 701, token usage: 0.10, #running-req: 1023, #queue-req: 291, 
[2025-10-21 00:28:09 TP0] Decode batch. #running-req: 1024, #token: 100612, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1730.32, #queue-req: 291, 
[2025-10-21 00:28:09] INFO:     127.0.0.1:38740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:09] INFO:     127.0.0.1:39154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:09] INFO:     127.0.0.1:42234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:09] INFO:     127.0.0.1:39644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:09] INFO:     127.0.0.1:43258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:09] INFO:     127.0.0.1:43272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:09 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2224, token usage: 0.11, #running-req: 1021, #queue-req: 288, 
[2025-10-21 00:28:09] INFO:     127.0.0.1:39838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:09] INFO:     127.0.0.1:40646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:09] INFO:     127.0.0.1:41414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:09 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4384, token usage: 0.11, #running-req: 1018, #queue-req: 282, 
[2025-10-21 00:28:09] INFO:     127.0.0.1:38920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:09] INFO:     127.0.0.1:44330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:09] INFO:     127.0.0.1:47026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:09 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2252, token usage: 0.11, #running-req: 1021, #queue-req: 279, 
[2025-10-21 00:28:10] INFO:     127.0.0.1:39290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:39710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:41292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:43212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:44310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:44808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:47182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5748, token usage: 0.11, #running-req: 1016, #queue-req: 271, 
[2025-10-21 00:28:10] INFO:     127.0.0.1:39652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:40250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1416, token usage: 0.11, #running-req: 1022, #queue-req: 269, 
[2025-10-21 00:28:10] INFO:     127.0.0.1:41812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:42210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:42394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:43312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:43462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:43784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:45166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:47468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5786, token usage: 0.11, #running-req: 1016, #queue-req: 261, 
[2025-10-21 00:28:10] INFO:     127.0.0.1:41952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:43610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:47166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:47304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2874, token usage: 0.11, #running-req: 1020, #queue-req: 257, 
[2025-10-21 00:28:10] INFO:     127.0.0.1:39060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:39136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:42874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:43136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:45384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:10] INFO:     127.0.0.1:46504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4316, token usage: 0.11, #running-req: 1018, #queue-req: 251, 
[2025-10-21 00:28:11] INFO:     127.0.0.1:38718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:39266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:40734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:40860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:41522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:46746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5143, token usage: 0.11, #running-req: 1017, #queue-req: 244, 
[2025-10-21 00:28:11] INFO:     127.0.0.1:39272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:39696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:40366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:41142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:42074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:42708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:46144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:46348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:46656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:47228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7358, token usage: 0.11, #running-req: 1014, #queue-req: 234, 
[2025-10-21 00:28:11] INFO:     127.0.0.1:39856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:42548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:42898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:44840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:45916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:46200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:46480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5155, token usage: 0.11, #running-req: 1017, #queue-req: 227, 
[2025-10-21 00:28:11] INFO:     127.0.0.1:40594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:42784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:45670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:46192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2986, token usage: 0.12, #running-req: 1020, #queue-req: 223, 
[2025-10-21 00:28:11] INFO:     127.0.0.1:38960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:39474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:39526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:39980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:41706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:41990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:43352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:43540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:44198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:11] INFO:     127.0.0.1:44902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7230, token usage: 0.12, #running-req: 1014, #queue-req: 213, 
[2025-10-21 00:28:12] INFO:     127.0.0.1:39040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:39804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:41938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:43120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:43714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:44058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:44382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:46528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:47688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6416, token usage: 0.12, #running-req: 1015, #queue-req: 204, 
[2025-10-21 00:28:12] INFO:     127.0.0.1:39634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:39760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:40280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:41238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:43216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:43786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:46490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:47018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5858, token usage: 0.12, #running-req: 1016, #queue-req: 196, 
[2025-10-21 00:28:12] INFO:     127.0.0.1:38702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:40334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:40382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:40454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:40608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:42010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:44340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:44464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:44914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:45028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:47606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8018, token usage: 0.12, #running-req: 1013, #queue-req: 185, 
[2025-10-21 00:28:12] INFO:     127.0.0.1:40070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:40512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:40562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:40578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:40676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:41264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:41372 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:44712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:44960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12] INFO:     127.0.0.1:45370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:12 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7225, token usage: 0.12, #running-req: 1014, #queue-req: 175, 
[2025-10-21 00:28:13] INFO:     127.0.0.1:38898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:40130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:40980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:41618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:41954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:43368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:44468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:45044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:45446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:47346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 7913, token usage: 0.12, #running-req: 1013, #queue-req: 164, 
[2025-10-21 00:28:13] INFO:     127.0.0.1:43088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:44124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:46352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:46482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2908, token usage: 0.12, #running-req: 1020, #queue-req: 160, 
[2025-10-21 00:28:13] INFO:     127.0.0.1:40816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:42934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:44544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:45310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13 TP0] Prefill batch. #new-seq: 6, #new-token: 6, #cached-token: 4355, token usage: 0.12, #running-req: 1018, #queue-req: 154, 
[2025-10-21 00:28:13] INFO:     127.0.0.1:38888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:39150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:39542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:40402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:40848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:43144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:44436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:45836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:45950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:46308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7315, token usage: 0.12, #running-req: 1014, #queue-req: 144, 
[2025-10-21 00:28:13] INFO:     127.0.0.1:38834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:38926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:40144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:40752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:43376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:44294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:45218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:13] INFO:     127.0.0.1:45868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5919, token usage: 0.12, #running-req: 1016, #queue-req: 136, 
[2025-10-21 00:28:14] INFO:     127.0.0.1:39158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:39354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:40104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:42130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:44666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:45662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:45680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:46768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:46930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6464, token usage: 0.12, #running-req: 1015, #queue-req: 127, 
[2025-10-21 00:28:14] INFO:     127.0.0.1:40812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:41354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:42094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:43196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:43384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:43564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:44128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:44210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:46678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:46792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7424, token usage: 0.12, #running-req: 1014, #queue-req: 117, 
[2025-10-21 00:28:14] INFO:     127.0.0.1:39028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:40028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:41564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:41646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:41674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:42644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:43738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:44906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:45492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:46032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:46168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:46562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8828, token usage: 0.12, #running-req: 1012, #queue-req: 105, 
[2025-10-21 00:28:14] INFO:     127.0.0.1:38946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:39288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:40218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:40378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:40872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:41062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:41530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:41634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:41654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:42364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:43290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:44002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:45440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:46578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:46724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:46874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14 TP0] Prefill batch. #new-seq: 17, #new-token: 17, #cached-token: 12429, token usage: 0.12, #running-req: 1007, #queue-req: 88, 
[2025-10-21 00:28:14] INFO:     127.0.0.1:39910 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:40056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:40152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:42346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:43882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:46436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:14] INFO:     127.0.0.1:46594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5760, token usage: 0.12, #running-req: 1016, #queue-req: 80, 
[2025-10-21 00:28:15] INFO:     127.0.0.1:38882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:39030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:39494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:39992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:40342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:40666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:41108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:41848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:43026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:43306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:43504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:45804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:46426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:46866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15 TP0] Prefill batch. #new-seq: 14, #new-token: 14, #cached-token: 10302, token usage: 0.12, #running-req: 1010, #queue-req: 66, 
[2025-10-21 00:28:15] INFO:     127.0.0.1:40318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:43008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:43980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:44834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:46598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3699, token usage: 0.13, #running-req: 1019, #queue-req: 61, 
[2025-10-21 00:28:15] INFO:     127.0.0.1:39826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:39888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:42020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:42524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:42904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:44378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:44898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:45386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:45426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:45970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:46576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8861, token usage: 0.13, #running-req: 1012, #queue-req: 49, 
[2025-10-21 00:28:15] INFO:     127.0.0.1:41234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:41480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:41988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:42354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:44110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:44168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:44278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:44312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:44578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:45060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:45566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:45712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:46340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:46392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:47656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15] INFO:     127.0.0.1:47682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:15 TP0] Prefill batch. #new-seq: 16, #new-token: 16, #cached-token: 11637, token usage: 0.13, #running-req: 1008, #queue-req: 33, 
[2025-10-21 00:28:16] INFO:     127.0.0.1:39226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:41250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:41330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:41724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:42624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:42778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:43222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:43514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:44208 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:44396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:46948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8752, token usage: 0.13, #running-req: 1012, #queue-req: 21, 
[2025-10-21 00:28:16] INFO:     127.0.0.1:38970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:40430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:41024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:41740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:41754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:42400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:42568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:43644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:43926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:44044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:45624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:45904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:47512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9490, token usage: 0.13, #running-req: 1011, #queue-req: 8, 
[2025-10-21 00:28:16] INFO:     127.0.0.1:39952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:40120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:40230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:42126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:42458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:45430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:47148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:47390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5768, token usage: 0.13, #running-req: 1016, #queue-req: 0, 
[2025-10-21 00:28:16] INFO:     127.0.0.1:40164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:44594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:44716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:44720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:45256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:45304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:45348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:46460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:46814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:39210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:40620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:44672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:46354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:47096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:16] INFO:     127.0.0.1:47580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:40780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:41388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:42098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:43052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:44384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:44704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:45174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:45396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:45806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:47274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:47672 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP0] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP3] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP2] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP4] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP7] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP6] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP1] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP5] [fused_moe] using default for (996, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17] INFO:     127.0.0.1:38956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:40508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:41046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:42496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:43112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:43412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:43450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:44636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:45280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:46688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:46748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:46784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:47108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:47324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17 TP0] Decode batch. #running-req: 996, #token: 120646, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5086.24, #queue-req: 0, 
[2025-10-21 00:28:17] INFO:     127.0.0.1:39612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:40012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:42844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:42962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:43690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:44406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:44700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:45478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP2] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP4] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP6] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP1] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP3] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP0] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP5] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP7] [fused_moe] using default for (968, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17] INFO:     127.0.0.1:38990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:41294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:43106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:46996 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP2] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP0] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP4] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP6] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP3] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP1] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP7] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP5] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17] INFO:     127.0.0.1:38852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:40554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:40740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:42140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:43626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:46404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:46632 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP2] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP0] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP6] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP4] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP3] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP7] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP1] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP5] [fused_moe] using default for (952, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17] INFO:     127.0.0.1:39512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:45792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:46184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:46298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:46712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:47282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:40466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:42046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:42676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:43404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:43650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:43874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:46658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:39310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:40788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:41178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:41928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:42532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:43674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:43812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:44988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:46084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:46446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:17] INFO:     127.0.0.1:47624 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP2] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP3] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP0] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP4] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP6] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP7] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP1] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:17 TP5] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18] INFO:     127.0.0.1:39128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:39218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:39764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:39938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:40778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:44160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:46146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:46272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:46770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:46924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:38756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:38786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:39002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:39668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:40894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:41146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:41668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:44770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:44924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:45542 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP2] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP4] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP6] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP3] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP0] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP7] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP1] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP5] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18] INFO:     127.0.0.1:41490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:41566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:43634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:43636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:44984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:45652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:39666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:40970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:44452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:44634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:45454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:45946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:46056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:46212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:47676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:47754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:39352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:39434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:39518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:40246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:41194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:41602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:41880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:43596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:43708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:43894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:44022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:45732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:46544 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP2] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP4] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP6] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP0] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP3] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP7] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP1] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP5] [fused_moe] using default for (860, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18] INFO:     127.0.0.1:39726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:39730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:40794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:41442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:45200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:46938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP2] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP4] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP6] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP3] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP0] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP7] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP1] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP5] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18] INFO:     127.0.0.1:40050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:40168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:43374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:43494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:43964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:44144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:44674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:45078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:45880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:46654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:46920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:47788 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP2] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP4] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP6] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP3] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP0] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP7] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP1] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP5] [fused_moe] using default for (839, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18] INFO:     127.0.0.1:39062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:39192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:39436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:41280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:41906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:42690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:43782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:45394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:46614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:46900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:47362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:18] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP2] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP4] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP6] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP3] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP0] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP7] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP1] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:18 TP5] [fused_moe] using default for (824, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19] INFO:     127.0.0.1:38742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:39852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:39894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:42830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:45952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:46332 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP2] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP4] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP6] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP3] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP0] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP7] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP1] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP5] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19] INFO:     127.0.0.1:40252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:40994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:42106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:44074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:44084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:44972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:45134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:45242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:45466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:47750 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP2] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP4] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP6] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP3] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP0] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP7] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP1] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP5] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19] INFO:     127.0.0.1:39112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:42048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:45506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:47048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:47524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:48182 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP2] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP4] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP6] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP3] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP0] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP7] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP1] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP5] [fused_moe] using default for (795, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19] INFO:     127.0.0.1:39256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:41768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:41796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:42572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:44376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:44660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:45436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:46150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:46328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:46558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:48344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:39176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:42096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:42716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:46286 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP2] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP4] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP6] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP3] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP0] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP7] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP1] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP5] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19] INFO:     127.0.0.1:40686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:40924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:41078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:41626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:41692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:47260 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP2] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP4] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP6] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP0] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP3] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP7] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP1] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP5] [fused_moe] using default for (768, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:38930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:39502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:42846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:44246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:44574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:46008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP2] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP4] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP6] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP0] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP3] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP7] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP1] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19 TP5] [fused_moe] using default for (758, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:19] INFO:     127.0.0.1:41716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:42602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:43754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:44054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:44532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:46414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:47330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:48398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:19] INFO:     127.0.0.1:48834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:39920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:42034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:43742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:44266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:45012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:45604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:47928 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP2] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP4] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP6] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP0] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP3] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP7] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP1] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP5] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP2] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP4] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP6] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP1] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP3] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP0] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP5] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP7] [fused_moe] using default for (733, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20] INFO:     127.0.0.1:38752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:42160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:47224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:47440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:47542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:38800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:39382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:39796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:41312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:41840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:42444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:45978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:46698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:46858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:38948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:38962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:39016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:39480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:45612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:45696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:47484 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP2] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP0] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP6] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP4] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP3] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP7] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP1] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP5] [fused_moe] using default for (717, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20] INFO:     127.0.0.1:39590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:41998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:42158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:42420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:42892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:44260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:45056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:45234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:45342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:46202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:46316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:47614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:47642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:48728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:39416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:40096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:42310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:42948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:43322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:46096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:46442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:46500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:47070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:48040 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP2] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP6] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP0] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP4] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP3] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP7] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP1] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP5] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20] INFO:     127.0.0.1:38814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:40258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:41580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:42768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:42868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:43490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:44518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:45000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:45154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:45708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:48454 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP2] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP6] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP4] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP0] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP3] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP7] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP1] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20 TP5] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:20] INFO:     127.0.0.1:38868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:38984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:40204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:41316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:41430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:41636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:42330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:43334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:44870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:46280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:46964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:47964 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:48490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:48972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:39788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:41110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:43510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:43666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:43916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:43948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:45380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:46912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:47240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:47706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:48270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:20] INFO:     127.0.0.1:49180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:39148 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:40502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:40786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:41218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:41404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:41864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:43378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:43776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:45626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:46244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:47374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:48506 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP2] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP6] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP4] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP0] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP3] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP7] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP1] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP5] [fused_moe] using default for (638, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21] INFO:     127.0.0.1:38846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:39538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:40826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:41436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:41760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:42522 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:43102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:43150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:44486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:44824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:45276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:45296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:47634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:47662 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP2] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP4] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP6] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP3] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP0] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP7] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP1] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP5] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21] INFO:     127.0.0.1:39968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:40292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:41510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:42196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:43350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:43900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:44414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:44602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:44890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:46120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:46972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:47548 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP2] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP4] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP6] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP0] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP3] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP7] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP1] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP5] [fused_moe] using default for (612, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21] INFO:     127.0.0.1:38822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:40178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:40702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:41698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:42774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:43932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:44240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:44558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:45570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:47828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:48790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:49730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:39548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:42166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:42664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:42978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:43524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:44478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:46194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:47206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP2] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP4] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP6] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP3] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP0] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP7] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP1] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP5] [fused_moe] using default for (588, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21] INFO:     127.0.0.1:38924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:39274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:39426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:40724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:41008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:41548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:41742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:42070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:42856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:44318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:45412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:47190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:48004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:48412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:48968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:39366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:39574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:39662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:39706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:40566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:46176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:46520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:47898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:49112 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP2] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP4] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP6] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP0] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP3] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP7] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP1] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP5] [fused_moe] using default for (564, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21] INFO:     127.0.0.1:38796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:39886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:40642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:41862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:41920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:42276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:42654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:43956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:45142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:45488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:45508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:45584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:47882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:48026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:49902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:40774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:42822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:44432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:44858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:46128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:21] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP2] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP4] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP6] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP3] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP0] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP7] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP1] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:21 TP5] [fused_moe] using default for (540, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP0] Decode batch. #running-req: 548, #token: 84679, token usage: 0.09, cuda graph: False, gen throughput (token/s): 6496.79, #queue-req: 0, 
[2025-10-21 00:28:22] INFO:     127.0.0.1:42100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:45932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:46048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:47982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:49556 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP2] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP4] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP6] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP0] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP3] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP7] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP1] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP5] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22] INFO:     127.0.0.1:40908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:41582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:41686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:43294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:44038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:44106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:44498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:45640 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:46764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:47118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:49392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP2] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP4] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP6] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP1] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP3] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP0] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP5] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22 TP7] [fused_moe] using default for (519, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:22] INFO:     127.0.0.1:40040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:40324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:40352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:40496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:41306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:44130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:44852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:47086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:39818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:40568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:41094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:45634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:47220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48314 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:49250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:40084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:40632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:40936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:42800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:44804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:45106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:46258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:47762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:39408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:40350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:41422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:42266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:42380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:42954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:43580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:44944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:45186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:39052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:41206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:42692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:44690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:46000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:46776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:47158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:47844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:38784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:42280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:42732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:42926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:44618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:44882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:49620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:46292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:46850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:49374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:41392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:44386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:45926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:46190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:49516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:41474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:41914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:42340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:43422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:46162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:49202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:38726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:40438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:41036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:42738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:43478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:46238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:49468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:38768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:39428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:45120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:46072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:42216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:42506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:42638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:43548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:45262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:48294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:49774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:22] INFO:     127.0.0.1:49820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:44648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:47250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:50064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:39170 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:41962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:41976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:42752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:43188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:44036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:46736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:47992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:50266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:43092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:44654 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:45832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:46022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:41784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:45318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:44090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:44374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:45760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:47052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:39786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:40412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:44236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:45322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:40388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:41458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:43798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:44782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:41164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:41220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:45558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:46368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:40534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:44006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:45382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:46662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:47786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:40660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:45744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:42806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:43836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:46016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:46798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:47988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:40482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:42390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:45206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:45984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:39684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:40810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:41112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:46618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:50016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:47004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:47316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:49952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:39082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:40718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:43062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:43694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:46376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:48336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:23] INFO:     127.0.0.1:50360 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:40136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:43174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:40882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:43308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:46888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:50262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:50326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:42294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:44736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:50032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:40098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:40196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:40960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:42694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:43848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:50324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:44184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:45326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:50296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:40524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:44762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:50072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:41506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:44422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:44832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:44936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:38862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:39222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:42610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:44796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:41118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:40836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:41890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24 TP0] Decode batch. #running-req: 248, #token: 47204, token usage: 0.05, cuda graph: True, gen throughput (token/s): 5999.26, #queue-req: 0, 
[2025-10-21 00:28:24] INFO:     127.0.0.1:39236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:41700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:42174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:45852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:44220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:44514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:39602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:40184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:40270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:42300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:42754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:45460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:50120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:50304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:45820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:43764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:43962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:44336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:50090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:45086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:24] INFO:     127.0.0.1:47366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:41584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:42574 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:39134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:39320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:40266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:40976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:42540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:47738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:50080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49526 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:43994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:47538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:50196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:38810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:38772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:40634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:47824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:41216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:42378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:45102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:39450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:40758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:43080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:43730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:43820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:40664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:40730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:45068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:45776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:47132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:39998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:40004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:41826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:44076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:40416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:46028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:46780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:46418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:40558 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:40902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:50048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:41600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:42848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:25] INFO:     127.0.0.1:48930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:42254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:47932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:48198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:39728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:39870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:45302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:46472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:49694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:42636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:48138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:39560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:41160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:46980 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:48226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:48700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:49232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:39240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:40022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:49038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:43744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:44750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:47470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:49234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:49746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:42614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:49506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26 TP0] Decode batch. #running-req: 86, #token: 21135, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3504.09, #queue-req: 0, 
[2025-10-21 00:28:26] INFO:     127.0.0.1:50158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:43194 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:43242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:43864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:39930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:40302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:47404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:47802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:44358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:42436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:46006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:50316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:39336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:48426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:49076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:47974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:48132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:48690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:48300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:40018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:40228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:45894 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:49698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:49800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:26] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:45730 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:50246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:50350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:41628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:50186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:49834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:43078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:49430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:46906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:49210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:49388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:44932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:49824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:47938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:45596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:49300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:40014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:48916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:48326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27 TP0] Decode batch. #running-req: 31, #token: 9038, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1586.96, #queue-req: 0, 
[2025-10-21 00:28:27] INFO:     127.0.0.1:49334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:45046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:42316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:49708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:39616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:27] INFO:     127.0.0.1:50212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:41452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:44942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:48992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:47790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:45898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:48622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:49846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:48290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:47632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28 TP0] Decode batch. #running-req: 9, #token: 3233, token usage: 0.00, cuda graph: True, gen throughput (token/s): 746.77, #queue-req: 0, 
[2025-10-21 00:28:28] INFO:     127.0.0.1:48210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:48984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:47724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:28] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:29] INFO:     127.0.0.1:45534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:29] INFO:     127.0.0.1:49060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:29] INFO:     127.0.0.1:48614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:29 TP0] Decode batch. #running-req: 2, #token: 966, token usage: 0.00, cuda graph: True, gen throughput (token/s): 204.56, #queue-req: 0, 
[2025-10-21 00:28:29] INFO:     127.0.0.1:49132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:43] INFO:     127.0.0.1:52652 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-10-21 00:28:43 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-21 00:28:43] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:43 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-10-21 00:28:43 TP0] Prefill batch. #new-seq: 23, #new-token: 23, #cached-token: 16687, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[2025-10-21 00:28:43 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 751, token usage: 0.00, #running-req: 24, #queue-req: 0, 
[2025-10-21 00:28:43 TP0] Prefill batch. #new-seq: 49, #new-token: 49, #cached-token: 35705, token usage: 0.01, #running-req: 25, #queue-req: 0, 
[2025-10-21 00:28:43 TP0] Prefill batch. #new-seq: 47, #new-token: 47, #cached-token: 34026, token usage: 0.01, #running-req: 74, #queue-req: 0, 
[2025-10-21 00:28:44 TP0] Prefill batch. #new-seq: 56, #new-token: 56, #cached-token: 41073, token usage: 0.01, #running-req: 121, #queue-req: 0, 
[2025-10-21 00:28:44 TP0] Prefill batch. #new-seq: 50, #new-token: 50, #cached-token: 36488, token usage: 0.01, #running-req: 177, #queue-req: 0, 
[2025-10-21 00:28:44 TP0] Prefill batch. #new-seq: 61, #new-token: 61, #cached-token: 44138, token usage: 0.02, #running-req: 227, #queue-req: 0, 
[2025-10-21 00:28:44 TP0] Prefill batch. #new-seq: 42, #new-token: 42, #cached-token: 30569, token usage: 0.02, #running-req: 288, #queue-req: 0, 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP7] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP4] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP5] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP6] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP2] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP0] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP1] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP3] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP0] Prefill batch. #new-seq: 16, #new-token: 16, #cached-token: 11670, token usage: 0.02, #running-req: 330, #queue-req: 0, 
[2025-10-21 00:28:44 TP0] Prefill batch. #new-seq: 70, #new-token: 70, #cached-token: 51027, token usage: 0.03, #running-req: 346, #queue-req: 0, 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP0] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP2] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP6] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP4] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP5] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP7] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP1] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP3] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:44 TP0] Prefill batch. #new-seq: 49, #new-token: 49, #cached-token: 35837, token usage: 0.03, #running-req: 416, #queue-req: 0, 
[2025-10-21 00:28:45 TP0] Prefill batch. #new-seq: 76, #new-token: 76, #cached-token: 55076, token usage: 0.04, #running-req: 465, #queue-req: 0, 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP0] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP2] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP1] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP4] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP3] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP5] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP6] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP7] [fused_moe] using default for (76, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP0] Prefill batch. #new-seq: 52, #new-token: 52, #cached-token: 37834, token usage: 0.04, #running-req: 541, #queue-req: 0, 
[2025-10-21 00:28:45 TP0] Prefill batch. #new-seq: 80, #new-token: 80, #cached-token: 58092, token usage: 0.04, #running-req: 593, #queue-req: 0, 
[2025-10-21 00:28:45 TP0] Prefill batch. #new-seq: 52, #new-token: 52, #cached-token: 37888, token usage: 0.05, #running-req: 673, #queue-req: 0, 
[2025-10-21 00:28:45 TP0] Prefill batch. #new-seq: 85, #new-token: 85, #cached-token: 61959, token usage: 0.05, #running-req: 725, #queue-req: 0, 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP0] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP2] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP3] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP6] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP7] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP5] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP4] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP1] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP0] Prefill batch. #new-seq: 54, #new-token: 54, #cached-token: 39383, token usage: 0.05, #running-req: 810, #queue-req: 0, 
[2025-10-21 00:28:45 TP0] Prefill batch. #new-seq: 92, #new-token: 92, #cached-token: 66995, token usage: 0.06, #running-req: 864, #queue-req: 0, 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP1] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP5] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP3] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP7] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP0] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP2] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP4] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:45 TP6] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:46 TP0] Prefill batch. #new-seq: 57, #new-token: 57, #cached-token: 41945, token usage: 0.06, #running-req: 956, #queue-req: 0, 
[2025-10-21 00:28:46 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8053, token usage: 0.07, #running-req: 1013, #queue-req: 84, 
[2025-10-21 00:28:49] INFO:     127.0.0.1:55566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:49 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-10-21 00:28:50] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:50 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 710, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-10-21 00:28:50] INFO:     127.0.0.1:33162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:50 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 737, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-10-21 00:28:50 TP0] Decode batch. #running-req: 1023, #token: 96090, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1630.81, #queue-req: 292, 
[2025-10-21 00:28:50] INFO:     127.0.0.1:52676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:50] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:50 TP0] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 735, token usage: 0.10, #running-req: 1023, #queue-req: 291, 
[2025-10-21 00:28:51] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:51 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1519, token usage: 0.10, #running-req: 1022, #queue-req: 289, 
[2025-10-21 00:28:51] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:51] INFO:     127.0.0.1:55456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:51 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1509, token usage: 0.10, #running-req: 1022, #queue-req: 287, 
[2025-10-21 00:28:51] INFO:     127.0.0.1:53204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:51] INFO:     127.0.0.1:53912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:51 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1472, token usage: 0.11, #running-req: 1022, #queue-req: 285, 
[2025-10-21 00:28:51] INFO:     127.0.0.1:53004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:51] INFO:     127.0.0.1:53810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:51] INFO:     127.0.0.1:53836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:51] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:51 TP0] Prefill batch. #new-seq: 4, #new-token: 4, #cached-token: 2925, token usage: 0.11, #running-req: 1020, #queue-req: 281, 
[2025-10-21 00:28:51] INFO:     127.0.0.1:52868 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:51] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:51] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:51 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2201, token usage: 0.11, #running-req: 1021, #queue-req: 278, 
[2025-10-21 00:28:52] INFO:     127.0.0.1:53602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:56952 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:58530 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:33288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3631, token usage: 0.11, #running-req: 1019, #queue-req: 273, 
[2025-10-21 00:28:52] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52 TP0] Prefill batch. #new-seq: 2, #new-token: 2, #cached-token: 1450, token usage: 0.11, #running-req: 1022, #queue-req: 271, 
[2025-10-21 00:28:52] INFO:     127.0.0.1:52664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:53134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3577, token usage: 0.11, #running-req: 1019, #queue-req: 266, 
[2025-10-21 00:28:52] INFO:     127.0.0.1:53778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:57114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6495, token usage: 0.11, #running-req: 1015, #queue-req: 257, 
[2025-10-21 00:28:52] INFO:     127.0.0.1:53260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:57764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:33282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:52] INFO:     127.0.0.1:33386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3598, token usage: 0.11, #running-req: 1019, #queue-req: 252, 
[2025-10-21 00:28:53] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:55858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:56494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:60826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5166, token usage: 0.11, #running-req: 1017, #queue-req: 245, 
[2025-10-21 00:28:53] INFO:     127.0.0.1:55280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:58800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:32834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53 TP0] Prefill batch. #new-seq: 3, #new-token: 3, #cached-token: 2144, token usage: 0.11, #running-req: 1021, #queue-req: 242, 
[2025-10-21 00:28:53] INFO:     127.0.0.1:53288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:55188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:56098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:57094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:57204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:57564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:60404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:60642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:60782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:60974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:33332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8900, token usage: 0.11, #running-req: 1012, #queue-req: 230, 
[2025-10-21 00:28:53] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:54436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:54634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:55718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:60462 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53] INFO:     127.0.0.1:33756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:53 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6597, token usage: 0.11, #running-req: 1015, #queue-req: 221, 
[2025-10-21 00:28:54] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:53564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:54394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:60002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:60460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5070, token usage: 0.12, #running-req: 1017, #queue-req: 214, 
[2025-10-21 00:28:54] INFO:     127.0.0.1:52660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:54542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:58382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:33608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5727, token usage: 0.12, #running-req: 1016, #queue-req: 206, 
[2025-10-21 00:28:54] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:55316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:59046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7271, token usage: 0.12, #running-req: 1014, #queue-req: 196, 
[2025-10-21 00:28:54] INFO:     127.0.0.1:54226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:54796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:55164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:55776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:56614 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:58702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:59076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:59218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:60798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:60928 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9444, token usage: 0.12, #running-req: 1011, #queue-req: 183, 
[2025-10-21 00:28:54] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54] INFO:     127.0.0.1:59136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:54 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3652, token usage: 0.12, #running-req: 1019, #queue-req: 178, 
[2025-10-21 00:28:55] INFO:     127.0.0.1:54826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:56190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:59624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3604, token usage: 0.12, #running-req: 1019, #queue-req: 173, 
[2025-10-21 00:28:55] INFO:     127.0.0.1:52798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:53178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:54502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:59280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:59794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6456, token usage: 0.12, #running-req: 1015, #queue-req: 164, 
[2025-10-21 00:28:55] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:54318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:54838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:56042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:59272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5802, token usage: 0.12, #running-req: 1016, #queue-req: 156, 
[2025-10-21 00:28:55] INFO:     127.0.0.1:53404 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:57682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:58770 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:60124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:60674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55 TP0] Prefill batch. #new-seq: 9, #new-token: 9, #cached-token: 6573, token usage: 0.12, #running-req: 1015, #queue-req: 147, 
[2025-10-21 00:28:55] INFO:     127.0.0.1:54942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:55400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:57248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:60212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:60430 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:55] INFO:     127.0.0.1:60994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5876, token usage: 0.12, #running-req: 1016, #queue-req: 139, 
[2025-10-21 00:28:56] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:55568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:57396 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:57514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:59452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:33468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56 TP0] Prefill batch. #new-seq: 7, #new-token: 7, #cached-token: 5087, token usage: 0.12, #running-req: 1017, #queue-req: 132, 
[2025-10-21 00:28:56] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:53824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:54900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:55120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:55552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:57060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:58874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:60600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:60808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:32880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:33074 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9628, token usage: 0.12, #running-req: 1011, #queue-req: 119, 
[2025-10-21 00:28:56] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:55070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:57728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:58308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:58400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:59766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:59924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:32902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:33122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56 TP0] Prefill batch. #new-seq: 10, #new-token: 10, #cached-token: 7240, token usage: 0.12, #running-req: 1014, #queue-req: 109, 
[2025-10-21 00:28:56] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:53460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:54048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:54450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:54972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:59150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:59722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:60872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8191, token usage: 0.12, #running-req: 1013, #queue-req: 98, 
[2025-10-21 00:28:56] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:54418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:54926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:56112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:58160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:56] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5766, token usage: 0.12, #running-req: 1016, #queue-req: 90, 
[2025-10-21 00:28:57] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:56708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:58478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:60722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:60908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57 TP0] Prefill batch. #new-seq: 8, #new-token: 8, #cached-token: 5797, token usage: 0.12, #running-req: 1016, #queue-req: 82, 
[2025-10-21 00:28:57] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:55682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:56460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:57016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:57198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:60114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:60900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:32970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9573, token usage: 0.13, #running-req: 1011, #queue-req: 69, 
[2025-10-21 00:28:57] INFO:     127.0.0.1:53754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:58150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:59050 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57 TP0] Prefill batch. #new-seq: 5, #new-token: 5, #cached-token: 3642, token usage: 0.13, #running-req: 1019, #queue-req: 64, 
[2025-10-21 00:28:57] INFO:     127.0.0.1:55008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:57636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:58604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:59108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:59832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:60884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57] INFO:     127.0.0.1:32810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:57 TP0] Prefill batch. #new-seq: 11, #new-token: 11, #cached-token: 8222, token usage: 0.13, #running-req: 1013, #queue-req: 53, 
[2025-10-21 00:28:58] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:54010 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:54230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:56044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:56846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:58352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:58466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:58514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:58780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:59668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:60028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:60658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:60690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58 TP0] Prefill batch. #new-seq: 16, #new-token: 16, #cached-token: 11554, token usage: 0.13, #running-req: 1008, #queue-req: 37, 
[2025-10-21 00:28:58] INFO:     127.0.0.1:53580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:54650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:56510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:57598 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:58386 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:58650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:59568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:33084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58 TP0] Prefill batch. #new-seq: 13, #new-token: 13, #cached-token: 9463, token usage: 0.13, #running-req: 1011, #queue-req: 24, 
[2025-10-21 00:28:58] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:55998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:57806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:59914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:60180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:33670 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8855, token usage: 0.13, #running-req: 1012, #queue-req: 12, 
[2025-10-21 00:28:58] INFO:     127.0.0.1:54724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:56678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:59300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:59700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:33260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:33524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:33708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58 TP0] Prefill batch. #new-seq: 12, #new-token: 12, #cached-token: 8642, token usage: 0.13, #running-req: 1012, #queue-req: 0, 
[2025-10-21 00:28:58 TP0] Decode batch. #running-req: 1012, #token: 121557, token usage: 0.13, cuda graph: False, gen throughput (token/s): 4882.56, #queue-req: 0, 
[2025-10-21 00:28:58] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:53038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:53236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:53978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:54414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:58798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:58970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:59546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:60758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:32924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:58] INFO:     127.0.0.1:33718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:52924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:55370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:32898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:33232 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP4] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP2] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP6] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP0] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP3] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP7] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP1] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP5] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59] INFO:     127.0.0.1:52810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:53144 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:53536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:55232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:57150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:58636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:59406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:60576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP4] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP2] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP6] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP0] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP3] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP7] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP1] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP5] [fused_moe] using default for (995, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59] INFO:     127.0.0.1:52976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:53270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:53672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:55078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:56590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:57326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:58968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:59348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:33220 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:33492 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP4] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP2] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP6] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP0] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP3] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP7] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP1] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP5] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:53466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:54084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:54956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:56110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:56916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:57178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:58844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:59756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:60232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:32806 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:52746 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:54662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:57032 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP2] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP4] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP6] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP0] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP3] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP7] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP1] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP5] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:53764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:55822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:56248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:60962 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP2] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP4] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP6] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP0] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP3] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP7] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP1] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP5] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59] INFO:     127.0.0.1:53054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:53340 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:57842 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:58034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:60596 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:32844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:33368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:28:59] INFO:     127.0.0.1:33486 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP2] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP4] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP6] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP0] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP3] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP7] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP1] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:28:59 TP5] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:53428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:57584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:57916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:59116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:59738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:59986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:32774 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP2] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP4] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP6] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP0] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP3] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP7] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP1] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP5] [fused_moe] using default for (927, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:54684 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:55578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:57826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:57970 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:59254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:59480 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:59492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60334 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:33602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:33862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:56732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:58338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:58648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:58904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60244 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60394 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60442 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:32864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:33068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:52716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:53390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:53774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:55642 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:56594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:59816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:33658 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP2] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP4] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP6] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP0] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP1] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP3] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP5] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP7] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00] INFO:     127.0.0.1:53402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:57412 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:59154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:59958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60702 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP2] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP4] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP6] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP0] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP3] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP1] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP7] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP5] [fused_moe] using default for (881, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00] INFO:     127.0.0.1:52920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:53632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:56172 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:56524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:56562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:56804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:58332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:58680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60508 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60734 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:53320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:53622 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:54336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:55092 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:56084 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:56618 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:60038 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:33672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:33728 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP4] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP2] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP6] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP0] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP3] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP1] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP7] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP5] [fused_moe] using default for (848, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00] INFO:     127.0.0.1:53916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:59444 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:33406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:00] INFO:     127.0.0.1:33814 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP4] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP2] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP6] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP0] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP1] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP3] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP5] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:00 TP7] [fused_moe] using default for (842, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01] INFO:     127.0.0.1:55902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:56296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:57634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:57792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:59316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:59680 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:60810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:60956 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:33598 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP4] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP2] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP6] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP0] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP3] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP7] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP1] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP5] [fused_moe] using default for (831, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:53656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:33438 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP4] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP2] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP6] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP0] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP3] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP7] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP1] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP5] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01] INFO:     127.0.0.1:53008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:56744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:58758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:59232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:59698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:33624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:53132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:56078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:58920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:59466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:33566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:34246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:54616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:57000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:57470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:59762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:33902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:34452 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:56998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:57860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:60418 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:60632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:60864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:60920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:32934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:33088 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:33950 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP4] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP2] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP6] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP0] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP3] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP1] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP7] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP5] [fused_moe] using default for (784, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01] INFO:     127.0.0.1:53218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:55478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:56628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:53848 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:53880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:53984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:55832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:56584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:56644 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:56648 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:57056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:01] INFO:     127.0.0.1:33376 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP2] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP4] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP6] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP0] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP3] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP7] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP1] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:01 TP5] [fused_moe] using default for (766, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02] INFO:     127.0.0.1:52752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55054 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:56576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:57890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:33202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:33510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55672 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:56222 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:57120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:57176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:58552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:60332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:32966 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:34924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:53500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:53722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:56926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:58458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:59260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:60610 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:34380 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP2] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP4] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP6] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP0] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP3] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP1] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP7] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP5] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP2] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP4] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP6] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP0] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP3] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP1] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP7] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP5] [fused_moe] using default for (722, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02] INFO:     127.0.0.1:53158 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:60168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:60364 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:60626 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:33322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:33362 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:33698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:33852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:53304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:54212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:56048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:57072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:57512 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:33548 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP2] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP4] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP6] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP0] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP3] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP1] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP7] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP5] [fused_moe] using default for (715, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02] INFO:     127.0.0.1:52818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:58424 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:59634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:60824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:32788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:33740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:33772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:34632 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP2] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP4] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP6] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP0] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP3] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP1] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP7] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02 TP5] [fused_moe] using default for (703, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:02] INFO:     127.0.0.1:54202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:54304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:54740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:56440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:56552 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:58454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:59282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:59374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:59472 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:60494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:33576 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:34766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:34874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:54840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55446 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:56268 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:59846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:60742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:33204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:34546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:53250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:59248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:60004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:60708 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:32882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:34028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:34570 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:02] INFO:     127.0.0.1:35040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:53856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:56140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:57646 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:33534 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:35200 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:50554 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP4] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP6] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP2] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP0] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP7] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP5] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP3] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP1] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:54400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:57940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:60878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:34348 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP4] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP2] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP6] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP0] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP3] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP1] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP5] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP7] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03] INFO:     127.0.0.1:52936 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:56426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:58118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:60510 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:33878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:34616 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP2] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP4] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP6] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP0] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP5] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP1] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP3] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP7] [fused_moe] using default for (627, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:58738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:58982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:60348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:33620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:35006 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:53456 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:54742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:54776 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:55520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:58068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:58818 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:60382 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:33096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:33702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:52768 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:54706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:55198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:56616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:56968 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:58256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:58794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:34886 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:35434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:50698 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP4] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP2] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP6] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP0] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP1] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP5] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP3] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP7] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03] INFO:     127.0.0.1:53554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:53940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:55434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:58110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:58722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:33002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:33304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:34516 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP2] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP4] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP6] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP0] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP3] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP1] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP5] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP7] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP0] Decode batch. #running-req: 590, #token: 89395, token usage: 0.09, cuda graph: False, gen throughput (token/s): 6514.03, #queue-req: 0, 
[2025-10-21 00:29:03] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:55210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:34020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:34062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:55562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:56094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:56736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:57702 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:58662 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:59916 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:33250 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:03] INFO:     127.0.0.1:33686 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP2] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP4] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP6] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP0] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP3] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP1] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP7] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:03 TP5] [fused_moe] using default for (561, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:58790 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:59782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:60844 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:60940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34164 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:35152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:50516 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP2] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP4] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP6] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP0] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP1] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP3] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP5] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP7] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04] INFO:     127.0.0.1:55206 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:59358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:59758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:60380 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:60564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:33940 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34308 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:50584 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP2] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP4] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP6] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP0] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP3] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP7] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP1] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04 TP5] [fused_moe] using default for (537, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-10-21 00:29:04] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:56724 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:50284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:55180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:56866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:57228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:58212 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:58278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:58748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:33586 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:50086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:55344 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:58316 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:58852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:33196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:35024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:52738 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:53434 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:54754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:60478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:58866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:59020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:60548 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:32822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34554 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:50202 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:55744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:57448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:58448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:59584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:60254 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:32872 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:33270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:33292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:32954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34052 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34690 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:54572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:56754 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:58484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:60282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:60590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:57260 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:60188 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:33564 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34458 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:59204 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:33924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:04] INFO:     127.0.0.1:34948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:57518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:60524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:33764 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:54274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:56788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:59082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50566 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:56374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:56532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:58620 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:35232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:55920 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:57496 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:59328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:60540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:49948 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50840 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:55328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:56694 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:33294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:33652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:53786 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:54528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:59890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:60696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34934 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:35216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:35318 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50068 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50700 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:55494 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:56008 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:57546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:35432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:56294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:58284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:60416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:33042 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34336 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:35176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50816 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:55830 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:60978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:35248 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:52828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:54518 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:33766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50176 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50182 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:53482 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:55346 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:58408 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:59862 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:60110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:60688 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34782 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:56484 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:58856 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:59332 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:33888 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:60062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:35302 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:35410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:60276 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34656 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50728 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:59428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:35422 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:56882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:58368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:33800 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:33908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34416 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:34584 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:35154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:05] INFO:     127.0.0.1:50810 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:54388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:55228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:35426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:33238 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:33982 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:55040 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:33012 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34282 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34904 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:35180 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50912 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:35174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50846 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:57532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:57632 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:58664 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:54946 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:55524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:56410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34090 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:35058 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:58930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34210 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34926 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:35266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:56468 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06 TP0] Decode batch. #running-req: 269, #token: 50529, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6137.21, #queue-req: 0, 
[2025-10-21 00:29:06] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:59166 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:59572 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34108 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50486 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50608 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:57930 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:33126 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:33880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34814 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:35072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:49954 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50884 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:54908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:33958 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:59192 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:35376 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:53228 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:53850 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:58986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:60128 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:33760 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34974 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:35224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:35402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50788 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50246 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:55138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:59326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34094 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:34196 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:35374 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:55906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:06] INFO:     127.0.0.1:60292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:55604 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:58140 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:33784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:35096 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:58750 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:33592 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34232 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:33838 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50150 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50476 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34736 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:52742 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:33612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50368 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50506 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:55216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:35136 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50686 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50774 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:58154 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:59666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:33972 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:59650 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34310 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34826 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50464 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:60082 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34410 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:59320 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:60938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34882 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50658 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:54062 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:54076 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:56384 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34142 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:35100 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:35186 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:57630 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:58046 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:49962 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:58242 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:60070 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:52780 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:60716 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:35350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:58020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34298 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34392 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34400 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:55628 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:33714 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34236 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34304 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34726 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:49988 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50612 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:54992 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:59306 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:33414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34174 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34798 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:34866 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:58998 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:07] INFO:     127.0.0.1:50624 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:35004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50256 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:56908 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:34216 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:35326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:53932 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:35290 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:33996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:34224 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:59516 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50266 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50440 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:60772 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:34960 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:32978 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:33828 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:34836 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:35086 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08 TP0] Decode batch. #running-req: 104, #token: 24270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 3745.71, #queue-req: 0, 
[2025-10-21 00:29:08] INFO:     127.0.0.1:33874 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50674 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50852 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:55388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:52794 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:53190 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:58994 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50896 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:33758 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:35014 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50102 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:58580 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:33638 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:33906 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:35278 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:57696 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50578 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:35358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:35028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:35116 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50348 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:56666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:34366 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:34524 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:50000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:56542 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:57356 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:33986 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:54000 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:08] INFO:     127.0.0.1:34784 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:50110 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:50274 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:34560 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:50338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:32950 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:35124 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:50766 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:54330 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:57984 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:34636 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:57078 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:59616 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:49942 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:35258 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:34004 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09 TP0] Decode batch. #running-req: 34, #token: 10105, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1735.54, #queue-req: 0, 
[2025-10-21 00:29:09] INFO:     127.0.0.1:33028 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:33502 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:35160 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:59284 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:35388 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:57878 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:09] INFO:     127.0.0.1:34426 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:53262 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:34996 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:50804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:33470 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:50822 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:33668 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:35048 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:56398 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:60152 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:34666 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:50652 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:34722 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:56358 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:34350 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:34478 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:34292 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:50002 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:50428 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10 TP0] Decode batch. #running-req: 7, #token: 3032, token usage: 0.00, cuda graph: True, gen throughput (token/s): 803.70, #queue-req: 0, 
[2025-10-21 00:29:10] INFO:     127.0.0.1:35030 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:35338 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:10] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:11] INFO:     127.0.0.1:33520 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:11] INFO:     127.0.0.1:35112 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:11] INFO:     127.0.0.1:60146 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:11 TP0] Decode batch. #running-req: 1, #token: 988, token usage: 0.00, cuda graph: True, gen throughput (token/s): 207.36, #queue-req: 0, 
[2025-10-21 00:29:11] INFO:     127.0.0.1:34704 - "POST /generate HTTP/1.1" 200 OK
[2025-10-21 00:29:16] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-10-21 00:29:18] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
