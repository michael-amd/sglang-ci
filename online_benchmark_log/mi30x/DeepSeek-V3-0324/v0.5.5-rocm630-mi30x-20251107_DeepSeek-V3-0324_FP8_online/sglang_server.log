INFO 11-07 14:15:42 __init__.py:179] Automatically detected platform rocm.
WARNING 11-07 14:15:42 rocm.py:34] `fork` method is not supported by ROCm. VLLM_WORKER_MULTIPROC_METHOD is overridden to `spawn` instead.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-07 14:15:42] WARNING server_args.py:1180: Attention backend not explicitly specified. Use aiter backend by default.
[2025-11-07 14:15:42] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-07 14:15:43] server_args=ServerArgs(model_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_path='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=True, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='127.0.0.1', port=30000, grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, mem_fraction_static=0.765, max_running_requests=1024, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=16384, max_prefill_tokens=16384, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='cuda', tp_size=8, pp_size=1, pp_max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=302984808, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', api_key=None, served_model_name='/mnt/raid/models/huggingface/deepseek-ai/DeepSeek-V3-0324', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='aiter', decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_moe_runner_backend=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_amx_weight_path=None, kt_amx_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=512, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=16, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, decrypted_config_file=None, decrypted_draft_config_file=None)
[2025-11-07 14:15:43] Using default HuggingFace chat template with detected content format: string
INFO 11-07 14:15:53 __init__.py:179] Automatically detected platform rocm.
INFO 11-07 14:15:53 __init__.py:179] Automatically detected platform rocm.
INFO 11-07 14:15:53 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-07 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-07 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-07 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-07 14:15:53 TP7] Process 294 gpu_id 7 is running on CPUs: [84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95]
[2025-11-07 14:15:53 TP5] Process 292 gpu_id 5 is running on CPUs: [60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]
[2025-11-07 14:15:53 TP2] Process 289 gpu_id 2 is running on CPUs: [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]
INFO 11-07 14:15:53 __init__.py:179] Automatically detected platform rocm.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-07 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
INFO 11-07 14:15:53 __init__.py:179] Automatically detected platform rocm.
[2025-11-07 14:15:53 TP1] Process 288 gpu_id 1 is running on CPUs: [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
INFO 11-07 14:15:53 __init__.py:179] Automatically detected platform rocm.
INFO 11-07 14:15:53 __init__.py:179] Automatically detected platform rocm.
[2025-11-07 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-07 14:15:53 TP2] Init torch distributed begin.
INFO 11-07 14:15:53 __init__.py:179] Automatically detected platform rocm.
INFO 11-07 14:15:53 __init__.py:179] Automatically detected platform rocm.
[2025-11-07 14:15:53 TP7] Init torch distributed begin.
[2025-11-07 14:15:53 TP5] Init torch distributed begin.
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/awq.py:67: UserWarning: HIP does not support fused_marlin_moe currently.
  warnings.warn(f"HIP does not support fused_marlin_moe currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
/sgl-workspace/sglang/python/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF q uantization currently.
  warnings.warn(f"Only CUDA support GGUF q uantization currently.")
[2025-11-07 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-07 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-07 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-07 14:15:53] INFO trace.py:52: opentelemetry package is not installed, tracing disabled
[2025-11-07 14:15:53 TP3] Process 290 gpu_id 3 is running on CPUs: [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
[2025-11-07 14:15:53 TP6] Process 293 gpu_id 6 is running on CPUs: [72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83]
[2025-11-07 14:15:53 TP0] Process 287 gpu_id 0 is running on CPUs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]
[2025-11-07 14:15:53 TP4] Process 291 gpu_id 4 is running on CPUs: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
[2025-11-07 14:15:53 TP1] Init torch distributed begin.
[2025-11-07 14:15:54 TP6] Init torch distributed begin.
[2025-11-07 14:15:54 TP3] Init torch distributed begin.
[2025-11-07 14:15:54 TP4] Init torch distributed begin.
[2025-11-07 14:15:54 TP0] Init torch distributed begin.
[2025-11-07 14:15:55 TP0] sglang is using nccl==2.21.5
[2025-11-07 14:15:57 TP7] Init torch distributed ends. mem usage=3.94 GB
[2025-11-07 14:15:57 TP0] Init torch distributed ends. mem usage=3.65 GB
[2025-11-07 14:15:57 TP5] Init torch distributed ends. mem usage=3.93 GB
[2025-11-07 14:15:57 TP6] Init torch distributed ends. mem usage=3.95 GB
[2025-11-07 14:15:57 TP4] Init torch distributed ends. mem usage=4.01 GB
[2025-11-07 14:15:57 TP3] Init torch distributed ends. mem usage=4.06 GB
[2025-11-07 14:15:57 TP2] Init torch distributed ends. mem usage=4.07 GB
[2025-11-07 14:15:57 TP1] Init torch distributed ends. mem usage=4.07 GB
[2025-11-07 14:15:58 TP2] Load weight begin. avail mem=187.19 GB
[2025-11-07 14:15:58 TP0] Load weight begin. avail mem=187.61 GB
[2025-11-07 14:15:58 TP0] Detected fp8 checkpoint.
[2025-11-07 14:15:58 TP0] Only Deepseek V3/R1 on NV-platform with capability >= 80 can use shared experts fusion optimization. Shared experts fusion optimization is disabled.
[2025-11-07 14:15:58 TP3] Load weight begin. avail mem=187.20 GB
[2025-11-07 14:15:58 TP7] Load weight begin. avail mem=187.32 GB
[2025-11-07 14:15:58 TP1] Load weight begin. avail mem=187.19 GB
[2025-11-07 14:15:58 TP5] Load weight begin. avail mem=187.33 GB
[2025-11-07 14:15:58 TP4] Load weight begin. avail mem=187.25 GB
[2025-11-07 14:15:58 TP6] Load weight begin. avail mem=187.31 GB
Loading safetensors checkpoint shards:   0% Completed | 0/163 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:   1% Completed | 1/163 [00:00<00:35,  4.55it/s]
Loading safetensors checkpoint shards:   1% Completed | 2/163 [00:00<00:27,  5.76it/s]
Loading safetensors checkpoint shards:   2% Completed | 3/163 [00:00<00:24,  6.58it/s]
Loading safetensors checkpoint shards:   2% Completed | 4/163 [00:00<00:22,  7.08it/s]
Loading safetensors checkpoint shards:   3% Completed | 5/163 [00:00<00:25,  6.21it/s]
Loading safetensors checkpoint shards:   4% Completed | 7/163 [00:00<00:19,  8.01it/s]
Loading safetensors checkpoint shards:   5% Completed | 8/163 [00:01<00:18,  8.16it/s]
Loading safetensors checkpoint shards:   6% Completed | 9/163 [00:01<00:22,  6.98it/s]
Loading safetensors checkpoint shards:   6% Completed | 10/163 [00:01<00:21,  7.18it/s]
Loading safetensors checkpoint shards:   7% Completed | 11/163 [00:02<00:42,  3.54it/s]
Loading safetensors checkpoint shards:   8% Completed | 13/163 [00:02<00:27,  5.49it/s]
Loading safetensors checkpoint shards:   9% Completed | 15/163 [00:02<00:21,  6.84it/s]
Loading safetensors checkpoint shards:  10% Completed | 17/163 [00:02<00:16,  8.95it/s]
Loading safetensors checkpoint shards:  12% Completed | 19/163 [00:02<00:15,  9.12it/s]
Loading safetensors checkpoint shards:  13% Completed | 21/163 [00:02<00:13, 10.90it/s]
Loading safetensors checkpoint shards:  14% Completed | 23/163 [00:03<00:14,  9.62it/s]
Loading safetensors checkpoint shards:  16% Completed | 26/163 [00:03<00:11, 12.24it/s]
Loading safetensors checkpoint shards:  18% Completed | 29/163 [00:03<00:09, 14.74it/s]
Loading safetensors checkpoint shards:  19% Completed | 31/163 [00:03<00:10, 12.54it/s]
Loading safetensors checkpoint shards:  21% Completed | 34/163 [00:03<00:08, 15.70it/s]
Loading safetensors checkpoint shards:  22% Completed | 36/163 [00:04<00:15,  8.32it/s]
Loading safetensors checkpoint shards:  24% Completed | 39/163 [00:04<00:12, 10.31it/s]
Loading safetensors checkpoint shards:  25% Completed | 41/163 [00:04<00:10, 11.58it/s]
Loading safetensors checkpoint shards:  26% Completed | 43/163 [00:04<00:09, 12.64it/s]
Loading safetensors checkpoint shards:  28% Completed | 45/163 [00:04<00:10, 11.73it/s]
Loading safetensors checkpoint shards:  29% Completed | 48/163 [00:04<00:08, 14.25it/s]
Loading safetensors checkpoint shards:  31% Completed | 50/163 [00:05<00:07, 14.46it/s]
Loading safetensors checkpoint shards:  32% Completed | 52/163 [00:05<00:07, 15.61it/s]
Loading safetensors checkpoint shards:  34% Completed | 55/163 [00:05<00:08, 12.01it/s]
Loading safetensors checkpoint shards:  35% Completed | 57/163 [00:05<00:07, 13.34it/s]
Loading safetensors checkpoint shards:  36% Completed | 59/163 [00:05<00:07, 14.59it/s]
Loading safetensors checkpoint shards:  38% Completed | 62/163 [00:05<00:06, 16.41it/s]
Loading safetensors checkpoint shards:  39% Completed | 64/163 [00:06<00:06, 15.29it/s]
Loading safetensors checkpoint shards:  40% Completed | 66/163 [00:06<00:06, 15.59it/s]
Loading safetensors checkpoint shards:  42% Completed | 68/163 [00:06<00:13,  7.18it/s]
Loading safetensors checkpoint shards:  44% Completed | 71/163 [00:07<00:09,  9.28it/s]
Loading safetensors checkpoint shards:  45% Completed | 73/163 [00:07<00:08, 10.43it/s]
Loading safetensors checkpoint shards:  46% Completed | 75/163 [00:07<00:08, 10.61it/s]
Loading safetensors checkpoint shards:  48% Completed | 78/163 [00:07<00:06, 13.40it/s]
Loading safetensors checkpoint shards:  49% Completed | 80/163 [00:07<00:06, 12.76it/s]
Loading safetensors checkpoint shards:  51% Completed | 83/163 [00:07<00:05, 14.90it/s]
Loading safetensors checkpoint shards:  52% Completed | 85/163 [00:07<00:04, 15.75it/s]
Loading safetensors checkpoint shards:  54% Completed | 88/163 [00:08<00:04, 17.13it/s]
Loading safetensors checkpoint shards:  55% Completed | 90/163 [00:08<00:04, 17.54it/s]
Loading safetensors checkpoint shards:  56% Completed | 92/163 [00:08<00:04, 15.97it/s]
Loading safetensors checkpoint shards:  58% Completed | 94/163 [00:08<00:04, 16.85it/s]
Loading safetensors checkpoint shards:  59% Completed | 96/163 [00:08<00:04, 14.50it/s]
Loading safetensors checkpoint shards:  60% Completed | 98/163 [00:08<00:04, 14.54it/s]
Loading safetensors checkpoint shards:  61% Completed | 100/163 [00:08<00:04, 15.16it/s]
Loading safetensors checkpoint shards:  63% Completed | 102/163 [00:09<00:05, 11.20it/s]
Loading safetensors checkpoint shards:  64% Completed | 105/163 [00:09<00:04, 14.23it/s]
Loading safetensors checkpoint shards:  66% Completed | 107/163 [00:10<00:09,  6.08it/s]
Loading safetensors checkpoint shards:  67% Completed | 109/163 [00:10<00:07,  7.34it/s]
Loading safetensors checkpoint shards:  69% Completed | 112/163 [00:10<00:05,  9.98it/s]
Loading safetensors checkpoint shards:  70% Completed | 114/163 [00:10<00:04, 10.15it/s]
Loading safetensors checkpoint shards:  72% Completed | 117/163 [00:10<00:03, 12.60it/s]
Loading safetensors checkpoint shards:  74% Completed | 120/163 [00:10<00:02, 14.74it/s]
Loading safetensors checkpoint shards:  75% Completed | 123/163 [00:10<00:02, 16.20it/s]
Loading safetensors checkpoint shards:  77% Completed | 126/163 [00:11<00:02, 17.81it/s]
Loading safetensors checkpoint shards:  79% Completed | 129/163 [00:11<00:02, 14.43it/s]
Loading safetensors checkpoint shards:  80% Completed | 131/163 [00:11<00:02, 15.04it/s]
Loading safetensors checkpoint shards:  82% Completed | 134/163 [00:11<00:01, 17.16it/s]
Loading safetensors checkpoint shards:  83% Completed | 136/163 [00:11<00:01, 17.72it/s]
Loading safetensors checkpoint shards:  85% Completed | 138/163 [00:11<00:01, 15.67it/s]
Loading safetensors checkpoint shards:  86% Completed | 140/163 [00:12<00:01, 12.38it/s]
Loading safetensors checkpoint shards:  87% Completed | 142/163 [00:12<00:01, 13.17it/s]
Loading safetensors checkpoint shards:  89% Completed | 145/163 [00:12<00:01, 15.51it/s]
Loading safetensors checkpoint shards:  90% Completed | 147/163 [00:12<00:01, 15.96it/s]
Loading safetensors checkpoint shards:  91% Completed | 149/163 [00:12<00:00, 16.80it/s]
Loading safetensors checkpoint shards:  93% Completed | 152/163 [00:12<00:00, 17.95it/s]
Loading safetensors checkpoint shards:  94% Completed | 154/163 [00:13<00:00,  9.37it/s]
Loading safetensors checkpoint shards:  96% Completed | 156/163 [00:13<00:01,  6.01it/s]
Loading safetensors checkpoint shards:  98% Completed | 159/163 [00:14<00:00,  8.14it/s]
Loading safetensors checkpoint shards:  99% Completed | 161/163 [00:14<00:00,  9.43it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:14<00:00,  9.80it/s]
Loading safetensors checkpoint shards: 100% Completed | 163/163 [00:14<00:00, 11.37it/s]

[2025-11-07 14:16:45 TP0] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=108.05 GB, mem usage=79.56 GB.
[2025-11-07 14:16:45 TP1] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-11-07 14:16:45 TP2] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.63 GB, mem usage=79.56 GB.
[2025-11-07 14:16:45 TP3] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.64 GB, mem usage=79.56 GB.
[2025-11-07 14:16:45 TP4] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.69 GB, mem usage=79.56 GB.
[2025-11-07 14:16:45 TP6] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.76 GB, mem usage=79.56 GB.
[2025-11-07 14:16:46 TP7] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.77 GB, mem usage=79.56 GB.
[2025-11-07 14:16:46 TP5] Load weight end. type=DeepseekV3ForCausalLM, dtype=torch.bfloat16, avail mem=107.78 GB, mem usage=79.56 GB.
[2025-11-07 14:16:46 TP0] Using KV cache dtype: torch.bfloat16
[2025-11-07 14:16:46 TP7] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-07 14:16:46 TP7] Memory pool end. avail mem=43.50 GB
[2025-11-07 14:16:46 TP6] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-07 14:16:46 TP5] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-07 14:16:46 TP6] Memory pool end. avail mem=43.49 GB
[2025-11-07 14:16:46 TP5] Memory pool end. avail mem=43.51 GB
[2025-11-07 14:16:46 TP0] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-07 14:16:46 TP0] Memory pool end. avail mem=43.78 GB
[2025-11-07 14:16:46 TP3] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-07 14:16:46 TP3] Memory pool end. avail mem=43.37 GB
[2025-11-07 14:16:46 TP1] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-07 14:16:46 TP1] Memory pool end. avail mem=43.37 GB
[2025-11-07 14:16:46 TP2] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-07 14:16:46 TP4] KV Cache is allocated. #tokens: 971639, KV size: 63.59 GB
[2025-11-07 14:16:46 TP2] Memory pool end. avail mem=43.36 GB
[2025-11-07 14:16:46 TP4] Memory pool end. avail mem=43.42 GB
[2025-11-07 14:16:47 TP7] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-11-07 14:16:47 TP0] Capture cuda graph begin. This can take up to several minutes. avail mem=43.58 GB
[2025-11-07 14:16:47 TP0] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512]
[2025-11-07 14:16:47 TP4] Capture cuda graph begin. This can take up to several minutes. avail mem=43.22 GB
[2025-11-07 14:16:47 TP3] Capture cuda graph begin. This can take up to several minutes. avail mem=43.17 GB
[2025-11-07 14:16:48 TP6] Capture cuda graph begin. This can take up to several minutes. avail mem=43.29 GB
[2025-11-07 14:16:48 TP1] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
  0%|          | 0/52 [00:00<?, ?it/s]Capturing batches (bs=512 avail_mem=42.94 GB):   0%|          | 0/52 [00:00<?, ?it/s][2025-11-07 14:16:48 TP2] Capture cuda graph begin. This can take up to several minutes. avail mem=43.16 GB
[2025-11-07 14:16:48 TP5] Capture cuda graph begin. This can take up to several minutes. avail mem=43.30 GB
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-07 14:16:50 TP1] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-07 14:16:50 TP2] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-07 14:16:50 TP5] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-07 14:16:50 TP3] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-07 14:16:50 TP0] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-07 14:16:50 TP6] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-07 14:16:50 TP4] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[2025-11-07 14:16:50 TP7] type hints mismatch, override to --> dynamic_per_token_scaled_quant(out: torch.Tensor, input: torch.Tensor, scales: torch.Tensor, scale_ub: Optional[torch.Tensor] = None, shuffle_scale: bool = True) -> None
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:51 TP3] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:51 TP5] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:51 TP0] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:51 TP7] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:51 TP6] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:51 TP4] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:51 TP1] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:51 TP2] shape M:512, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-07 14:16:52 TP0] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-07 14:16:52 TP6] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-07 14:16:52 TP4] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-07 14:16:52 TP3] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-07 14:16:52 TP7] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP6] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP4] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP6] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP4] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP6] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP4] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP6] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP6] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP4] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP4] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP0] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP0] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP0] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP3] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP0] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP3] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP0] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:52 TP3] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:16:53 TP3] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:53 TP3] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:53 TP4] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:53 TP6] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:53 TP0] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:53 TP3] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:53 TP7] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:53 TP7] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:53 TP7] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:16:53 TP7] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:53 TP7] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:53 TP7] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-07 14:16:53 TP2] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-07 14:16:53 TP1] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[2025-11-07 14:16:53 TP5] type hints mismatch, override to --> rope_cached_positions_2c_fwd_impl(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor, arg5: torch.Tensor, arg6: torch.Tensor, arg7: int, arg8: bool, arg9: bool) -> None
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//mla/mla_dec_stage1_bf16_a16w16_subQ16_mqa16.co GetFunction: _ZN5aiter39mla_dec_stage1_bf16_a16w16_subQ16_mqa16E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP2] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP1] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP2] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP1] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP2] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP1] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP2] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP2] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP1] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP1] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:54 TP2] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:54 TP1] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
[aiter] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP5] shape M:512, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP5] shape M:512, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP5] shape M:512, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP5] shape M:512, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:16:54 TP5] shape M:512, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:54 TP5] [fused_moe] using default for (512, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] hipModuleLoad: /sgl-workspace/aiter/hsa/gfx942//fmoe/fmoe_fp8_blockscale_g1u1_novs_subGU_256.co GetFunction: _ZN5aiter39fmoe_fp8_blockscale_g1u1_novs_subGU_256E Success
Capturing batches (bs=512 avail_mem=42.94 GB):   2%|         | 1/52 [00:06<05:27,  6.42s/it]Capturing batches (bs=496 avail_mem=42.28 GB):   2%|         | 1/52 [00:06<05:27,  6.42s/it][aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP5] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP3] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP7] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP1] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP4] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP6] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP0] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP2] [fused_moe] using default for (496, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=496 avail_mem=42.28 GB):   4%|         | 2/52 [00:07<02:39,  3.20s/it]Capturing batches (bs=480 avail_mem=42.27 GB):   4%|         | 2/52 [00:07<02:39,  3.20s/it][aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP5] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP2] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP3] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP1] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP0] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP6] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP4] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:55 TP7] [fused_moe] using default for (480, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=480 avail_mem=42.27 GB):   6%|         | 3/52 [00:07<01:34,  1.93s/it]Capturing batches (bs=464 avail_mem=42.26 GB):   6%|         | 3/52 [00:07<01:34,  1.93s/it][aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP7] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP5] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP1] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP6] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP4] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP0] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP2] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP3] [fused_moe] using default for (464, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=464 avail_mem=42.26 GB):   8%|         | 4/52 [00:08<01:03,  1.33s/it]Capturing batches (bs=448 avail_mem=42.26 GB):   8%|         | 4/52 [00:08<01:03,  1.33s/it][aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP0] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP3] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP1] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP4] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP6] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP2] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP7] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP5] [fused_moe] using default for (448, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=448 avail_mem=42.26 GB):  10%|         | 5/52 [00:08<00:47,  1.00s/it]Capturing batches (bs=432 avail_mem=42.25 GB):  10%|         | 5/52 [00:08<00:47,  1.00s/it][aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP6] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP4] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP5] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP3] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP2] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP1] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP0] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:56 TP7] [fused_moe] using default for (432, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=432 avail_mem=42.25 GB):  12%|        | 6/52 [00:09<00:36,  1.25it/s]Capturing batches (bs=416 avail_mem=42.25 GB):  12%|        | 6/52 [00:09<00:36,  1.25it/s][aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP3] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP2] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP4] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP7] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP0] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP1] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP5] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP6] [fused_moe] using default for (416, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=416 avail_mem=42.25 GB):  13%|        | 7/52 [00:09<00:30,  1.48it/s]Capturing batches (bs=400 avail_mem=42.24 GB):  13%|        | 7/52 [00:09<00:30,  1.48it/s][aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP5] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP6] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP4] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP7] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP0] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP2] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP3] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:57 TP1] [fused_moe] using default for (400, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=400 avail_mem=42.24 GB):  15%|        | 8/52 [00:09<00:26,  1.68it/s]Capturing batches (bs=384 avail_mem=42.24 GB):  15%|        | 8/52 [00:09<00:26,  1.68it/s][aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP3] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP5] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP1] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP4] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP0] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP2] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP6] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP7] [fused_moe] using default for (384, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=384 avail_mem=42.24 GB):  17%|        | 9/52 [00:10<00:21,  2.01it/s]Capturing batches (bs=368 avail_mem=42.23 GB):  17%|        | 9/52 [00:10<00:21,  2.01it/s][aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP7] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP3] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP4] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP6] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP2] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP5] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP0] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP1] [fused_moe] using default for (368, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=368 avail_mem=42.23 GB):  19%|        | 10/52 [00:10<00:19,  2.12it/s]Capturing batches (bs=352 avail_mem=42.23 GB):  19%|        | 10/52 [00:10<00:19,  2.12it/s][aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP6] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP2] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP1] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP5] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP4] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP7] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP0] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:58 TP3] [fused_moe] using default for (352, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=352 avail_mem=42.23 GB):  21%|        | 11/52 [00:10<00:18,  2.19it/s]Capturing batches (bs=336 avail_mem=42.22 GB):  21%|        | 11/52 [00:10<00:18,  2.19it/s][aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP2] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP5] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP4] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP0] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP1] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP6] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP3] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP7] [fused_moe] using default for (336, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=336 avail_mem=42.22 GB):  23%|       | 12/52 [00:11<00:17,  2.25it/s]Capturing batches (bs=320 avail_mem=42.22 GB):  23%|       | 12/52 [00:11<00:17,  2.25it/s][aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP3] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP1] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP5] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP4] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP7] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP0] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP2] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP6] [fused_moe] using default for (320, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=320 avail_mem=42.22 GB):  25%|       | 13/52 [00:11<00:15,  2.55it/s]Capturing batches (bs=304 avail_mem=42.22 GB):  25%|       | 13/52 [00:11<00:15,  2.55it/s][aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP4] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP5] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP6] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP7] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP3] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:16:59 TP0] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP1] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP2] [fused_moe] using default for (304, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=304 avail_mem=42.22 GB):  27%|       | 14/52 [00:12<00:15,  2.49it/s]Capturing batches (bs=288 avail_mem=42.21 GB):  27%|       | 14/52 [00:12<00:15,  2.49it/s][aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP5] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP4] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP1] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP3] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP0] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP6] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP2] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP7] [fused_moe] using default for (288, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=288 avail_mem=42.21 GB):  29%|       | 15/52 [00:12<00:13,  2.76it/s]Capturing batches (bs=272 avail_mem=42.21 GB):  29%|       | 15/52 [00:12<00:13,  2.76it/s][aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP4] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP0] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP2] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP6] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP7] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP3] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP1] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:00 TP5] [fused_moe] using default for (272, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=272 avail_mem=42.21 GB):  31%|       | 16/52 [00:12<00:13,  2.62it/s]Capturing batches (bs=256 avail_mem=42.21 GB):  31%|       | 16/52 [00:12<00:13,  2.62it/s][aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:00 TP7] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:00 TP5] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:00 TP1] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:00 TP3] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:00 TP0] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:00 TP4] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:00 TP6] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:00 TP2] shape M:256, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP5] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP5] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP5] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP5] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:01 TP5] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP4] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP5] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP4] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP4] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP4] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:01 TP4] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP4] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP7] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP7] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP7] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP7] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:01 TP7] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP6] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP7] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP6] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP6] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP6] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP2] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP2] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP2] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:01 TP6] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP6] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP2] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP1] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP1] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP1] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:01 TP2] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP2] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP1] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP0] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP0] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP0] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:01 TP1] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP1] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP0] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:01 TP0] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP3] shape M:256, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP0] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP3] shape M:256, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP3] shape M:256, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:01 TP3] shape M:256, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:01 TP3] shape M:256, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x128_16x16_16x16_8x32x1_8x32x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP3] [fused_moe] using default for (256, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=256 avail_mem=42.21 GB):  33%|      | 17/52 [00:13<00:13,  2.54it/s]Capturing batches (bs=248 avail_mem=42.20 GB):  33%|      | 17/52 [00:13<00:13,  2.54it/s][aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP5] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP1] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP3] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP4] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP7] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP0] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP6] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP2] [fused_moe] using default for (248, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=248 avail_mem=42.20 GB):  35%|      | 18/52 [00:13<00:13,  2.49it/s]Capturing batches (bs=240 avail_mem=42.19 GB):  35%|      | 18/52 [00:13<00:13,  2.49it/s][aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP7] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP0] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP1] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP5] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP2] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP3] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP4] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:01 TP6] [fused_moe] using default for (240, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=240 avail_mem=42.19 GB):  37%|      | 19/52 [00:14<00:13,  2.45it/s]Capturing batches (bs=232 avail_mem=42.19 GB):  37%|      | 19/52 [00:14<00:13,  2.45it/s][aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP2] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP5] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP7] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP0] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP1] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP3] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP6] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP4] [fused_moe] using default for (232, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=232 avail_mem=42.19 GB):  38%|      | 20/52 [00:14<00:13,  2.42it/s]Capturing batches (bs=224 avail_mem=42.18 GB):  38%|      | 20/52 [00:14<00:13,  2.42it/s][aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP7] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP5] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP4] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP6] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP1] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP2] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP3] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:02 TP0] [fused_moe] using default for (224, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=224 avail_mem=42.18 GB):  40%|      | 21/52 [00:14<00:12,  2.40it/s]Capturing batches (bs=216 avail_mem=42.18 GB):  40%|      | 21/52 [00:14<00:12,  2.40it/s][aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP6] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP7] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP5] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP4] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP1] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP3] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP0] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP2] [fused_moe] using default for (216, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=216 avail_mem=42.18 GB):  42%|     | 22/52 [00:15<00:12,  2.40it/s]Capturing batches (bs=208 avail_mem=42.17 GB):  42%|     | 22/52 [00:15<00:12,  2.40it/s][aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP0] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP3] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP5] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP4] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP1] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP6] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP2] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP7] [fused_moe] using default for (208, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=208 avail_mem=42.17 GB):  44%|     | 23/52 [00:15<00:10,  2.67it/s]Capturing batches (bs=200 avail_mem=42.17 GB):  44%|     | 23/52 [00:15<00:10,  2.67it/s][aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP5] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP3] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP7] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP4] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP0] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP6] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP2] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:03 TP1] [fused_moe] using default for (200, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=200 avail_mem=42.17 GB):  46%|     | 24/52 [00:16<00:10,  2.58it/s]Capturing batches (bs=192 avail_mem=42.16 GB):  46%|     | 24/52 [00:16<00:10,  2.58it/s][aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP0] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP3] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP1] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP4] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP2] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP5] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP7] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP6] [fused_moe] using default for (192, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=192 avail_mem=42.16 GB):  48%|     | 25/52 [00:16<00:09,  2.83it/s]Capturing batches (bs=184 avail_mem=42.16 GB):  48%|     | 25/52 [00:16<00:09,  2.83it/s][aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP0] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP3] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP2] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP4] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP1] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP6] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP7] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP5] [fused_moe] using default for (184, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=184 avail_mem=42.16 GB):  50%|     | 26/52 [00:16<00:08,  3.04it/s]Capturing batches (bs=176 avail_mem=42.16 GB):  50%|     | 26/52 [00:16<00:08,  3.04it/s][aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP3] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP4] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP0] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP2] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP1] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP6] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP7] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:04 TP5] [fused_moe] using default for (176, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=176 avail_mem=42.16 GB):  52%|    | 27/52 [00:16<00:07,  3.21it/s]Capturing batches (bs=168 avail_mem=42.15 GB):  52%|    | 27/52 [00:16<00:07,  3.21it/s][aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP0] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP5] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP4] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP1] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP6] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP2] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP3] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP7] [fused_moe] using default for (168, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=168 avail_mem=42.15 GB):  54%|    | 28/52 [00:17<00:08,  2.90it/s]Capturing batches (bs=160 avail_mem=42.15 GB):  54%|    | 28/52 [00:17<00:08,  2.90it/s][aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP0] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP3] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP4] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP1] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP6] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP5] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP2] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP7] [fused_moe] using default for (160, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=160 avail_mem=42.15 GB):  56%|    | 29/52 [00:17<00:07,  3.08it/s]Capturing batches (bs=152 avail_mem=42.15 GB):  56%|    | 29/52 [00:17<00:07,  3.08it/s][aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP4] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP6] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP5] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP7] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP3] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP1] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP2] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:05 TP0] [fused_moe] using default for (152, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=152 avail_mem=42.15 GB):  58%|    | 30/52 [00:17<00:07,  2.85it/s]Capturing batches (bs=144 avail_mem=42.14 GB):  58%|    | 30/52 [00:17<00:07,  2.85it/s][aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP0] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP1] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP4] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP2] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP6] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP3] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP5] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP7] [fused_moe] using default for (144, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=144 avail_mem=42.14 GB):  60%|    | 31/52 [00:18<00:06,  3.03it/s]Capturing batches (bs=136 avail_mem=42.14 GB):  60%|    | 31/52 [00:18<00:06,  3.03it/s][aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP0] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP3] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP1] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP2] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP4] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP7] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP5] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP6] [fused_moe] using default for (136, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=136 avail_mem=42.14 GB):  62%|   | 32/52 [00:18<00:06,  3.22it/s]Capturing batches (bs=128 avail_mem=42.14 GB):  62%|   | 32/52 [00:18<00:06,  3.22it/s][aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:06 TP5] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:06 TP2] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:06 TP7] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:06 TP0] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:06 TP3] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:06 TP1] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:06 TP4] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:06 TP6] shape M:128, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP7] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP1] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP3] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP5] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP0] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP2] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP4] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP6] shape M:128, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP7] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP1] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP3] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP5] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP0] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP2] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP6] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP4] shape M:128, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP7] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP1] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP3] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP5] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP0] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP6] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP2] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP4] shape M:128, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP1] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP3] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP7] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP0] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP6] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP2] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP4] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:06 TP5] shape M:128, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:06 TP1] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:06 TP3] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:06 TP7] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:06 TP0] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:06 TP6] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:06 TP2] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:06 TP4] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:06 TP5] shape M:128, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP3] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP1] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP7] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP0] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP6] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP2] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP4] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:06 TP5] [fused_moe] using default for (128, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=128 avail_mem=42.14 GB):  63%|   | 33/52 [00:18<00:05,  3.32it/s]Capturing batches (bs=120 avail_mem=42.13 GB):  63%|   | 33/52 [00:18<00:05,  3.32it/s][aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP0] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP1] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP2] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP4] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP3] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP6] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP7] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP5] [fused_moe] using default for (120, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=120 avail_mem=42.13 GB):  65%|   | 34/52 [00:19<00:06,  2.97it/s]Capturing batches (bs=112 avail_mem=42.13 GB):  65%|   | 34/52 [00:19<00:06,  2.97it/s][aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP0] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP3] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP1] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP4] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP2] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP6] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP5] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP7] [fused_moe] using default for (112, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=112 avail_mem=42.13 GB):  67%|   | 35/52 [00:19<00:05,  3.15it/s]Capturing batches (bs=104 avail_mem=42.12 GB):  67%|   | 35/52 [00:19<00:05,  3.15it/s][aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP6] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP5] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP4] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP3] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP2] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP7] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP0] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:07 TP1] [fused_moe] using default for (104, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=104 avail_mem=42.12 GB):  69%|   | 36/52 [00:19<00:05,  2.87it/s]Capturing batches (bs=96 avail_mem=42.12 GB):  69%|   | 36/52 [00:19<00:05,  2.87it/s] [aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP1] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP3] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP0] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP4] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP2] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP5] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP7] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP6] [fused_moe] using default for (96, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=96 avail_mem=42.12 GB):  71%|   | 37/52 [00:20<00:04,  3.05it/s]Capturing batches (bs=88 avail_mem=42.12 GB):  71%|   | 37/52 [00:20<00:04,  3.05it/s][aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP2] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP5] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP4] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP7] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP6] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP1] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP3] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP0] [fused_moe] using default for (88, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=88 avail_mem=42.12 GB):  73%|  | 38/52 [00:20<00:05,  2.79it/s]Capturing batches (bs=80 avail_mem=42.11 GB):  73%|  | 38/52 [00:20<00:05,  2.79it/s][aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP0] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP1] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP3] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP2] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP5] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP4] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP6] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:08 TP7] [fused_moe] using default for (80, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=80 avail_mem=42.11 GB):  75%|  | 39/52 [00:20<00:04,  3.01it/s]Capturing batches (bs=72 avail_mem=42.11 GB):  75%|  | 39/52 [00:20<00:04,  3.01it/s][aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP7] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP5] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP3] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP6] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP0] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP4] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP2] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP1] [fused_moe] using default for (72, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=72 avail_mem=42.11 GB):  77%|  | 40/52 [00:21<00:04,  2.78it/s]Capturing batches (bs=64 avail_mem=42.10 GB):  77%|  | 40/52 [00:21<00:04,  2.78it/s][aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP0] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP3] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP1] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP7] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP5] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP6] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP2] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP4] shape M:64, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:09 TP0] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:09 TP3] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:09 TP1] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:09 TP7] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:09 TP6] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:09 TP2] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:09 TP5] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[2025-11-07 14:17:09 TP4] shape M:64, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x128x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_8_1x2_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP0] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP3] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP6] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP7] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP1] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP2] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP5] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP4] shape M:64, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP0] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP3] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP6] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP7] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP1] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP2] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP4] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP5] shape M:64, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP0] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP3] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP6] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP1] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP2] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP4] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:09 TP5] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP0] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:09 TP7] shape M:64, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:09 TP3] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:09 TP6] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:09 TP1] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:09 TP2] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:09 TP4] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:09 TP5] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[2025-11-07 14:17:09 TP7] shape M:64, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x32x64x256_16x16_16x16_16x16x1_16x16x1_1x32x1x8_8_2x1_intrawave_v1!
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP3] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP1] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP0] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP2] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP7] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP5] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP6] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP4] [fused_moe] using default for (64, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=64 avail_mem=42.10 GB):  79%|  | 41/52 [00:21<00:03,  3.00it/s]Capturing batches (bs=56 avail_mem=42.10 GB):  79%|  | 41/52 [00:21<00:03,  3.00it/s][aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP4] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP0] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP7] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP1] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP6] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP2] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP3] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:09 TP5] [fused_moe] using default for (56, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=56 avail_mem=42.10 GB):  81%|  | 42/52 [00:21<00:03,  2.79it/s]Capturing batches (bs=48 avail_mem=42.09 GB):  81%|  | 42/52 [00:21<00:03,  2.79it/s][aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP0] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP1] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP2] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP3] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP5] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP7] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP4] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP6] [fused_moe] using default for (48, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=48 avail_mem=42.09 GB):  83%| | 43/52 [00:22<00:02,  3.00it/s]Capturing batches (bs=40 avail_mem=42.09 GB):  83%| | 43/52 [00:22<00:02,  3.00it/s][aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP3] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP5] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP2] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP7] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP0] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP1] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP6] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP4] [fused_moe] using default for (40, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=40 avail_mem=42.09 GB):  85%| | 44/52 [00:22<00:02,  2.77it/s]Capturing batches (bs=32 avail_mem=42.08 GB):  85%| | 44/52 [00:22<00:02,  2.77it/s][aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP0] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP5] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP7] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP3] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP1] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP4] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP2] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP6] shape M:32, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP0] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP5] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP1] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP7] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP2] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP3] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP6] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP4] shape M:32, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP1] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP3] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP0] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP2] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP7] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP5] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP6] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP4] shape M:32, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP1] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP3] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP0] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP2] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP7] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP5] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP6] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP4] shape M:32, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP1] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP3] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP0] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP2] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP6] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP7] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP5] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP1] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP3] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP0] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP4] shape M:32, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP2] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP6] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP7] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP5] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:10 TP4] shape M:32, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP1] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP3] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP0] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP2] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP7] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP5] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP6] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:10 TP4] [fused_moe] using default for (32, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=32 avail_mem=42.08 GB):  87%| | 45/52 [00:22<00:02,  2.99it/s]Capturing batches (bs=24 avail_mem=42.08 GB):  87%| | 45/52 [00:22<00:02,  2.99it/s][aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP7] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP6] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP3] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP0] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP5] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP4] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP1] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP2] [fused_moe] using default for (24, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=24 avail_mem=42.08 GB):  88%| | 46/52 [00:23<00:02,  2.79it/s]Capturing batches (bs=16 avail_mem=42.07 GB):  88%| | 46/52 [00:23<00:02,  2.79it/s][aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:17:11 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP3] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP0] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP1] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP2] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP4] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP5] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP6] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP7] [fused_moe] using default for (16, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=16 avail_mem=42.07 GB):  90%| | 47/52 [00:23<00:01,  3.03it/s]Capturing batches (bs=12 avail_mem=42.07 GB):  90%| | 47/52 [00:23<00:01,  3.03it/s][aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP0] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP2] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP4] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP3] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP1] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP7] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP5] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:11 TP6] [fused_moe] using default for (12, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=12 avail_mem=42.07 GB):  92%|| 48/52 [00:23<00:01,  3.23it/s]Capturing batches (bs=8 avail_mem=42.07 GB):  92%|| 48/52 [00:23<00:01,  3.23it/s] [aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP0] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP1] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP3] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP4] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP2] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP5] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP6] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP7] [fused_moe] using default for (8, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=8 avail_mem=42.07 GB):  94%|| 49/52 [00:24<00:00,  3.35it/s]Capturing batches (bs=4 avail_mem=42.06 GB):  94%|| 49/52 [00:24<00:00,  3.35it/s][aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP0] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP4] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP1] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP3] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP2] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP5] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP7] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP6] [fused_moe] using default for (4, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=4 avail_mem=42.06 GB):  96%|| 50/52 [00:24<00:00,  3.48it/s]Capturing batches (bs=2 avail_mem=42.06 GB):  96%|| 50/52 [00:24<00:00,  3.48it/s][aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP0] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP3] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP4] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP1] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP2] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP7] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP5] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:12 TP6] [fused_moe] using default for (2, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=2 avail_mem=42.06 GB):  98%|| 51/52 [00:24<00:00,  3.53it/s]Capturing batches (bs=1 avail_mem=42.06 GB):  98%|| 51/52 [00:24<00:00,  3.53it/s][aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:13 TP1] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:13 TP7] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:13 TP5] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:13 TP6] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:13 TP2] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:13 TP3] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:13 TP0] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:13 TP4] [fused_moe] using default for (1, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
Capturing batches (bs=1 avail_mem=42.06 GB): 100%|| 52/52 [00:25<00:00,  1.94it/s]Capturing batches (bs=1 avail_mem=42.06 GB): 100%|| 52/52 [00:25<00:00,  2.02it/s]
[2025-11-07 14:17:14 TP0] Registering 6396 cuda graph addresses
[2025-11-07 14:17:14 TP7] Capture cuda graph end. Time elapsed: 26.86 s. mem usage=1.53 GB. avail mem=41.77 GB.
[2025-11-07 14:17:14 TP3] Capture cuda graph end. Time elapsed: 26.82 s. mem usage=1.53 GB. avail mem=41.64 GB.
[2025-11-07 14:17:14 TP5] Capture cuda graph end. Time elapsed: 25.76 s. mem usage=1.53 GB. avail mem=41.78 GB.
[2025-11-07 14:17:14 TP0] Capture cuda graph end. Time elapsed: 26.86 s. mem usage=1.53 GB. avail mem=42.05 GB.
[2025-11-07 14:17:14 TP1] Capture cuda graph end. Time elapsed: 26.68 s. mem usage=1.53 GB. avail mem=41.63 GB.
[2025-11-07 14:17:14 TP4] Capture cuda graph end. Time elapsed: 26.95 s. mem usage=1.53 GB. avail mem=41.69 GB.
[2025-11-07 14:17:14 TP2] Capture cuda graph end. Time elapsed: 25.95 s. mem usage=1.53 GB. avail mem=41.63 GB.
[2025-11-07 14:17:14 TP6] Capture cuda graph end. Time elapsed: 26.83 s. mem usage=1.53 GB. avail mem=41.76 GB.
[2025-11-07 14:17:15 TP0] max_total_num_tokens=971639, chunked_prefill_size=16384, max_prefill_tokens=16384, max_running_requests=1024, context_len=163840, available_gpu_mem=42.05 GB
[2025-11-07 14:17:15] INFO:     Started server process [47]
[2025-11-07 14:17:15] INFO:     Waiting for application startup.
[2025-11-07 14:17:15] INFO:     Application startup complete.
[2025-11-07 14:17:15] INFO:     Uvicorn running on http://127.0.0.1:30000 (Press CTRL+C to quit)
[2025-11-07 14:17:16] INFO:     127.0.0.1:49130 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-07 14:17:16 TP0] Prefill batch, #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:17 TP7] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:17 TP2] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:17 TP5] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:17 TP4] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:17 TP6] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:17 TP0] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:17 TP1] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:17 TP3] [fused_moe] using default for (7, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:18] INFO:     127.0.0.1:49132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:18] The server is fired up and ready to roll!
[2025-11-07 14:17:18] INFO:     127.0.0.1:49144 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-07 14:17:26] INFO:     127.0.0.1:51434 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-07 14:17:26 TP0] Prefill batch, #new-seq: 1, #new-token: 666, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP3] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP0] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP1] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP7] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP2] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP4] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP5] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP6] [fused_moe] using default for (666, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26] INFO:     127.0.0.1:51444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:26 TP0] Prefill batch, #new-seq: 1, #new-token: 67, #cached-token: 667, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP3] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP1] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP0] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP4] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP2] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP7] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP6] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP5] [fused_moe] using default for (67, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP0] Prefill batch, #new-seq: 43, #new-token: 2574, #cached-token: 28681, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP5] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP1] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP6] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP3] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP0] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP2] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP7] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP4] [fused_moe] using default for (1024, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:26 TP0] Prefill batch, #new-seq: 133, #new-token: 8186, #cached-token: 88977, token usage: 0.00, #running-req: 44, #queue-req: 0, 
[2025-11-07 14:17:27 TP0] Prefill batch, #new-seq: 142, #new-token: 8383, #cached-token: 95039, token usage: 0.01, #running-req: 177, #queue-req: 0, 
[2025-11-07 14:17:28 TP0] Prefill batch, #new-seq: 277, #new-token: 16323, #cached-token: 185448, token usage: 0.02, #running-req: 319, #queue-req: 64, 
[2025-11-07 14:17:29 TP0] Prefill batch, #new-seq: 275, #new-token: 16371, #cached-token: 184176, token usage: 0.04, #running-req: 596, #queue-req: 144, 
[2025-11-07 14:17:30 TP0] Prefill batch, #new-seq: 153, #new-token: 9545, #cached-token: 102480, token usage: 0.05, #running-req: 871, #queue-req: 295, 
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP2] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP2] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP2] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP2] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP2] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP2] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP0] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP0] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP0] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP0] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP0] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP0] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP1] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP1] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP1] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP1] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP1] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP1] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP3] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP3] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP3] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP3] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP3] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP3] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP6] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP6] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP6] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP6] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP6] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP6] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP4] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP4] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP4] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP4] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP4] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP4] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP7] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP7] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP7] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP7] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP7] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP7] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP5] shape M:1024, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP5] shape M:1024, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP5] shape M:1024, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[2025-11-07 14:17:31 TP5] shape M:1024, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x128x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v3!
[aiter] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP5] shape M:1024, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x256_16x16_32x32_16x16x1_16x16x1_1x32x1x8_8_1x1_intrawave_v1!
[aiter] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:31 TP5] shape M:1024, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x64x64x128_16x16_32x32_8x32x1_8x32x1_1x32x1x8_8_1x1_intrawave_v1!
[2025-11-07 14:17:34] INFO:     127.0.0.1:54050 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP3] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP4] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP2] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP1] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP7] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP0] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP6] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP5] [fused_moe] using default for (1023, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP0] Prefill batch, #new-seq: 1, #new-token: 68, #cached-token: 669, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP3] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP0] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP1] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP2] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP4] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP6] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP7] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:34 TP5] [fused_moe] using default for (68, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35] INFO:     127.0.0.1:51608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:35 TP0] Decode batch, #running-req: 1024, #token: 95421, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1660.78, #queue-req: 294, 
[2025-11-07 14:17:35 TP0] Prefill batch, #new-seq: 1, #new-token: 53, #cached-token: 669, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP3] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP0] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP4] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP1] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP2] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP7] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP6] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP5] [fused_moe] using default for (53, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:35 TP0] Prefill batch, #new-seq: 1, #new-token: 69, #cached-token: 669, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP3] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP2] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP1] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP4] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP0] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP7] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP6] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:35 TP5] [fused_moe] using default for (69, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP3] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP1] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP2] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP4] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP7] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP6] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP0] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP5] [fused_moe] using default for (1021, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:55550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36 TP0] Prefill batch, #new-seq: 3, #new-token: 248, #cached-token: 2009, token usage: 0.11, #running-req: 1021, #queue-req: 289, 
[2025-11-07 14:17:36] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:52712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:54220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:54822 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP4] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP0] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP1] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP5] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP2] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP3] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP6] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP7] [fused_moe] using default for (1016, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP0] Prefill batch, #new-seq: 8, #new-token: 552, #cached-token: 5362, token usage: 0.11, #running-req: 1016, #queue-req: 281, 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP2] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP3] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP4] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP1] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP0] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP7] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP6] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP5] [fused_moe] using default for (552, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:52416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:53392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:55492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:59862 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP4] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP0] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP2] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP1] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP6] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP5] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP3] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP7] [fused_moe] using default for (1017, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP0] Prefill batch, #new-seq: 7, #new-token: 440, #cached-token: 4692, token usage: 0.11, #running-req: 1017, #queue-req: 274, 
[aiter] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP3] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP4] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP2] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP1] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP0] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP7] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP5] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP6] [fused_moe] using default for (440, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36] INFO:     127.0.0.1:51742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:51800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:52192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:54120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:55722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:55796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:55876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36 TP0] Prefill batch, #new-seq: 8, #new-token: 403, #cached-token: 5362, token usage: 0.11, #running-req: 1016, #queue-req: 266, 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP3] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP2] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP4] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP1] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP0] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP7] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP6] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP5] [fused_moe] using default for (403, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36] INFO:     127.0.0.1:53008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:60026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:36] INFO:     127.0.0.1:60134 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP4] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP0] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP1] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP2] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP5] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP3] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP6] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:36 TP7] [fused_moe] using default for (1019, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP0] Prefill batch, #new-seq: 5, #new-token: 264, #cached-token: 3350, token usage: 0.11, #running-req: 1019, #queue-req: 261, 
[aiter] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP3] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP4] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP1] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP2] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP0] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP7] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP6] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP5] [fused_moe] using default for (264, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37] INFO:     127.0.0.1:54402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:54974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:56032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:59274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37 TP0] Prefill batch, #new-seq: 5, #new-token: 258, #cached-token: 3347, token usage: 0.11, #running-req: 1019, #queue-req: 256, 
[aiter] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP3] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP4] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP1] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP2] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP0] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP7] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP6] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP5] [fused_moe] using default for (258, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37] INFO:     127.0.0.1:53484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:54522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:57266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:59504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:60004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37 TP0] Prefill batch, #new-seq: 5, #new-token: 242, #cached-token: 3352, token usage: 0.11, #running-req: 1019, #queue-req: 251, 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP3] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP1] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP2] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP4] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP0] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP6] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP7] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP5] [fused_moe] using default for (242, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37] INFO:     127.0.0.1:52096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:53600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:54096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:56012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:56132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:59140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:60070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37 TP0] Prefill batch, #new-seq: 8, #new-token: 517, #cached-token: 5358, token usage: 0.11, #running-req: 1016, #queue-req: 243, 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP3] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP1] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP4] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP2] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP0] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP7] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP6] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP5] [fused_moe] using default for (517, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37] INFO:     127.0.0.1:51454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:52112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:53938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:54038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:57518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:58666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:58934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:58988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:59258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:37] INFO:     127.0.0.1:60230 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP4] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP0] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP1] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP5] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP2] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP6] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP3] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP7] [fused_moe] using default for (1013, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP0] Prefill batch, #new-seq: 11, #new-token: 764, #cached-token: 7371, token usage: 0.11, #running-req: 1013, #queue-req: 232, 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP3] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP2] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP4] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP1] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP0] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP7] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP6] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP5] [fused_moe] using default for (764, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP4] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP1] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP0] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP3] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP2] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP7] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP6] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:37 TP5] [fused_moe] using default for (1018, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP0] Prefill batch, #new-seq: 6, #new-token: 394, #cached-token: 4023, token usage: 0.11, #running-req: 1018, #queue-req: 226, 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP3] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP1] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP0] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP2] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP4] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP7] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP6] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP5] [fused_moe] using default for (394, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP0] Prefill batch, #new-seq: 8, #new-token: 484, #cached-token: 5359, token usage: 0.11, #running-req: 1016, #queue-req: 218, 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP3] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP2] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP1] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP0] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP4] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP7] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP6] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP5] [fused_moe] using default for (484, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38] INFO:     127.0.0.1:52710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:53138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:53692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:54646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:58464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:58978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:52390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:52690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:52728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:59836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:60468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:53444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:56732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:59302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38 TP0] Prefill batch, #new-seq: 7, #new-token: 387, #cached-token: 4686, token usage: 0.12, #running-req: 1017, #queue-req: 211, 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP2] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP3] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP1] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP0] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP4] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP7] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP6] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP5] [fused_moe] using default for (387, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38] INFO:     127.0.0.1:52246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:52328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:54318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:56452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:57138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:59264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38 TP0] Prefill batch, #new-seq: 8, #new-token: 371, #cached-token: 5358, token usage: 0.12, #running-req: 1016, #queue-req: 203, 
[aiter] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP3] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP1] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP2] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP4] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP0] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP6] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP7] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP5] [fused_moe] using default for (371, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:52504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:54518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:54802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:56992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:58264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38 TP0] Prefill batch, #new-seq: 8, #new-token: 499, #cached-token: 5360, token usage: 0.12, #running-req: 1016, #queue-req: 195, 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP1] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP2] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP4] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP3] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP7] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP0] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP6] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38 TP5] [fused_moe] using default for (499, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:38] INFO:     127.0.0.1:52596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:55714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:55744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:57586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:57600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:38] INFO:     127.0.0.1:58058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39 TP0] Prefill batch, #new-seq: 7, #new-token: 410, #cached-token: 4688, token usage: 0.12, #running-req: 1017, #queue-req: 188, 
[aiter] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP3] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP4] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP1] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP2] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP7] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP6] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP0] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP5] [fused_moe] using default for (410, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39] INFO:     127.0.0.1:51450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:54232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:54578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:57240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:58594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:59566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:60184 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP4] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP0] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP1] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP3] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP5] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP2] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP7] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP6] [fused_moe] using default for (1011, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP0] Prefill batch, #new-seq: 13, #new-token: 729, #cached-token: 8708, token usage: 0.12, #running-req: 1011, #queue-req: 175, 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP3] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP2] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP4] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP1] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP7] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP6] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP0] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP5] [fused_moe] using default for (729, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39] INFO:     127.0.0.1:51958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:52830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:53434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:53570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:57654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:59152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:59266 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP4] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP0] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP1] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP2] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP6] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP3] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP5] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP7] [fused_moe] using default for (1015, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP0] Prefill batch, #new-seq: 9, #new-token: 449, #cached-token: 6030, token usage: 0.12, #running-req: 1015, #queue-req: 166, 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP3] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP1] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP2] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP4] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP7] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP6] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP0] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP5] [fused_moe] using default for (449, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39] INFO:     127.0.0.1:52468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:52878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:54534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:55458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39 TP0] Prefill batch, #new-seq: 8, #new-token: 449, #cached-token: 5361, token usage: 0.12, #running-req: 1016, #queue-req: 158, 
[2025-11-07 14:17:39] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:58444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:59102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:59984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39 TP0] Prefill batch, #new-seq: 6, #new-token: 380, #cached-token: 4017, token usage: 0.12, #running-req: 1018, #queue-req: 152, 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP3] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP2] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP1] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP6] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP4] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP7] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP0] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39 TP5] [fused_moe] using default for (380, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:39] INFO:     127.0.0.1:55502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:55754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:57820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:58824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:59772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:59864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:39] INFO:     127.0.0.1:60328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40 TP0] Prefill batch, #new-seq: 7, #new-token: 398, #cached-token: 4690, token usage: 0.12, #running-req: 1017, #queue-req: 145, 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP3] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP4] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP1] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP2] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP6] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP7] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP0] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP5] [fused_moe] using default for (398, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:53716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:53902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:55896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:56850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:58248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:59518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40 TP0] Prefill batch, #new-seq: 7, #new-token: 486, #cached-token: 4694, token usage: 0.12, #running-req: 1017, #queue-req: 138, 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP3] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP2] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP1] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP6] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP4] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP0] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP7] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP5] [fused_moe] using default for (486, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40] INFO:     127.0.0.1:51968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:52364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:52912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:54234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:54700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:56782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40 TP0] Prefill batch, #new-seq: 11, #new-token: 600, #cached-token: 7368, token usage: 0.12, #running-req: 1013, #queue-req: 127, 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP3] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP2] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP4] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP1] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP6] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP0] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP7] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP5] [fused_moe] using default for (600, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40] INFO:     127.0.0.1:52170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:53126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:56542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:57616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:57708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:59304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:59456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:60276 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP4] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP0] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP2] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP1] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP3] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP6] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP7] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP5] [fused_moe] using default for (1012, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP0] Prefill batch, #new-seq: 12, #new-token: 844, #cached-token: 8041, token usage: 0.12, #running-req: 1012, #queue-req: 115, 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP3] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP2] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP1] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP6] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP4] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP0] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP7] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP5] [fused_moe] using default for (844, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:53886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:54266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:54566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:54676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:54720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:56650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:57276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:58408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:59354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:59486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:59674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40 TP0] Prefill batch, #new-seq: 13, #new-token: 929, #cached-token: 8714, token usage: 0.12, #running-req: 1011, #queue-req: 102, 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP3] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP4] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP2] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP1] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP0] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP6] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP7] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40 TP5] [fused_moe] using default for (929, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:40] INFO:     127.0.0.1:52074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:52696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:52794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:54292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:56534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:59232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:40] INFO:     127.0.0.1:59370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41 TP0] Prefill batch, #new-seq: 7, #new-token: 424, #cached-token: 4692, token usage: 0.12, #running-req: 1017, #queue-req: 95, 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP2] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP3] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP1] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP4] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP0] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP7] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP6] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP5] [fused_moe] using default for (424, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41] INFO:     127.0.0.1:51716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:53340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:53978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:56038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:58450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:59400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:59652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41 TP0] Prefill batch, #new-seq: 11, #new-token: 590, #cached-token: 7369, token usage: 0.12, #running-req: 1013, #queue-req: 84, 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP3] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP2] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP1] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP4] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP0] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP6] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP7] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP5] [fused_moe] using default for (590, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:53648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:56628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:58724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:59402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41 TP0] Prefill batch, #new-seq: 13, #new-token: 760, #cached-token: 8705, token usage: 0.12, #running-req: 1011, #queue-req: 71, 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP2] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP3] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP1] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP4] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP0] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP6] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP7] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP5] [fused_moe] using default for (760, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41] INFO:     127.0.0.1:51638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:52030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:52072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:52358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:54110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:54226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:56182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:57020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:57568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:58182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:58318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:59344 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP4] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP0] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP1] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP2] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP3] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP5] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP7] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP6] [fused_moe] using default for (1008, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP0] Prefill batch, #new-seq: 16, #new-token: 1156, #cached-token: 10717, token usage: 0.12, #running-req: 1008, #queue-req: 55, 
[2025-11-07 14:17:41] INFO:     127.0.0.1:52290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:52738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:54588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:54922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:55864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:56828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:56964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:57226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:58478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:59146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:59192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41 TP0] Prefill batch, #new-seq: 12, #new-token: 773, #cached-token: 8039, token usage: 0.12, #running-req: 1012, #queue-req: 43, 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP3] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP2] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP1] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP4] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP0] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP6] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP7] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41 TP5] [fused_moe] using default for (773, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:41] INFO:     127.0.0.1:52982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:53304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:53550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:55108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:55348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:55536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:55900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:56856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:57060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:57746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:59272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:41] INFO:     127.0.0.1:59796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42 TP0] Prefill batch, #new-seq: 12, #new-token: 680, #cached-token: 8040, token usage: 0.12, #running-req: 1012, #queue-req: 31, 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP3] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP4] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP2] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP1] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP6] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP0] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP7] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP5] [fused_moe] using default for (680, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42] INFO:     127.0.0.1:51662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:52694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:53608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:53956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:54538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:54930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:56274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:56566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:57612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:57984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:58552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:58654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:59936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:60450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:60454 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP4] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP0] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP1] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP5] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP3] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP7] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP2] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP6] [fused_moe] using default for (1005, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP0] Prefill batch, #new-seq: 19, #new-token: 1223, #cached-token: 12727, token usage: 0.12, #running-req: 1005, #queue-req: 12, 
[2025-11-07 14:17:42] INFO:     127.0.0.1:51686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:52708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:53110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:54352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:56700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:58164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:58544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42 TP0] Prefill batch, #new-seq: 9, #new-token: 456, #cached-token: 6032, token usage: 0.13, #running-req: 1015, #queue-req: 3, 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP3] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP1] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP2] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP0] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP4] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP6] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP7] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP5] [fused_moe] using default for (456, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP0] Decode batch, #running-req: 1015, #token: 120475, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5534.91, #queue-req: 3, 
[2025-11-07 14:17:42] INFO:     127.0.0.1:52084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:53994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:55238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:55820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:56092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:57976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:59240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:59596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:60306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:60316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42 TP0] Prefill batch, #new-seq: 3, #new-token: 153, #cached-token: 2013, token usage: 0.13, #running-req: 1011, #queue-req: 0, 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP3] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP1] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP2] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP4] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP0] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP6] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP7] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP5] [fused_moe] using default for (153, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42] INFO:     127.0.0.1:52546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP4] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP0] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP1] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP2] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP6] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP5] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP3] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42 TP7] [fused_moe] using default for (1009, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:42] INFO:     127.0.0.1:52934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:55712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:57402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:57842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:59178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:60114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:42] INFO:     127.0.0.1:60340 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP3] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP1] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP4] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP2] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP6] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP7] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP0] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP5] [fused_moe] using default for (999, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:52996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:55072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:55736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:60162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:60358 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP0] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP1] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP2] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP4] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP6] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP5] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP7] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP3] [fused_moe] using default for (990, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:53414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:53504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:55992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:57300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:57790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:58226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:58746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:58918 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP3] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP1] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP2] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP6] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP4] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP0] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP7] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP5] [fused_moe] using default for (980, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43] INFO:     127.0.0.1:52516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:53050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:53806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:55642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:57918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59828 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP4] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP0] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP5] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP1] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP2] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP6] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP7] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP3] [fused_moe] using default for (971, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43] INFO:     127.0.0.1:51704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:54378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:55780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:56280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59470 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP1] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP5] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP4] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP0] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP6] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP2] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP7] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP3] [fused_moe] using default for (961, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43] INFO:     127.0.0.1:51774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:51880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:52014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:53714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:56516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:58198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:58960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:60116 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP5] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP4] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP1] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP3] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP7] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP2] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP0] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP6] [fused_moe] using default for (946, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:52006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:52492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:53360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:55100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59436 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP4] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP0] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP6] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP5] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP2] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP1] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP7] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP3] [fused_moe] using default for (933, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43] INFO:     127.0.0.1:52340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:54730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:56470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:57416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:57688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59760 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP0] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP4] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP1] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP5] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP6] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP2] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP7] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP3] [fused_moe] using default for (924, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:53278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:56824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:57202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:58736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:59530 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP0] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP4] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP5] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP1] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP6] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP2] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP3] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43 TP7] [fused_moe] using default for (913, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:43] INFO:     127.0.0.1:52204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:52802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:53472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:57474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:58290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:43] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP3] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP1] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP6] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP5] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP7] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP4] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP2] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP0] [fused_moe] using default for (905, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44] INFO:     127.0.0.1:51722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:51940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:52368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:53194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:53426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:54704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:54832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:55048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:56006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:56242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:60106 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP4] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP7] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP5] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP6] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP0] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP3] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP1] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP2] [fused_moe] using default for (892, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44] INFO:     127.0.0.1:51762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:52156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:53684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:54186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:54300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:55924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:58486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:58994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:59282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:59460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:60546 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP4] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP3] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP1] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP7] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP5] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP0] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP2] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP6] [fused_moe] using default for (875, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:53344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:58708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:59784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:60056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:60372 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP4] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP0] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP3] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP1] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP7] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP2] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP6] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP5] [fused_moe] using default for (862, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44] INFO:     127.0.0.1:51700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:52434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:52680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:52856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:58512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:59384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:60420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:60436 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP4] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP0] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP2] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP6] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP3] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP7] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP1] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP5] [fused_moe] using default for (852, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44] INFO:     127.0.0.1:51980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:53540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:53798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:54810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:55036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:56244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:56600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:57756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:58192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:58626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:59746 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP4] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP0] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP2] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP3] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP6] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP7] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP1] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP5] [fused_moe] using default for (836, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44] INFO:     127.0.0.1:51542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:52614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:53722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:55130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:58888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:59710 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP4] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP0] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP3] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP7] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP2] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP6] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP1] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP5] [fused_moe] using default for (827, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44] INFO:     127.0.0.1:51950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:53408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:54610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:54690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:56378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:58144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:60198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:60210 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP4] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP0] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP3] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP2] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP7] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP6] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP1] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP5] [fused_moe] using default for (818, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44] INFO:     127.0.0.1:51966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:52526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:55258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:56748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:57902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:59320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:59404 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP4] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP0] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP3] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP7] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP2] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP6] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP1] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP5] [fused_moe] using default for (808, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:52298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:55404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:56100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:58428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:60456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:60528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:44] INFO:     127.0.0.1:60958 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP4] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP0] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP3] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP7] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP2] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP6] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP1] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:44 TP5] [fused_moe] using default for (799, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45] INFO:     127.0.0.1:52716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:58940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:59130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:59268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:59612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP4] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP3] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP0] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP7] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP2] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP6] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP1] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP5] [fused_moe] using default for (788, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45] INFO:     127.0.0.1:51900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:56228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:32786 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP4] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP3] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP0] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP2] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP6] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP7] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP1] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP5] [fused_moe] using default for (782, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45] INFO:     127.0.0.1:51724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:53238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:54400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:56002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:60588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:56900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:57016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:59810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:60256 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP4] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP2] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP3] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP0] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP6] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP7] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP1] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP5] [fused_moe] using default for (761, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:52686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:54268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:54748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:56020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:58462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:33388 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP4] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP2] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP6] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP3] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP0] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP7] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP1] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP5] [fused_moe] using default for (750, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45] INFO:     127.0.0.1:51646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:56916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:57694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:58376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:58692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:33404 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP4] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP2] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP6] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP0] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP3] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP7] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP1] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP5] [fused_moe] using default for (744, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP4] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP2] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP6] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP3] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP0] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP7] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP1] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP5] [fused_moe] using default for (736, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45] INFO:     127.0.0.1:53222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:57828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:58896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:60086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:60466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:54744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:57494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:58348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:54010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:54384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:60308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:60376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:32900 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP2] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP4] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP6] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP0] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP3] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP7] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP1] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP5] [fused_moe] using default for (721, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45] INFO:     127.0.0.1:51556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:51694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:52040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:52446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:53674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:54434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:55816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:56118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:57886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:58990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:45] INFO:     127.0.0.1:59118 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP2] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP4] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP6] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP1] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP0] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP3] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP5] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:45 TP7] [fused_moe] using default for (704, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46] INFO:     127.0.0.1:51750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:52730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:60632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:33116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:33336 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP4] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP0] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP2] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP6] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP1] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP5] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP3] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP7] [fused_moe] using default for (693, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46] INFO:     127.0.0.1:52642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:55848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:56116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:56388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:57204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:58868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:60776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:33036 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP4] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP0] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP2] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP6] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP1] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP5] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP3] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP7] [fused_moe] using default for (681, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46] INFO:     127.0.0.1:53810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:54580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:54894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:55892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:56902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:57554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:57940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:60290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:60694 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP4] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP0] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP2] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP6] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP1] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP5] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP3] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP7] [fused_moe] using default for (668, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46] INFO:     127.0.0.1:51596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:56558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:56596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:60044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:60378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:33074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:33372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:33546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:33686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:34554 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP4] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP0] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP2] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP6] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP1] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP5] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP3] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP7] [fused_moe] using default for (654, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:52964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:53726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:53790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:54314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:55910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:56434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:60536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:32818 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP4] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP0] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP2] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP6] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP1] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP5] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP3] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP7] [fused_moe] using default for (639, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:52406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:54198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:54790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:55984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:56286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:58640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:58984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:60280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:33102 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP4] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP2] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP0] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP6] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP5] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP1] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP3] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP7] [fused_moe] using default for (626, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46] INFO:     127.0.0.1:52628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:53908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:54162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:54338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:60440 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP4] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP0] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP2] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP6] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP3] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP1] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP7] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP5] [fused_moe] using default for (620, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46] INFO:     127.0.0.1:51612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:51928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:52242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:54146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:54238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:55610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:57234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:60564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:33522 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP4] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP0] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP2] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP6] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP3] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP1] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP7] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP5] [fused_moe] using default for (608, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46] INFO:     127.0.0.1:52304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:52356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:60228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:34718 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP4] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP0] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP2] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP6] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP1] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP5] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP3] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP7] [fused_moe] using default for (599, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP0] Decode batch, #running-req: 608, #token: 91268, token usage: 0.09, cuda graph: False, gen throughput (token/s): 7563.24, #queue-req: 0, 
[2025-11-07 14:17:46] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:57934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:59946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:60486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:46] INFO:     127.0.0.1:33934 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP4] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP0] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP2] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP6] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP1] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP5] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP3] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:46 TP7] [fused_moe] using default for (591, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47] INFO:     127.0.0.1:53410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:57516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:58148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:60270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:60736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33014 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP4] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP0] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP2] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP6] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP1] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP5] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP3] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP7] [fused_moe] using default for (584, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47] INFO:     127.0.0.1:52126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:53460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:55284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:58346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:59390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33940 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP4] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP0] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP2] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP6] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP1] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP5] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP3] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP7] [fused_moe] using default for (574, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47] INFO:     127.0.0.1:51494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:51920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:52608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:53574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:55440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:57090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:60750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:32944 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP4] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP0] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP2] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP6] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP1] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP5] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP3] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP7] [fused_moe] using default for (562, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47] INFO:     127.0.0.1:53496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:54498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:55496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:57796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:58840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:58966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:59054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:59216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:60684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:60706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:60856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:34596 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP4] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP0] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP2] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP6] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP1] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP5] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP3] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP7] [fused_moe] using default for (549, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:55690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:56778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:59500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:60362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:60628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:34146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:34254 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP4] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP0] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP2] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP6] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP1] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP5] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP3] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP7] [fused_moe] using default for (539, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47] INFO:     127.0.0.1:52420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:57186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:59906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:60912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33458 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP4] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP0] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP2] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP6] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP1] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP5] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP3] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP7] [fused_moe] using default for (532, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47] INFO:     127.0.0.1:53632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:54224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:54250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:54450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:58394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:58446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:59286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:59512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:59620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:34088 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP4] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP0] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP2] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP6] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP1] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP5] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP3] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47 TP7] [fused_moe] using default for (518, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:17:47] INFO:     127.0.0.1:51888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:53064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:53960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:57328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:57622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:57772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:59052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:60050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:32986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:53172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:56192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:53680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:55322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:55564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:57532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:58210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:58778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:59434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:59534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:51884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:54852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:54952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:58876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:33432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:53824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:54170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:55272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:57284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:58010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:47] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:52188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:53854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:54274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:54536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:58674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:60836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:54678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:55310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:56082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:51850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:53210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:55962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:57956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:58944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:59016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:59044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:60238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:53880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:55088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:58760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:60614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:51468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:53182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:53764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:53796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:56156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:59972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:32808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:32848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:51514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:53748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:57004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:57310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:57452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:55210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:56162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:56680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:57318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:58800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:54394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:55516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:57540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:60096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:60734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:56464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:59200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:58342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:60216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:60802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:32956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:52228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:54488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:56886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:57076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:58418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:57470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:60474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:52320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:53160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:54908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:57994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:58306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:59168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:59894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:60012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:55592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:56658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:58078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:59822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:33260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:48] INFO:     127.0.0.1:34108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:53292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:54966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:56490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:58798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:59176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:53150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:55394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:59850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:32970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:52656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:32918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:51802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:52844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:54172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:53274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:52760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:52954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:54328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:56324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:59882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:53544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:53976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49 TP0] Decode batch, #running-req: 281, #token: 52320, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6071.73, #queue-req: 0, 
[2025-11-07 14:17:49] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:53380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:53704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:55196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:54948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:52780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:52060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:53756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:54472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:60572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:33716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:49] INFO:     127.0.0.1:34798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:52034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:58004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:55166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:52950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:54060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:56400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:60146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:60294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:54780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:60822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:58522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:60038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:60130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:51862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:60500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:52848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:56374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:55416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:54138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:56980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:32792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:52534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:52556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:60620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:55034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:60868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:51730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:52146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:53834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:55522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:56744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:56476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:55836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:55112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:56950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:60946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:32910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:33232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:50] INFO:     127.0.0.1:34154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:60990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:32902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:54080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:60974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:52334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:60244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:60844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:53646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:60882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:32796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:60678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:51788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:56842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:53320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:56500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:59244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:60922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:58046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51 TP0] Decode batch, #running-req: 109, #token: 25806, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3814.96, #queue-req: 0, 
[2025-11-07 14:17:51] INFO:     127.0.0.1:56614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:52210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:52142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:53864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:54422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:55236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:60556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:53028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:60532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:33492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:59738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:60406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:52276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:34000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:52776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:51] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:58768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:33834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:60658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:33618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:55232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:32862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:33538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:51784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:60728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:33278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:58498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:33220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:33760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:59398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:54026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:32774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:60426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:33136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:60392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:57638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:33936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:33726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:34382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:33866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:60666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:33678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:52] INFO:     127.0.0.1:59664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53 TP0] Decode batch, #running-req: 29, #token: 8693, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1701.93, #queue-req: 0, 
[2025-11-07 14:17:53] INFO:     127.0.0.1:34696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:52766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:33478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:54896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:57724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:32932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:34828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:60344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:52624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:33610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:33558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:34422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:32992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:33146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:32830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:53] INFO:     127.0.0.1:33822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:54] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:54 TP0] Decode batch, #running-req: 7, #token: 3062, token usage: 0.00, cuda graph: True, gen throughput (token/s): 668.18, #queue-req: 0, 
[2025-11-07 14:17:54] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:54] INFO:     127.0.0.1:60594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:54] INFO:     127.0.0.1:58276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:54] INFO:     127.0.0.1:33544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:54] INFO:     127.0.0.1:60268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:54 TP0] Decode batch, #running-req: 2, #token: 1371, token usage: 0.00, cuda graph: True, gen throughput (token/s): 147.69, #queue-req: 0, 
[2025-11-07 14:17:54] INFO:     127.0.0.1:53920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:17:55] INFO:     127.0.0.1:33654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:18:07] INFO:     127.0.0.1:59140 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-07 14:18:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-07 14:18:08 TP7] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-07 14:18:08 TP0] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-07 14:18:08 TP3] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-07 14:18:08 TP1] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-07 14:18:08 TP4] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-07 14:18:08 TP6] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-07 14:18:08 TP2] waiting for baton release at /sgl-workspace/aiter/aiter/jit/build/lock_mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[aiter] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
[2025-11-07 14:18:08 TP5] start build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip] under /sgl-workspace/aiter/aiter/jit/build/mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for gfx942.
warning: unknown warning option '-Wno-missing-template-arg-list-after-template-kw'; did you mean '-Wno-gnu-string-literal-operator-template'? [-Wunknown-warning-option]
1 warning generated when compiling for host.
[92mSuccessfully preprocessed all matching files.[0m
[aiter] finish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip], cost 53.62577739s
[2025-11-07 14:19:01 TP5] finish build [mha_varlen_fwd_bf16_nlogits_nbias_nmask_nlse_ndropout_nskip], cost 53.62577739s
[2025-11-07 14:19:02] INFO:     127.0.0.1:59150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:19:02 TP0] Prefill batch, #new-seq: 42, #new-token: 42, #cached-token: 30645, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP0] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP6] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP3] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP2] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP4] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP5] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP7] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP1] [fused_moe] using default for (42, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP0] Prefill batch, #new-seq: 47, #new-token: 47, #cached-token: 33987, token usage: 0.01, #running-req: 43, #queue-req: 0, 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP0] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP3] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP4] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP2] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP1] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP7] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP5] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP6] [fused_moe] using default for (47, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP0] Prefill batch, #new-seq: 54, #new-token: 54, #cached-token: 39253, token usage: 0.01, #running-req: 90, #queue-req: 0, 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP0] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP3] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP2] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP1] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP4] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP6] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP7] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP5] [fused_moe] using default for (54, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP0] Prefill batch, #new-seq: 53, #new-token: 53, #cached-token: 38928, token usage: 0.01, #running-req: 144, #queue-req: 0, 
[2025-11-07 14:19:02 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44445, token usage: 0.02, #running-req: 197, #queue-req: 0, 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP0] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP1] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP2] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP3] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP4] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP6] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP5] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP7] [fused_moe] using default for (61, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 43464, token usage: 0.02, #running-req: 258, #queue-req: 0, 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP3] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP1] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP2] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP0] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP5] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP7] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP4] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP6] [fused_moe] using default for (60, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:02 TP0] Prefill batch, #new-seq: 67, #new-token: 67, #cached-token: 48769, token usage: 0.02, #running-req: 318, #queue-req: 0, 
[2025-11-07 14:19:03 TP0] Prefill batch, #new-seq: 66, #new-token: 66, #cached-token: 48343, token usage: 0.03, #running-req: 385, #queue-req: 0, 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP3] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP0] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP2] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP1] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP4] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP5] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP7] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP6] [fused_moe] using default for (66, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP0] Prefill batch, #new-seq: 74, #new-token: 74, #cached-token: 53584, token usage: 0.03, #running-req: 451, #queue-req: 0, 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP5] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP0] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP3] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP7] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP4] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP1] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP6] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP2] [fused_moe] using default for (74, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP0] Prefill batch, #new-seq: 74, #new-token: 74, #cached-token: 53784, token usage: 0.04, #running-req: 525, #queue-req: 0, 
[2025-11-07 14:19:03 TP0] Prefill batch, #new-seq: 80, #new-token: 80, #cached-token: 58165, token usage: 0.04, #running-req: 599, #queue-req: 0, 
[2025-11-07 14:19:03 TP0] Prefill batch, #new-seq: 79, #new-token: 79, #cached-token: 57558, token usage: 0.05, #running-req: 679, #queue-req: 0, 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP5] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP0] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP3] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP2] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP4] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP1] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP7] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP6] [fused_moe] using default for (79, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP0] Prefill batch, #new-seq: 87, #new-token: 87, #cached-token: 63553, token usage: 0.05, #running-req: 758, #queue-req: 0, 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP4] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP3] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP0] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP2] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP1] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP6] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP7] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP5] [fused_moe] using default for (87, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:03 TP0] Prefill batch, #new-seq: 85, #new-token: 85, #cached-token: 61659, token usage: 0.06, #running-req: 845, #queue-req: 0, 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP0] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP3] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP4] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP2] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP6] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP5] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP1] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP7] [fused_moe] using default for (85, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP0] Prefill batch, #new-seq: 92, #new-token: 92, #cached-token: 67690, token usage: 0.06, #running-req: 930, #queue-req: 0, 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP3] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP0] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP1] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP2] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP7] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP5] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP4] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP6] [fused_moe] using default for (92, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:04 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1452, token usage: 0.06, #running-req: 1022, #queue-req: 22, 
[2025-11-07 14:19:07] INFO:     127.0.0.1:45624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 745, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-07 14:19:07] INFO:     127.0.0.1:51286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:07 TP0] Decode batch, #running-req: 1024, #token: 95450, token usage: 0.10, cuda graph: False, gen throughput (token/s): 448.91, #queue-req: 294, 
[2025-11-07 14:19:07] INFO:     127.0.0.1:43382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 718, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-07 14:19:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 736, token usage: 0.10, #running-req: 1023, #queue-req: 292, 
[2025-11-07 14:19:08] INFO:     127.0.0.1:45674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 723, token usage: 0.10, #running-req: 1023, #queue-req: 291, 
[2025-11-07 14:19:08] INFO:     127.0.0.1:43268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:08] INFO:     127.0.0.1:46310 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:08 TP0] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:08 TP6] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:08 TP7] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:08 TP4] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:08 TP3] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:08 TP5] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:08 TP2] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:08 TP1] [fused_moe] using default for (1022, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:08] INFO:     127.0.0.1:42690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:08] INFO:     127.0.0.1:43848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:08] INFO:     127.0.0.1:46952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:08 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1475, token usage: 0.11, #running-req: 1022, #queue-req: 289, 
[2025-11-07 14:19:09] INFO:     127.0.0.1:43240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:43710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:43774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:44678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:45024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6587, token usage: 0.11, #running-req: 1015, #queue-req: 280, 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP0] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP4] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP3] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP6] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP5] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP7] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP1] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP2] [fused_moe] using default for (9, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09] INFO:     127.0.0.1:43814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:45630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:47092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:47478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:48368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4376, token usage: 0.11, #running-req: 1018, #queue-req: 274, 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP0] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP4] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP6] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP3] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP2] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP1] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP7] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP5] [fused_moe] using default for (6, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09] INFO:     127.0.0.1:43208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:43346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:43478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:45268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:47014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:47484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:47560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:47842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6494, token usage: 0.11, #running-req: 1015, #queue-req: 265, 
[2025-11-07 14:19:09] INFO:     127.0.0.1:44072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:44098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:45610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:45996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:47666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:51494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5009, token usage: 0.11, #running-req: 1017, #queue-req: 258, 
[2025-11-07 14:19:09] INFO:     127.0.0.1:43168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:45886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:46438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:50104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09] INFO:     127.0.0.1:50722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:09 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3613, token usage: 0.11, #running-req: 1019, #queue-req: 253, 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP4] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP6] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP3] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP0] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP2] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP7] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP1] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:09 TP5] [fused_moe] using default for (5, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:10] INFO:     127.0.0.1:47234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2214, token usage: 0.11, #running-req: 1021, #queue-req: 250, 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:10 TP4] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:10 TP6] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:10 TP0] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:10 TP3] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:10 TP2] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:10 TP1] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:10 TP7] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:10 TP5] [fused_moe] using default for (3, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:10] INFO:     127.0.0.1:43042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:44268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:46278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:47462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:49318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:50570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:50876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:51428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5807, token usage: 0.11, #running-req: 1016, #queue-req: 242, 
[2025-11-07 14:19:10] INFO:     127.0.0.1:42674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:45040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:45072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:50370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:50456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5929, token usage: 0.11, #running-req: 1016, #queue-req: 234, 
[2025-11-07 14:19:10] INFO:     127.0.0.1:44168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:44880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:46124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:49068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:49932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:50438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4408, token usage: 0.11, #running-req: 1018, #queue-req: 228, 
[2025-11-07 14:19:10] INFO:     127.0.0.1:43182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:43542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:44226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:46914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:47152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:47584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:48236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10] INFO:     127.0.0.1:51558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:10 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5874, token usage: 0.12, #running-req: 1016, #queue-req: 220, 
[2025-11-07 14:19:11] INFO:     127.0.0.1:44744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:45724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:45988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:46052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:46818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:49212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:50760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:51876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7907, token usage: 0.12, #running-req: 1013, #queue-req: 209, 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP4] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP3] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP6] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP0] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP7] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP1] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP2] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP5] [fused_moe] using default for (11, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11] INFO:     127.0.0.1:43788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:44138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:45830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:46558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:47020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:47850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:48526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:49010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:49128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:50710 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP4] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP3] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP7] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP0] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP1] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP5] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP6] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP2] [fused_moe] using default for (1014, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7262, token usage: 0.12, #running-req: 1014, #queue-req: 199, 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP0] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP6] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP4] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP3] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP2] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP7] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP5] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP1] [fused_moe] using default for (10, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11] INFO:     127.0.0.1:43104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:43900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:46746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:48438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:49726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5078, token usage: 0.12, #running-req: 1017, #queue-req: 192, 
[2025-11-07 14:19:11] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:48176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:49052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:49550 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP4] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP5] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP1] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP3] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP7] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP0] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP6] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP2] [fused_moe] using default for (1020, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2926, token usage: 0.12, #running-req: 1020, #queue-req: 188, 
[2025-11-07 14:19:11] INFO:     127.0.0.1:42662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:44410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:44484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:44508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:44942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:45148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:46062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:48686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:48850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:49222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:49662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11] INFO:     127.0.0.1:51552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:11 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9424, token usage: 0.12, #running-req: 1011, #queue-req: 175, 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP6] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP4] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP0] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP3] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP2] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP1] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP5] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:11 TP7] [fused_moe] using default for (13, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:12] INFO:     127.0.0.1:43184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:43680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:43910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:44956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:45224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:45494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:50586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5757, token usage: 0.12, #running-req: 1016, #queue-req: 167, 
[2025-11-07 14:19:12] INFO:     127.0.0.1:43592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:43850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:45050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:45706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:46028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:47662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:48628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:49488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5775, token usage: 0.12, #running-req: 1016, #queue-req: 159, 
[2025-11-07 14:19:12] INFO:     127.0.0.1:43276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:44978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:48754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:49902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:51192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5087, token usage: 0.12, #running-req: 1017, #queue-req: 152, 
[2025-11-07 14:19:12] INFO:     127.0.0.1:44570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:46918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:46986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:47434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:49388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4374, token usage: 0.12, #running-req: 1018, #queue-req: 146, 
[2025-11-07 14:19:12] INFO:     127.0.0.1:44492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:44936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:45214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:47026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:48316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:50714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:12 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6632, token usage: 0.12, #running-req: 1015, #queue-req: 137, 
[2025-11-07 14:19:13] INFO:     127.0.0.1:42796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:43252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:43462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:43948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:44338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:44436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:44874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:46184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:47218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:47646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:48188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10959, token usage: 0.12, #running-req: 1009, #queue-req: 122, 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:13 TP0] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:13 TP4] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:13 TP6] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:13 TP3] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:13 TP2] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:13 TP5] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:13 TP7] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:13 TP1] [fused_moe] using default for (15, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:13] INFO:     127.0.0.1:47974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:49080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:50396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:50812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5182, token usage: 0.12, #running-req: 1017, #queue-req: 115, 
[2025-11-07 14:19:13] INFO:     127.0.0.1:42702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:45264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:45752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:46136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:48688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:49574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5166, token usage: 0.12, #running-req: 1017, #queue-req: 108, 
[2025-11-07 14:19:13] INFO:     127.0.0.1:43386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:43512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:43524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:43540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:44994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:45780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:46218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:46680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:47964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:49846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:50660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:50820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9543, token usage: 0.12, #running-req: 1011, #queue-req: 95, 
[2025-11-07 14:19:13] INFO:     127.0.0.1:43086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:44164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:44476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:44644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:44850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:45416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:45620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:46408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:46928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:46962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:47076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:47720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:50874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:13 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10832, token usage: 0.12, #running-req: 1009, #queue-req: 80, 
[2025-11-07 14:19:14] INFO:     127.0.0.1:43666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:44100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:44196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:44324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:44720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:44838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:48056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:48168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:50858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8139, token usage: 0.12, #running-req: 1013, #queue-req: 69, 
[2025-11-07 14:19:14] INFO:     127.0.0.1:42862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:42912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:43218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:44692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:44712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:45278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:45742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:46196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:47562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:48326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:48416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:49046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:49604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:49650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:49778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:50808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP4] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP0] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP6] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP5] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP2] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP1] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP3] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP7] [fused_moe] using default for (1006, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP0] Prefill batch, #new-seq: 18, #new-token: 18, #cached-token: 13284, token usage: 0.12, #running-req: 1006, #queue-req: 51, 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP0] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP3] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP1] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP2] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP4] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP6] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP7] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP5] [fused_moe] using default for (18, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14] INFO:     127.0.0.1:44172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:44452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:44590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:46068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:46388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:47282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:48218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:48640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:49578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:50582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:51134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:51568 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP4] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP1] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP5] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP7] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP3] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP0] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP6] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP2] [fused_moe] using default for (1010, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10134, token usage: 0.12, #running-req: 1010, #queue-req: 37, 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP4] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP0] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP6] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP3] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP5] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP2] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP1] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14 TP7] [fused_moe] using default for (14, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:14] INFO:     127.0.0.1:44892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:46552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:46808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:47502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:48240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:48476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5111, token usage: 0.13, #running-req: 1017, #queue-req: 30, 
[2025-11-07 14:19:14] INFO:     127.0.0.1:42890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:45316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:45580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:45930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:46042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:46392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:46698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:47700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:47998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:49820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:50082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:50144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:14 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10224, token usage: 0.13, #running-req: 1010, #queue-req: 16, 
[2025-11-07 14:19:15] INFO:     127.0.0.1:43658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:43844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:44016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:45852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:46166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:46582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:46644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:48106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:49620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9462, token usage: 0.13, #running-req: 1011, #queue-req: 3, 
[2025-11-07 14:19:15 TP0] Decode batch, #running-req: 1011, #token: 120337, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5524.06, #queue-req: 3, 
[2025-11-07 14:19:15] INFO:     127.0.0.1:43056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:44056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:44514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:45350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:45850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:46452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:47032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:47368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:48862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:49486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:50040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:50690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2163, token usage: 0.13, #running-req: 1010, #queue-req: 0, 
[2025-11-07 14:19:15] INFO:     127.0.0.1:43122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:43630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:46192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:46822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:48766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:49940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51768 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP4] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP0] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP5] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP6] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP1] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP2] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP3] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP7] [fused_moe] using default for (1003, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15] INFO:     127.0.0.1:44256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:44308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:48470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:48834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:50610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51470 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP0] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP4] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP2] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP3] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP1] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP6] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP7] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP5] [fused_moe] using default for (993, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15] INFO:     127.0.0.1:44000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:45312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:46544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:47094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:47298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:47312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:47792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:49348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:49428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51518 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP4] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP1] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP5] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP2] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP6] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP0] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP7] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP3] [fused_moe] using default for (982, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15] INFO:     127.0.0.1:44878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:45944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:47080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:47882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:48708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:49686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:50032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:50202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:50916 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP4] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP5] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP0] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP6] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP2] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP1] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP3] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP7] [fused_moe] using default for (973, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15] INFO:     127.0.0.1:43326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:43800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:43980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:44404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:45122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:46106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:47268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:15] INFO:     127.0.0.1:51240 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP0] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP1] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP6] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP2] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP5] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP4] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP7] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:15 TP3] [fused_moe] using default for (963, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:45862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:47694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:47708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:48952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:51152 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP4] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP5] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP2] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP6] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP0] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP3] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP7] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP1] [fused_moe] using default for (953, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16] INFO:     127.0.0.1:43466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:45486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:45902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:46902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:47724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:47944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:48424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:48466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:51904 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP4] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP2] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP6] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP0] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP1] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP5] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP3] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP7] [fused_moe] using default for (939, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16] INFO:     127.0.0.1:42834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:44670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:47492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50906 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP4] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP2] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP0] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP6] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP5] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP1] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP3] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP7] [fused_moe] using default for (931, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16] INFO:     127.0.0.1:42974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:45068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:48798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:48838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP1] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP0] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP4] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP5] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP2] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP6] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP7] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP3] [fused_moe] using default for (923, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16] INFO:     127.0.0.1:43120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:44532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:44608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:48206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:48912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:51010 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP4] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP5] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP0] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP2] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP6] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP1] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP3] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP7] [fused_moe] using default for (911, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16] INFO:     127.0.0.1:43486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:44654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:46000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:46554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:47062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:49748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:51706 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP4] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP6] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP2] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP0] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP5] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP1] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP3] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP7] [fused_moe] using default for (900, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16] INFO:     127.0.0.1:42932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:46090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:46212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:46324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:46518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:47334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:48202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP4] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP2] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP6] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP0] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP5] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP1] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP3] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP7] [fused_moe] using default for (889, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16] INFO:     127.0.0.1:42742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:42978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:44764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:45110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:46504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:46712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:47138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:47364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:48524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:51994 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP6] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP2] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP4] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP0] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP5] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP1] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP3] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP7] [fused_moe] using default for (873, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16] INFO:     127.0.0.1:43396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:45662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:46546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:48076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:49998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:51722 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP2] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP4] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP6] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP0] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP5] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP1] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP3] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP7] [fused_moe] using default for (864, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16] INFO:     127.0.0.1:43030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:43882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:45362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:49030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:49666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:50828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:51464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:16] INFO:     127.0.0.1:51508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP2] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP4] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP6] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP0] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP5] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP1] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP3] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:16 TP7] [fused_moe] using default for (856, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17] INFO:     127.0.0.1:43930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:44116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:44790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:44930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:45766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:45948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:46140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:46306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:47252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:47554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:47696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:47824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:48040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:49380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:49680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:50118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:50886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:43752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:44312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:44780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:48782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:49142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:51164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:51184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:51536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:43560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:49634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP4] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP6] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP2] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP5] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP0] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP7] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP1] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP3] [fused_moe] using default for (823, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17] INFO:     127.0.0.1:43412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:43918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:44048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:44326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:45482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:45578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:45954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:46108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:47378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:48126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:49420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP4] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP6] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP2] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP0] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP5] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP3] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP1] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP7] [fused_moe] using default for (811, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17] INFO:     127.0.0.1:42696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:45082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:46868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:49710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:51316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:51926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:52434 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP4] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP6] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP2] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP5] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP0] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP7] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP1] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP3] [fused_moe] using default for (804, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17] INFO:     127.0.0.1:42944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:44562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:46462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:47748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:50172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:50564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:50776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:51796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:52088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP4] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP5] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP6] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP0] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP2] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP1] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP3] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP7] [fused_moe] using default for (793, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17] INFO:     127.0.0.1:46776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:47674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP4] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP5] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP1] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP0] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP3] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP7] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP2] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP6] [fused_moe] using default for (790, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17] INFO:     127.0.0.1:43434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:45876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:46356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:46496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:46592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:47104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:47982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:49156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP4] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP1] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP5] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP6] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP0] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP2] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP3] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP7] [fused_moe] using default for (780, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17] INFO:     127.0.0.1:42924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:43744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:44212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:46150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:46598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:46878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:47122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:47200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:48292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:48664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:50244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:50376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:50908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:51324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:45012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:45112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:45708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:46234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:46968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:48612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:49238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:49836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:51530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:17] INFO:     127.0.0.1:53132 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP4] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP5] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP0] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP2] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP6] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP1] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP3] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:17 TP7] [fused_moe] using default for (754, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18] INFO:     127.0.0.1:42876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:45694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:45732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:48314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:49196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:51210 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP4] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP5] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP0] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP6] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP2] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP1] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP3] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP7] [fused_moe] using default for (748, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP4] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP5] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP0] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP2] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP6] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP1] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP3] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP7] [fused_moe] using default for (741, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18] INFO:     127.0.0.1:43704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:45532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:46616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:50324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:51810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:51820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:43768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:46226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:46972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:47000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:47046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:50716 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP4] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP0] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP2] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP6] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP5] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP1] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP3] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP7] [fused_moe] using default for (732, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18] INFO:     127.0.0.1:45098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:47186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:49548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:49834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:50734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:51628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:51688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:52566 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP4] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP0] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP6] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP2] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP5] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP1] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP3] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP7] [fused_moe] using default for (724, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18] INFO:     127.0.0.1:42748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:43004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:44040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:44398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:44754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:45404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:45922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:46468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:46480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:49338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:52520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:52814 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP4] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP6] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP2] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP0] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP5] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP1] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP3] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP7] [fused_moe] using default for (710, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18] INFO:     127.0.0.1:43420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:45394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:45646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:49774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:50940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:51388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:52244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:53072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP4] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP6] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP2] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP0] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP5] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP1] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP3] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP7] [fused_moe] using default for (697, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18] INFO:     127.0.0.1:43620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:44370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:44544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:46060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:46826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:48600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:49014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:49186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:50462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:52708 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP6] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP2] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP4] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP0] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP5] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP1] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP3] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP7] [fused_moe] using default for (686, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18] INFO:     127.0.0.1:43642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:44434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:44736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:47514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:48134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:49476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:51630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:51830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:53296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:54314 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP4] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP6] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP2] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP0] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP5] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP1] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP3] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP7] [fused_moe] using default for (673, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18] INFO:     127.0.0.1:47710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:47834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:47992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:48024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:50320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:50650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:51034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:51410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:52750 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP4] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP6] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP2] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP0] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP5] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP1] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP3] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP7] [fused_moe] using default for (664, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18] INFO:     127.0.0.1:42846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:43538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:43628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:45468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:46382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:47182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:48306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:51892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:52798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:18] INFO:     127.0.0.1:53438 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP4] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP6] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP2] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP0] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP5] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP1] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP3] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:18 TP7] [fused_moe] using default for (652, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19] INFO:     127.0.0.1:44832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:45538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:49000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:49508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:50128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:51750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:51778 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP4] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP6] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP2] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP0] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP5] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP1] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP3] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP7] [fused_moe] using default for (644, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19] INFO:     127.0.0.1:42988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:44130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:44242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:44702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:45064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:45860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:46890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:47212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:48690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP4] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP6] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP2] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP0] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP5] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP1] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP3] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP7] [fused_moe] using default for (634, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19] INFO:     127.0.0.1:42818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:43134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:43234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:43746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:43764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:45048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:45870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:46954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:48148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:51146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:51860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP4] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP6] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP2] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP0] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP5] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP1] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP3] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP7] [fused_moe] using default for (619, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19] INFO:     127.0.0.1:44026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:44112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:44380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:46520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:51880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:53128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:54450 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP4] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP6] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP2] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP0] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP5] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP1] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP3] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP7] [fused_moe] using default for (610, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP0] Decode batch, #running-req: 619, #token: 93011, token usage: 0.10, cuda graph: False, gen throughput (token/s): 7844.14, #queue-req: 0, 
[2025-11-07 14:19:19] INFO:     127.0.0.1:42972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:43014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:44828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:44914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:48570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:49460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:51362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:51984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:53694 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP4] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP6] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP2] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP0] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP5] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP1] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP7] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP3] [fused_moe] using default for (601, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19] INFO:     127.0.0.1:44150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:44280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:44424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:46268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:46702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:46742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:49608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:50446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:50746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:52212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:45200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:45338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:47216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:48020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:49324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:49716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:51862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:52616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:54272 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP4] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP6] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP2] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP0] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP5] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP1] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP3] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP7] [fused_moe] using default for (579, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19] INFO:     127.0.0.1:43308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:43460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:44054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:44638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:46252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:48508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:49694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:52228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:52322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:54350 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP4] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP6] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP5] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP7] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP2] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP0] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP1] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP3] [fused_moe] using default for (569, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19] INFO:     127.0.0.1:42762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:43372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:45136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:45510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:45814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:45974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:46838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:46984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:47596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:50300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:50344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:50516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:50842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:52168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:52488 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP2] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP0] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP4] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP6] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP5] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP1] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP3] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP7] [fused_moe] using default for (554, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:44966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:45176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:47230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:48100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:48172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:48378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:50948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:19] INFO:     127.0.0.1:53914 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP4] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP6] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP0] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP2] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP5] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP1] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP3] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:19 TP7] [fused_moe] using default for (542, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20] INFO:     127.0.0.1:43608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:44512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:46338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:47788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:48200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:48730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:51336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:52902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP4] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP0] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP6] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP2] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP5] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP1] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP3] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP7] [fused_moe] using default for (526, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20] INFO:     127.0.0.1:42802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:47114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:49918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:50566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:52310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53722 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP4] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP0] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP6] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP2] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP5] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP1] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP3] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20 TP7] [fused_moe] using default for (515, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:20] INFO:     127.0.0.1:44552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:48936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:49298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:51412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:52502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:43726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:47642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:48818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:49126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:50160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:42694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:42740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:46834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:47340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:46348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:47636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:51086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:43142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:44800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:50502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:50528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:52918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:43158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:51174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:44816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:46768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:47384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:48452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:49482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:50380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:43468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:46080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:50480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:51606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:52628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:46726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:48802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:52040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:54320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:42678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:44528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:46172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:46244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:47430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:47622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:51378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:52370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:52564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:48062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:52550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:54094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:54150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:54596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:43832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:45872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:46178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:47442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:48090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:50248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:20] INFO:     127.0.0.1:54584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:44292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:46116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:47414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:48740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:49022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:51978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:43822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:45450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:46016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:46044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:45410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:46292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:46572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:47530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:48174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:48408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:46410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:48286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:48338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:47866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:44358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:44496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:46350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:49762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:50072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:50604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:51054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:43090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:43866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:45062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:45564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:49310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:49566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:44254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:44594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:45008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:48876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:49394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:50008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:51226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:47908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:50240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:43342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:44500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:46788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:49522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:46856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:51942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:44586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:46432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:51966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:43058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:48900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:44204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:45236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:46366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:47538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:50422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:52462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:21] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:45462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:43576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:49492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:52112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:52570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:42822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:44462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:45026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:45588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:46426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:47536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:52356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22 TP0] Decode batch, #running-req: 288, #token: 53289, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6055.46, #queue-req: 0, 
[2025-11-07 14:19:22] INFO:     127.0.0.1:43960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:48976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:49094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:52270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:52446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:49498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:51570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:52252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:52260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:42718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:43936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:51254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:51502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:51614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:42728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:44244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:47038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:52746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:44858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:50090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:51980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:44974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:45432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:48590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:51874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:42906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:45160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:45818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:46600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:51952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:52124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:45188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:45682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:46794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:45250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:45946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:46352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:47802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:49258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:49278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:46256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:51752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:44624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:46690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:48480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:52294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:54352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:46758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:48248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:22] INFO:     127.0.0.1:53360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:42998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:44234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:48050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:50034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:44118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:46854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:48370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:49594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:50794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:50022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:46502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:49290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:51968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:44908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:47436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:48162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:47326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:43316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:47764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:48926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:45808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:46568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:43492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:47938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:43302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:43736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:43886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:47924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:44920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:47128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:45314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:45696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:46326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:54466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:42780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:53976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:23] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:50236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:52516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:52732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:48886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:50698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24 TP0] Decode batch, #running-req: 107, #token: 25686, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3835.45, #queue-req: 0, 
[2025-11-07 14:19:24] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:42774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:47798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:54072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:43358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:52408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:52558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:45918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:51168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:44082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:47626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:43694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:45382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:48582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:44890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:43722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:46974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:54440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:46660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:46936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:52172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:52478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:52028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:43290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:52122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:50108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:54012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:24] INFO:     127.0.0.1:53956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:52686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:47898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:45716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:49944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:44394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:54188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:51842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:52830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:53830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:51848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:49802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:49110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:49876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:53680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:54178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:54654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:54682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:48038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:50124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:51138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25 TP0] Decode batch, #running-req: 36, #token: 10995, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1839.38, #queue-req: 0, 
[2025-11-07 14:19:25] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:49234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:53608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:53634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:53500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:44216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:52604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:25] INFO:     127.0.0.1:46638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:44186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:46360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:53222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:54424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:54546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:45544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:52534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:53302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:52876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:46340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:52458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:49124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:53376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26 TP0] Decode batch, #running-req: 12, #token: 3905, token usage: 0.00, cuda graph: True, gen throughput (token/s): 824.98, #queue-req: 0, 
[2025-11-07 14:19:26] INFO:     127.0.0.1:49734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:51918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:54522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:54572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:26] INFO:     127.0.0.1:53578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:27] INFO:     127.0.0.1:51674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:27] INFO:     127.0.0.1:51648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:27] INFO:     127.0.0.1:53370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:27 TP0] Decode batch, #running-req: 1, #token: 961, token usage: 0.00, cuda graph: True, gen throughput (token/s): 180.95, #queue-req: 0, 
[2025-11-07 14:19:27] INFO:     127.0.0.1:53422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:40] INFO:     127.0.0.1:47560 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-07 14:19:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:19:40] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:19:40 TP0] Prefill batch, #new-seq: 35, #new-token: 35, #cached-token: 25448, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP0] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP2] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP3] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP1] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP5] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP4] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP7] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP6] [fused_moe] using default for (35, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP0] Prefill batch, #new-seq: 46, #new-token: 46, #cached-token: 33409, token usage: 0.01, #running-req: 36, #queue-req: 0, 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP0] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP4] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP2] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP3] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP1] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP5] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP7] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP6] [fused_moe] using default for (46, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:40 TP0] Prefill batch, #new-seq: 53, #new-token: 53, #cached-token: 38554, token usage: 0.01, #running-req: 82, #queue-req: 0, 
[2025-11-07 14:19:40 TP0] Prefill batch, #new-seq: 54, #new-token: 54, #cached-token: 39627, token usage: 0.01, #running-req: 135, #queue-req: 0, 
[2025-11-07 14:19:41 TP0] Prefill batch, #new-seq: 65, #new-token: 65, #cached-token: 47243, token usage: 0.02, #running-req: 189, #queue-req: 0, 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:41 TP2] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:41 TP3] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:41 TP1] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:41 TP4] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:41 TP0] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:41 TP5] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:41 TP7] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:41 TP6] [fused_moe] using default for (65, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:41 TP0] Prefill batch, #new-seq: 56, #new-token: 56, #cached-token: 40652, token usage: 0.02, #running-req: 254, #queue-req: 0, 
[2025-11-07 14:19:41 TP0] Prefill batch, #new-seq: 68, #new-token: 68, #cached-token: 49549, token usage: 0.02, #running-req: 310, #queue-req: 0, 
[2025-11-07 14:19:41 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44546, token usage: 0.03, #running-req: 378, #queue-req: 0, 
[2025-11-07 14:19:41 TP0] Prefill batch, #new-seq: 74, #new-token: 74, #cached-token: 53696, token usage: 0.03, #running-req: 439, #queue-req: 0, 
[2025-11-07 14:19:41 TP0] Prefill batch, #new-seq: 67, #new-token: 67, #cached-token: 48597, token usage: 0.04, #running-req: 513, #queue-req: 0, 
[2025-11-07 14:19:41 TP0] Prefill batch, #new-seq: 80, #new-token: 80, #cached-token: 58220, token usage: 0.04, #running-req: 580, #queue-req: 0, 
[2025-11-07 14:19:42 TP0] Prefill batch, #new-seq: 72, #new-token: 72, #cached-token: 52430, token usage: 0.05, #running-req: 660, #queue-req: 0, 
[2025-11-07 14:19:42 TP0] Prefill batch, #new-seq: 86, #new-token: 86, #cached-token: 62782, token usage: 0.05, #running-req: 732, #queue-req: 0, 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP3] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP2] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP5] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP1] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP7] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP4] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP0] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP6] [fused_moe] using default for (86, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP0] Prefill batch, #new-seq: 53, #new-token: 53, #cached-token: 38535, token usage: 0.05, #running-req: 818, #queue-req: 0, 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP0] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP4] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP1] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP3] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP5] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP7] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP2] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP6] [fused_moe] using default for (871, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP0] Prefill batch, #new-seq: 38, #new-token: 38, #cached-token: 27481, token usage: 0.06, #running-req: 871, #queue-req: 0, 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP4] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP5] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP1] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP3] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP7] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP0] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP2] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP6] [fused_moe] using default for (38, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP0] Prefill batch, #new-seq: 53, #new-token: 53, #cached-token: 38896, token usage: 0.06, #running-req: 909, #queue-req: 0, 
[2025-11-07 14:19:42 TP0] Prefill batch, #new-seq: 50, #new-token: 50, #cached-token: 36739, token usage: 0.06, #running-req: 962, #queue-req: 0, 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP3] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP4] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP7] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP1] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP2] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP5] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP6] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:42 TP0] [fused_moe] using default for (50, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:43 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8827, token usage: 0.07, #running-req: 1012, #queue-req: 51, 
[2025-11-07 14:19:45] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 731, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-07 14:19:46] INFO:     127.0.0.1:48772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:46 TP0] Decode batch, #running-req: 1024, #token: 96103, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1776.93, #queue-req: 294, 
[2025-11-07 14:19:46] INFO:     127.0.0.1:55992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 748, token usage: 0.10, #running-req: 1023, #queue-req: 293, 
[2025-11-07 14:19:46] INFO:     127.0.0.1:50254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:46 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1475, token usage: 0.10, #running-req: 1022, #queue-req: 291, 
[2025-11-07 14:19:47] INFO:     127.0.0.1:47588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:48172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:50940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:51746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:47926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:52008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:53192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2915, token usage: 0.11, #running-req: 1020, #queue-req: 287, 
[2025-11-07 14:19:47] INFO:     127.0.0.1:48608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:49074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:49804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:50112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5891, token usage: 0.11, #running-req: 1016, #queue-req: 279, 
[2025-11-07 14:19:47] INFO:     127.0.0.1:48220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2196, token usage: 0.11, #running-req: 1021, #queue-req: 276, 
[2025-11-07 14:19:47] INFO:     127.0.0.1:47930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:48954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:51646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:52052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:52364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6449, token usage: 0.11, #running-req: 1015, #queue-req: 267, 
[2025-11-07 14:19:47] INFO:     127.0.0.1:48332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:52122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:47] INFO:     127.0.0.1:55978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2928, token usage: 0.11, #running-req: 1020, #queue-req: 263, 
[2025-11-07 14:19:48] INFO:     127.0.0.1:49458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:51102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:52672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:54274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3592, token usage: 0.11, #running-req: 1019, #queue-req: 258, 
[2025-11-07 14:19:48] INFO:     127.0.0.1:51848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:52252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:53442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:56086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3599, token usage: 0.11, #running-req: 1019, #queue-req: 253, 
[2025-11-07 14:19:48] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:51618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:53212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:54992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:55240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:55430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:56108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5845, token usage: 0.11, #running-req: 1016, #queue-req: 245, 
[2025-11-07 14:19:48] INFO:     127.0.0.1:47582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:49438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:49756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:50214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:52556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:53516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:53722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:54794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:55082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:48 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7398, token usage: 0.11, #running-req: 1014, #queue-req: 235, 
[2025-11-07 14:19:49] INFO:     127.0.0.1:48310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:48908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:49492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:54582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:55048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:55572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5853, token usage: 0.11, #running-req: 1016, #queue-req: 227, 
[2025-11-07 14:19:49] INFO:     127.0.0.1:48206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:48464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:51832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:51898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:52120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:53046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:55384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:56464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5872, token usage: 0.11, #running-req: 1016, #queue-req: 219, 
[2025-11-07 14:19:49] INFO:     127.0.0.1:49950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:50704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:51414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:51516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:53266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:53690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:53910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5746, token usage: 0.12, #running-req: 1016, #queue-req: 211, 
[2025-11-07 14:19:49] INFO:     127.0.0.1:47770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:48436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:48794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:50440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:51246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:53846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:56386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5014, token usage: 0.12, #running-req: 1017, #queue-req: 204, 
[2025-11-07 14:19:49] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:49106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:49688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:50626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:50916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:53348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:55196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8030, token usage: 0.12, #running-req: 1013, #queue-req: 193, 
[2025-11-07 14:19:49] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:51672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:53782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:54246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:49] INFO:     127.0.0.1:55398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4381, token usage: 0.12, #running-req: 1018, #queue-req: 187, 
[2025-11-07 14:19:50] INFO:     127.0.0.1:47580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:49136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:49152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:49184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:49308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:49878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:50714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:53350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:53432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:53576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:53914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:54362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:54452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:55954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:56568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50 TP0] Prefill batch, #new-seq: 16, #new-token: 16, #cached-token: 11639, token usage: 0.12, #running-req: 1008, #queue-req: 171, 
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP0] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP2] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP3] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP7] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP5] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP1] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP4] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP6] shape M:16, N:3072, K:1536 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP0] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP3] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP2] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP5] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP7] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP1] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP4] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP6] shape M:16, N:4096, K:512 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP0] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP3] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP2] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP5] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP7] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP1] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP4] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP0] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP3] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP2] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP6] shape M:16, N:7168, K:2048 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP0] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP3] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP5] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP2] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP7] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP1] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP4] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP6] shape M:16, N:4608, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP5] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP7] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP1] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP4] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP6] shape M:16, N:7168, K:2304 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP3] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP0] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP2] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP3] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP0] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP1] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP5] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP2] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP4] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP7] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP1] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP5] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP6] shape M:16, N:512, K:7168 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP4] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP7] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[aiter] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50 TP6] shape M:16, N:7168, K:256 is tuned on cu_num = 304 in CKGEMM, kernel name is a8w8_blockscale_1x128x128_256x16x64x256_16x16_16x16_16x16x1_16x16x1_1x16x1x16_4_1x1_intrawave_v1!
[2025-11-07 14:19:50] INFO:     127.0.0.1:48258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:49012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:49252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:49332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:49976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:50062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:55252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6404, token usage: 0.12, #running-req: 1015, #queue-req: 162, 
[2025-11-07 14:19:50] INFO:     127.0.0.1:48594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:49600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:50310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:50652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:53384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:55738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:56292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5057, token usage: 0.12, #running-req: 1017, #queue-req: 155, 
[2025-11-07 14:19:50] INFO:     127.0.0.1:47882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:48132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:49532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:54728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3697, token usage: 0.12, #running-req: 1019, #queue-req: 150, 
[2025-11-07 14:19:50] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:51716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:52050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:54018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:54562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:50] INFO:     127.0.0.1:54594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5198, token usage: 0.12, #running-req: 1017, #queue-req: 143, 
[2025-11-07 14:19:51] INFO:     127.0.0.1:49156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:52066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:52482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:54496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:55906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5025, token usage: 0.12, #running-req: 1017, #queue-req: 136, 
[2025-11-07 14:19:51] INFO:     127.0.0.1:47688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:48296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:48650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:48888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:49626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:50836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:51660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:52272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:52688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:53066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:54290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:54540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:54802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:54816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:55756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:56636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51 TP0] Prefill batch, #new-seq: 19, #new-token: 19, #cached-token: 13968, token usage: 0.12, #running-req: 1005, #queue-req: 117, 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:51 TP4] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:51 TP0] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:51 TP3] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:51 TP5] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:51 TP2] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:51 TP6] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:51 TP7] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:51 TP1] [fused_moe] using default for (19, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:51] INFO:     127.0.0.1:48358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:48638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:52784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:53808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:54416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:55026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:55412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:55688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6650, token usage: 0.12, #running-req: 1015, #queue-req: 108, 
[2025-11-07 14:19:51] INFO:     127.0.0.1:50070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:52856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:53468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:54910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3667, token usage: 0.12, #running-req: 1019, #queue-req: 103, 
[2025-11-07 14:19:51] INFO:     127.0.0.1:47740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:48218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:48388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:48878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:49118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:52764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:51] INFO:     127.0.0.1:55568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8758, token usage: 0.12, #running-req: 1012, #queue-req: 91, 
[2025-11-07 14:19:52] INFO:     127.0.0.1:47862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:48068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:48620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:49134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:49516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:49722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:50224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:50322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:51060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:51872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:51956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:52540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:55498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10118, token usage: 0.12, #running-req: 1010, #queue-req: 77, 
[2025-11-07 14:19:52] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:48698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:48854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:49670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:50030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:50056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:54832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:55342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7411, token usage: 0.12, #running-req: 1014, #queue-req: 67, 
[2025-11-07 14:19:52] INFO:     127.0.0.1:47714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:48006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:48416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:50364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:50846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:52470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:52588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:53156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:53262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:53706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:53766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:54322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:55806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52 TP0] Prefill batch, #new-seq: 14, #new-token: 14, #cached-token: 10408, token usage: 0.12, #running-req: 1010, #queue-req: 53, 
[2025-11-07 14:19:52] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:49666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:51910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:53036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:53162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:53410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:54608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:55528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:56066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9386, token usage: 0.12, #running-req: 1011, #queue-req: 40, 
[2025-11-07 14:19:52] INFO:     127.0.0.1:47734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:48230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:48428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:49290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:49744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:51054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:51212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:52264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:52288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:53078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:53286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:52] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8743, token usage: 0.12, #running-req: 1012, #queue-req: 28, 
[2025-11-07 14:19:53] INFO:     127.0.0.1:47732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:49196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:49396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:50158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:50484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:50530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:50670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:52512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:52796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:53400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:54536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:54786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:55292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:55642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:56640 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP0] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP1] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP4] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP5] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP3] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP7] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP2] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP6] [fused_moe] using default for (1007, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP0] Prefill batch, #new-seq: 17, #new-token: 17, #cached-token: 12352, token usage: 0.12, #running-req: 1007, #queue-req: 11, 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP3] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP0] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP2] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP1] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP5] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP4] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP7] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP6] [fused_moe] using default for (17, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53] INFO:     127.0.0.1:48778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:48948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:50458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:54354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:55932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5143, token usage: 0.13, #running-req: 1017, #queue-req: 4, 
[2025-11-07 14:19:53] INFO:     127.0.0.1:48496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:48532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:49572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:51104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:51938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:52274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:54190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:54694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2890, token usage: 0.13, #running-req: 1013, #queue-req: 0, 
[2025-11-07 14:19:53 TP0] Decode batch, #running-req: 1013, #token: 120991, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5518.89, #queue-req: 0, 
[2025-11-07 14:19:53] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:48998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:50840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:53538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:56496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:50548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:55654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:53] INFO:     127.0.0.1:56656 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP4] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP5] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP2] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP3] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP1] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP0] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP7] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:53 TP6] [fused_moe] using default for (1004, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54] INFO:     127.0.0.1:48030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:49830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:51192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:51572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:52416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP2] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP1] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP4] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP6] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP0] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP5] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP7] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP3] [fused_moe] using default for (997, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54] INFO:     127.0.0.1:49828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:50314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:50566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:50746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:52300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:54036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:54116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:54396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:54680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:54838 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP0] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP4] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP5] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP1] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP2] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP6] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP7] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP3] [fused_moe] using default for (983, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54] INFO:     127.0.0.1:47928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:48162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:48586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:48696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:49242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:49608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:52106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:54106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:56196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:56266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:56430 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP1] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP5] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP4] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP0] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP6] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP7] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP2] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP3] [fused_moe] using default for (966, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54] INFO:     127.0.0.1:47896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:51630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:52090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:52410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55808 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP4] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP1] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP0] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP5] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP7] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP3] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP6] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP2] [fused_moe] using default for (959, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:48760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:49732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:49904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:51236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:51960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:52538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:54386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP1] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP4] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP5] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP0] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP7] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP3] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP6] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP2] [fused_moe] using default for (948, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54] INFO:     127.0.0.1:47694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:48016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:48722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:49778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:50852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:51762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:52532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:47818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:48086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:54944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:56212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:48566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:49288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:50616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:54986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55588 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP5] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP1] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP4] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP0] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP7] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP3] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP2] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP6] [fused_moe] using default for (920, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54] INFO:     127.0.0.1:48366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:48544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:49328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:51226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:54962 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP5] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP1] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP4] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP0] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP3] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP7] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP2] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP6] [fused_moe] using default for (912, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54] INFO:     127.0.0.1:47912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:48080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:49028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:49584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:50858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:50952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:52230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:52296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:53020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:54372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:54] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP4] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP5] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP1] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP0] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP3] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP7] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP2] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:54 TP6] [fused_moe] using default for (898, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55] INFO:     127.0.0.1:47620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:47634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:47874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:48526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:48642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:48668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:48764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:49214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:51168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:51922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:54634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:54928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:56762 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP4] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP1] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP5] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP0] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP3] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP7] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP2] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP6] [fused_moe] using default for (884, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55] INFO:     127.0.0.1:48714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:48756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:49236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:50180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:50768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:53608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:48046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:48584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:48646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:49454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:49792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:50378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:51138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:53760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:54066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:55920 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP1] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP4] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP5] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP0] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP3] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP7] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP2] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP6] [fused_moe] using default for (861, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55] INFO:     127.0.0.1:47678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:47918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:48100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:49844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:50560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:50922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:51678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:53478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:53940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:54308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:54392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:54754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:56560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:47958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:49656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:49870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:53862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:55526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:56190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP1] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP3] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP7] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP4] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP5] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP2] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP0] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP6] [fused_moe] using default for (834, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55] INFO:     127.0.0.1:48534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:55414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:55562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:55660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP1] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP4] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP3] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP0] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP5] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP7] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP2] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP6] [fused_moe] using default for (826, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55] INFO:     127.0.0.1:47800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:48228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:49632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:50744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:53552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:53928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:54092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:56270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:56328 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP1] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP4] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP5] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP0] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP3] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP7] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP2] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP6] [fused_moe] using default for (814, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:49934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:51558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:51602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:54414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:57080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:49700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:51190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:51286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:51326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:51642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:54342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:48724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:54632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:55856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:55] INFO:     127.0.0.1:56004 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP4] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP5] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP3] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP1] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP0] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP7] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP2] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:55 TP6] [fused_moe] using default for (789, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56] INFO:     127.0.0.1:47606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:48560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:50504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:52686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:52904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:55766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP1] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP4] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP5] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP2] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP0] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP3] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP6] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP7] [fused_moe] using default for (778, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56] INFO:     127.0.0.1:47780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:52616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:53420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:54890 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP1] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP4] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP5] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP0] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP2] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP6] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP3] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP7] [fused_moe] using default for (770, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56] INFO:     127.0.0.1:48514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:52026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:52218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:52498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:54476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:54548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:54994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:57802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:47726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:52578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:53130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:53898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:52458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:54952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:48944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:53336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:53876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:54844 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP1] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP0] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP4] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP5] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP3] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP2] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP7] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP6] [fused_moe] using default for (739, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56] INFO:     127.0.0.1:48266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:49618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:50874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:54224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:55368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:47648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:47840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:48582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:50730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:53142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:54002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:55220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:57492 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP4] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP1] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP5] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP0] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP3] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP2] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP7] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP6] [fused_moe] using default for (711, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56] INFO:     127.0.0.1:47792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:52018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:53674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:55626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:57742 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP4] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP5] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP0] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP1] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP2] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP6] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP3] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP7] [fused_moe] using default for (702, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56] INFO:     127.0.0.1:47982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:48700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:49974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:50866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:51494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:53736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:53906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:54080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:55070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:56] INFO:     127.0.0.1:56962 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP4] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP5] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP0] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP1] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP2] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP6] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP3] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:56 TP7] [fused_moe] using default for (688, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57] INFO:     127.0.0.1:53758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:55230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:56450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:56888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:57484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP4] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP5] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP0] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP1] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP2] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP6] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP3] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP7] [fused_moe] using default for (677, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57] INFO:     127.0.0.1:48194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:51026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:51170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:52520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:55720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:57780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP4] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP5] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP1] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP0] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP2] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP3] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP6] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP7] [fused_moe] using default for (667, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57] INFO:     127.0.0.1:47724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:48250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:48336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:49044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:50342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:58072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP4] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP5] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP1] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP0] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP2] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP6] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP3] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP7] [fused_moe] using default for (657, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57] INFO:     127.0.0.1:48644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:49400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:52916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:54132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:56146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:56710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:57486 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP4] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP5] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP0] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP1] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP2] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP6] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP3] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP7] [fused_moe] using default for (647, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:49462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:52234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:55938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:57916 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP4] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP5] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP0] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP1] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP2] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP6] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP3] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP7] [fused_moe] using default for (640, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:48324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:49984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:50424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:50500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:52818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:52958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:53414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:55790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:59030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:48448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:49082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:50010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:50754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:51012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:51508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:52802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:55940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:56062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:56670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:57206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:58318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP4] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP5] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP1] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP0] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP2] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP6] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP3] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP7] [fused_moe] using default for (611, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:51970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:54326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:55842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:56548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP4] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP5] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP0] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP1] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP2] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP6] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP3] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP7] [fused_moe] using default for (603, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP0] Decode batch, #running-req: 611, #token: 92630, token usage: 0.10, cuda graph: False, gen throughput (token/s): 8035.90, #queue-req: 0, 
[2025-11-07 14:19:57] INFO:     127.0.0.1:47798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:50910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:50974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:51428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:54534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:57376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:58320 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP4] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP5] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP0] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP1] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP2] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP6] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP3] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57 TP7] [fused_moe] using default for (593, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:57] INFO:     127.0.0.1:47684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:47846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:51888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:52808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:53354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:53446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:54428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:57] INFO:     127.0.0.1:54482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:49296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:49576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:51588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:52366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:53320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:54976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:56476 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP4] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP5] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP0] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP1] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP6] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP2] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP7] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP3] [fused_moe] using default for (575, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58] INFO:     127.0.0.1:47664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:49924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:51542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:53988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:54912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:55138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:58954 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP4] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP5] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP0] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP1] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP2] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP6] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP3] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP7] [fused_moe] using default for (560, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58] INFO:     127.0.0.1:48726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:49098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:51778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:52976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:53224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:53300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:56822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:58546 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP4] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP5] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP0] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP1] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP2] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP6] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP7] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP3] [fused_moe] using default for (551, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58] INFO:     127.0.0.1:48626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:55770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:56330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP1] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP3] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP2] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP0] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP4] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP5] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP7] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP6] [fused_moe] using default for (543, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58] INFO:     127.0.0.1:48304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:48740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:53980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:47938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:49176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:49996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:54398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:55698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:56692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57524 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP1] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP0] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP4] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP5] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP3] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP2] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP7] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58 TP6] [fused_moe] using default for (521, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:19:58] INFO:     127.0.0.1:49230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:53688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:54812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:56174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:49164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:52336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:53012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:53122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:54886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:47974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:48982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:49858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:56764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:51400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:51532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:53456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:54956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:55180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:55716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:56912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:58302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:58584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:47632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:49368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:51438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:55020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:55032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:52242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:52792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:54156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:49908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:51072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:52650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:55008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:55106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:55162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:58] INFO:     127.0.0.1:57610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:50098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:51002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:54860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:55490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:49206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:50208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:56408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:47586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:48050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:49008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:53626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:53950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:59180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:49392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:51342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:56528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:53504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:53744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:54906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:56188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:48234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:48352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:50656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:51464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:51740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:59046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:51200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:52144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:52988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:53060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:53240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:56078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:59168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:52382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:52694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:53176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:53640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:55870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:56722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:48796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:51024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:53102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:54714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:55742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:50410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:50780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:51088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:52864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:53972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:54260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:54568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:55616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:49892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:54656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:56010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:52730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:54892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:59070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:59162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:49158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:51556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:54050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:56906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:47700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:56790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:19:59] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:49558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:52542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:54216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:55950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:51706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:48484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:47756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:48444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:50678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:52132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:48912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:49586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:59270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:49512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:49566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:59284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:48968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:59226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:47832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:49542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:51068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:59276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:59006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00 TP0] Decode batch, #running-req: 270, #token: 50313, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6030.62, #queue-req: 0, 
[2025-11-07 14:20:00] INFO:     127.0.0.1:48892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:49260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:49704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:53560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:47608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:48682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:51368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:59242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:48368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:51306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:55968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:59104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:49476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:49350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:49408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:50192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:50900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:47992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:50426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:53962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:57846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:59114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:59212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:48994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:53092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:56246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:50262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:50582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:00] INFO:     127.0.0.1:58912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:52632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:53316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:54194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:48378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:52634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:56040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:59012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:52428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:56366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:48186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:50818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:48350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:48838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:51242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:51300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:51560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:53208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:54300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:56512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:47622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:53958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:54908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:51432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:56828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:52942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:54766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:48812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:49130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:49312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:51146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:54674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:48406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:52572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:59138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:50278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:59002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:52850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:57692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:01] INFO:     127.0.0.1:58988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:49504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:58342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:58742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:58836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:52748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:58350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:58622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:59078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:56878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:56904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:58634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:58168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:56486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:58136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:59208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02 TP0] Decode batch, #running-req: 106, #token: 25381, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3642.40, #queue-req: 0, 
[2025-11-07 14:20:02] INFO:     127.0.0.1:52600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:55378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:50524 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:58638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:48156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:49062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:50328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:55996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:56382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:50066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:52444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:54240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:55880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:48930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:58372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:58726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:52194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:54710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:56848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:02] INFO:     127.0.0.1:59090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:51348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:54740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:47738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:57930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:50144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:54620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:57704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:52720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:57152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:57382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:55786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:57510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:47834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:51812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:58806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:59256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:59278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:56872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:52822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03 TP0] Decode batch, #running-req: 31, #token: 9333, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1728.79, #queue-req: 0, 
[2025-11-07 14:20:03] INFO:     127.0.0.1:55820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:03] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:58238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:49000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:58202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:53924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:51014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:57308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:56298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:59150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:48138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:57898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:51690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:59126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:50998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:57238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:51316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:57362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:57546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04] INFO:     127.0.0.1:54444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:04 TP0] Decode batch, #running-req: 9, #token: 3591, token usage: 0.00, cuda graph: True, gen throughput (token/s): 704.72, #queue-req: 0, 
[2025-11-07 14:20:05] INFO:     127.0.0.1:58818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:05] INFO:     127.0.0.1:59016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:05] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:05] INFO:     127.0.0.1:57128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:05] INFO:     127.0.0.1:58194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:05] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:05 TP0] Decode batch, #running-req: 3, #token: 1665, token usage: 0.00, cuda graph: True, gen throughput (token/s): 225.18, #queue-req: 0, 
[2025-11-07 14:20:05] INFO:     127.0.0.1:56752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:05] INFO:     127.0.0.1:58044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:05] INFO:     127.0.0.1:56798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:18] INFO:     127.0.0.1:50170 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-07 14:20:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:20:18] INFO:     127.0.0.1:50180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:20:18 TP0] Prefill batch, #new-seq: 37, #new-token: 37, #cached-token: 26856, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:18 TP0] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:18 TP3] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:18 TP2] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:18 TP1] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:18 TP6] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:18 TP4] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:18 TP5] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:18 TP7] [fused_moe] using default for (37, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 34952, token usage: 0.01, #running-req: 38, #queue-req: 0, 
[2025-11-07 14:20:19 TP0] Prefill batch, #new-seq: 50, #new-token: 50, #cached-token: 36329, token usage: 0.01, #running-req: 86, #queue-req: 0, 
[2025-11-07 14:20:19 TP0] Prefill batch, #new-seq: 59, #new-token: 59, #cached-token: 42963, token usage: 0.01, #running-req: 136, #queue-req: 0, 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP1] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP3] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP0] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP2] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP4] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP7] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP5] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP6] [fused_moe] using default for (59, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP0] Prefill batch, #new-seq: 57, #new-token: 57, #cached-token: 41665, token usage: 0.02, #running-req: 195, #queue-req: 0, 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP2] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP3] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP0] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP5] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP1] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP4] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP6] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP7] [fused_moe] using default for (57, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP0] Prefill batch, #new-seq: 65, #new-token: 65, #cached-token: 47249, token usage: 0.02, #running-req: 252, #queue-req: 0, 
[2025-11-07 14:20:19 TP0] Prefill batch, #new-seq: 63, #new-token: 63, #cached-token: 45885, token usage: 0.02, #running-req: 317, #queue-req: 0, 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP2] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP1] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP3] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP0] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP5] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP4] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP6] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP7] [fused_moe] using default for (63, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP0] Prefill batch, #new-seq: 71, #new-token: 71, #cached-token: 51935, token usage: 0.03, #running-req: 380, #queue-req: 0, 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP2] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP3] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP1] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP4] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP5] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP0] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP7] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP6] [fused_moe] using default for (71, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP0] Prefill batch, #new-seq: 70, #new-token: 70, #cached-token: 50628, token usage: 0.03, #running-req: 451, #queue-req: 0, 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP3] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP0] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP1] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP2] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP5] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP7] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP4] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:19 TP6] [fused_moe] using default for (70, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP0] Prefill batch, #new-seq: 79, #new-token: 79, #cached-token: 57441, token usage: 0.04, #running-req: 521, #queue-req: 0, 
[2025-11-07 14:20:20 TP0] Prefill batch, #new-seq: 75, #new-token: 75, #cached-token: 54543, token usage: 0.04, #running-req: 600, #queue-req: 0, 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP3] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP1] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP0] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP4] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP5] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP7] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP2] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP6] [fused_moe] using default for (75, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP0] Prefill batch, #new-seq: 90, #new-token: 90, #cached-token: 65523, token usage: 0.05, #running-req: 675, #queue-req: 0, 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP2] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP1] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP3] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP0] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP4] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP5] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP7] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP6] [fused_moe] using default for (90, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1416, token usage: 0.05, #running-req: 765, #queue-req: 0, 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP0] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP1] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP4] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP2] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP5] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP3] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP6] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP7] [fused_moe] using default for (767, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP0] Prefill batch, #new-seq: 43, #new-token: 43, #cached-token: 31435, token usage: 0.05, #running-req: 767, #queue-req: 0, 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP0] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP4] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP2] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP5] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP6] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP3] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP1] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP7] [fused_moe] using default for (43, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:20 TP0] Prefill batch, #new-seq: 57, #new-token: 57, #cached-token: 41566, token usage: 0.05, #running-req: 810, #queue-req: 0, 
[2025-11-07 14:20:20 TP0] Prefill batch, #new-seq: 52, #new-token: 52, #cached-token: 37591, token usage: 0.06, #running-req: 867, #queue-req: 0, 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:21 TP2] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:21 TP0] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:21 TP3] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:21 TP1] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:21 TP5] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:21 TP4] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:21 TP6] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:21 TP7] [fused_moe] using default for (52, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:21 TP0] Prefill batch, #new-seq: 67, #new-token: 67, #cached-token: 49226, token usage: 0.06, #running-req: 919, #queue-req: 0, 
[2025-11-07 14:20:21 TP0] Prefill batch, #new-seq: 38, #new-token: 38, #cached-token: 27983, token usage: 0.06, #running-req: 986, #queue-req: 19, 
[2025-11-07 14:20:24] INFO:     127.0.0.1:53026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 701, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-07 14:20:24 TP0] Decode batch, #running-req: 1024, #token: 94209, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1679.53, #queue-req: 294, 
[2025-11-07 14:20:24] INFO:     127.0.0.1:50614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:24] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1496, token usage: 0.10, #running-req: 1022, #queue-req: 292, 
[2025-11-07 14:20:24] INFO:     127.0.0.1:53080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 732, token usage: 0.10, #running-req: 1023, #queue-req: 291, 
[2025-11-07 14:20:25] INFO:     127.0.0.1:50216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25] INFO:     127.0.0.1:50506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25] INFO:     127.0.0.1:51364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25] INFO:     127.0.0.1:54814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25] INFO:     127.0.0.1:55778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1443, token usage: 0.11, #running-req: 1022, #queue-req: 289, 
[2025-11-07 14:20:25] INFO:     127.0.0.1:51332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25] INFO:     127.0.0.1:52536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25] INFO:     127.0.0.1:52938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25] INFO:     127.0.0.1:53738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25] INFO:     127.0.0.1:55758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5906, token usage: 0.11, #running-req: 1016, #queue-req: 281, 
[2025-11-07 14:20:25] INFO:     127.0.0.1:50422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25] INFO:     127.0.0.1:53062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25] INFO:     127.0.0.1:54510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25] INFO:     127.0.0.1:54706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:25] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4385, token usage: 0.11, #running-req: 1018, #queue-req: 275, 
[2025-11-07 14:20:26] INFO:     127.0.0.1:50742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:50864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:52602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:54562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:54948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:55032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:55308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5819, token usage: 0.11, #running-req: 1016, #queue-req: 267, 
[2025-11-07 14:20:26] INFO:     127.0.0.1:51696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:55158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:56036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:57324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2896, token usage: 0.11, #running-req: 1020, #queue-req: 263, 
[2025-11-07 14:20:26] INFO:     127.0.0.1:51032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:53900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:58730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:58874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3592, token usage: 0.11, #running-req: 1019, #queue-req: 258, 
[2025-11-07 14:20:26] INFO:     127.0.0.1:53024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:53458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:54420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:58016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2866, token usage: 0.11, #running-req: 1020, #queue-req: 254, 
[2025-11-07 14:20:26] INFO:     127.0.0.1:50444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:51430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:52460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:54590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:54930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:58280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:26] INFO:     127.0.0.1:58756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5872, token usage: 0.11, #running-req: 1016, #queue-req: 246, 
[2025-11-07 14:20:27] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:52344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:56316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:57812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:58172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:58808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5229, token usage: 0.11, #running-req: 1017, #queue-req: 239, 
[2025-11-07 14:20:27] INFO:     127.0.0.1:51180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:51676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:53568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:56360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:57664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:58994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5062, token usage: 0.11, #running-req: 1017, #queue-req: 232, 
[2025-11-07 14:20:27] INFO:     127.0.0.1:50562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:51184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:52358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:54410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:54692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:55670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:55794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:57172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:57648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7367, token usage: 0.12, #running-req: 1014, #queue-req: 222, 
[2025-11-07 14:20:27] INFO:     127.0.0.1:52774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:54304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:55244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:55538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:56294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5094, token usage: 0.12, #running-req: 1017, #queue-req: 215, 
[2025-11-07 14:20:27] INFO:     127.0.0.1:50998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:51040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:51560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:53248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:54072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:55320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:27] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5715, token usage: 0.12, #running-req: 1016, #queue-req: 207, 
[2025-11-07 14:20:28] INFO:     127.0.0.1:50984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:51604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:51756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:53702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:54240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:55834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:55948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:56498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:57968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:58144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7217, token usage: 0.12, #running-req: 1014, #queue-req: 197, 
[2025-11-07 14:20:28] INFO:     127.0.0.1:51116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:51764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:54536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:56184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:56372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5108, token usage: 0.12, #running-req: 1017, #queue-req: 190, 
[2025-11-07 14:20:28] INFO:     127.0.0.1:50184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:52086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:52154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:53514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:54520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:56064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4389, token usage: 0.12, #running-req: 1018, #queue-req: 184, 
[2025-11-07 14:20:28] INFO:     127.0.0.1:50414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:51574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:51884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:52128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:52554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:52796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:55920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:58362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:58946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:59086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8694, token usage: 0.12, #running-req: 1012, #queue-req: 172, 
[2025-11-07 14:20:28] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:52612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:52632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:53142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:55956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:55996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:56766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:57982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:28] INFO:     127.0.0.1:59042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7168, token usage: 0.12, #running-req: 1014, #queue-req: 162, 
[2025-11-07 14:20:29] INFO:     127.0.0.1:50408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:50648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:52262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:54542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:55676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:56136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:57830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5057, token usage: 0.12, #running-req: 1017, #queue-req: 155, 
[2025-11-07 14:20:29] INFO:     127.0.0.1:53628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:54900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:56428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:56634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:56656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:58204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4404, token usage: 0.12, #running-req: 1018, #queue-req: 149, 
[2025-11-07 14:20:29] INFO:     127.0.0.1:52062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:52274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:53166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:54414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:55134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:57184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:57500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:59014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6646, token usage: 0.12, #running-req: 1015, #queue-req: 140, 
[2025-11-07 14:20:29] INFO:     127.0.0.1:50318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:51188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:51294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:51678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:51842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:52500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:54516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:57154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:57760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:58320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8696, token usage: 0.12, #running-req: 1012, #queue-req: 128, 
[2025-11-07 14:20:29] INFO:     127.0.0.1:50752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:51310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:51518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:53644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:55402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:56292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:56378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:29] INFO:     127.0.0.1:57404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5897, token usage: 0.12, #running-req: 1016, #queue-req: 120, 
[2025-11-07 14:20:30] INFO:     127.0.0.1:50394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:56474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:56922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:56936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:58060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5903, token usage: 0.12, #running-req: 1016, #queue-req: 112, 
[2025-11-07 14:20:30] INFO:     127.0.0.1:50526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:50818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:51490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:51946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:53182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:53210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:55378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:55480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:56846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:57122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:58090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9640, token usage: 0.12, #running-req: 1011, #queue-req: 99, 
[2025-11-07 14:20:30] INFO:     127.0.0.1:50638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:51090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:51882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:52616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:53048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:53844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:54624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:54688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:56992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:58106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10809, token usage: 0.12, #running-req: 1009, #queue-req: 84, 
[2025-11-07 14:20:30] INFO:     127.0.0.1:50932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:51016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:51334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:55464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:55582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:55772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:58412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7282, token usage: 0.12, #running-req: 1014, #queue-req: 74, 
[2025-11-07 14:20:30] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:50772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:51800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:52548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:52844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:53196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:55024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:55826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:56354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:56890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:30] INFO:     127.0.0.1:58520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8137, token usage: 0.12, #running-req: 1013, #queue-req: 63, 
[2025-11-07 14:20:31] INFO:     127.0.0.1:50388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:51110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:53832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:54738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:55646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:55734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:57422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:58084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:58244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8915, token usage: 0.13, #running-req: 1012, #queue-req: 51, 
[2025-11-07 14:20:31] INFO:     127.0.0.1:51236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:51388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:52194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:52258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:53910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:54042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:54556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:55674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:55868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:57050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:57208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:57814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:57862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:57874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:58432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:59074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31 TP0] Prefill batch, #new-seq: 17, #new-token: 17, #cached-token: 12336, token usage: 0.13, #running-req: 1007, #queue-req: 34, 
[2025-11-07 14:20:31] INFO:     127.0.0.1:52580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:52920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:53342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:53486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:55176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:55422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:55522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:56006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:56692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:57300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8725, token usage: 0.13, #running-req: 1012, #queue-req: 22, 
[2025-11-07 14:20:31 TP0] Decode batch, #running-req: 1012, #token: 120559, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5671.79, #queue-req: 22, 
[2025-11-07 14:20:31] INFO:     127.0.0.1:50712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:51340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:53268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:53500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:53608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:54138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:56512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:56910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:57098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:57364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:58670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8084, token usage: 0.13, #running-req: 1013, #queue-req: 11, 
[2025-11-07 14:20:31] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:52452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:53284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:54080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:54176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:54868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:56198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:31] INFO:     127.0.0.1:56750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5777, token usage: 0.13, #running-req: 1016, #queue-req: 3, 
[2025-11-07 14:20:32] INFO:     127.0.0.1:50924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:51142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:51752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:52354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:53620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:54318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:56152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:57264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:57920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:58370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:58534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:59194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 2163, token usage: 0.13, #running-req: 1012, #queue-req: 0, 
[2025-11-07 14:20:32] INFO:     127.0.0.1:50362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:51000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:51228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:53380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:54802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:55840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:56174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:56620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:51590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:54006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:54394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:54628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:54774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:56584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:58718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:58844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:59088 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP5] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP4] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP1] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP6] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP7] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP0] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP2] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP3] [fused_moe] using default for (998, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32] INFO:     127.0.0.1:52210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:54576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:55212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:56092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:56858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:58288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:58348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:58926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:59028 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP1] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP4] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP6] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP0] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP3] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP5] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP7] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP2] [fused_moe] using default for (987, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32] INFO:     127.0.0.1:50366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:50726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:51170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:51252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:51568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:51984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:52298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:52636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:54464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:56960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:59326 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP0] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP5] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP7] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP2] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP4] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP3] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP1] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP6] [fused_moe] using default for (972, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32] INFO:     127.0.0.1:50946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:51714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:53550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:55058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:55098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:55144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:55184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:58578 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP0] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP5] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP2] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP6] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP3] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP1] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP7] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP4] [fused_moe] using default for (962, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32] INFO:     127.0.0.1:50498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:50594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:51456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:54034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:55866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:32] INFO:     127.0.0.1:58258 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP5] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP0] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP2] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP6] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP4] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP7] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP3] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:32 TP1] [fused_moe] using default for (956, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33] INFO:     127.0.0.1:50340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:50822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:50876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:51404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:51440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:54964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:59236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:59318 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP4] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP5] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP0] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP6] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP2] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP1] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP3] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP7] [fused_moe] using default for (942, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33] INFO:     127.0.0.1:50572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:51918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:55204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:55336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:56160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:56194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:56456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:58178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:58196 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP0] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP5] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP2] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP6] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP4] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP1] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP3] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP7] [fused_moe] using default for (928, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33] INFO:     127.0.0.1:51064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:51642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:51738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:54722 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:55636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP6] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP0] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP2] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP4] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP5] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP1] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP3] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP7] [fused_moe] using default for (921, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:51322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:54052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:54644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:56250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:56392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:58306 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP2] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP5] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP4] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP6] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP0] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP1] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP7] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP3] [fused_moe] using default for (910, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33] INFO:     127.0.0.1:51380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:54832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:55628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:58854 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP0] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP4] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP2] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP5] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP6] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP1] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP3] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP7] [fused_moe] using default for (899, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33] INFO:     127.0.0.1:50240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:50276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:50586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:51536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:54266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:54448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:54822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:54848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:55140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57156 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP4] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP2] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP6] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP0] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP5] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP1] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP7] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP3] [fused_moe] using default for (883, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33] INFO:     127.0.0.1:51438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:51654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:54022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:54462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:55390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:55482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:58230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:58532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:59206 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP5] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP2] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP6] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP4] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP0] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP1] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP3] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP7] [fused_moe] using default for (866, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33] INFO:     127.0.0.1:50678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:51830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:58890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP4] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP2] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP6] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP0] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP5] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP1] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP3] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP7] [fused_moe] using default for (858, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:50762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:55022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:55172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:56878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:56938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:58826 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP4] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP2] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP6] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP0] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP5] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP1] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP7] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP3] [fused_moe] using default for (841, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:52512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:53220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:57344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:58156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:33] INFO:     127.0.0.1:58492 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP4] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP6] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP2] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP0] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP5] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP1] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP7] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:33 TP3] [fused_moe] using default for (833, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34] INFO:     127.0.0.1:51746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:52444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:56898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:57196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:58132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:58378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:58470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:58508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:59038 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP4] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP2] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP6] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP5] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP0] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP1] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP3] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP7] [fused_moe] using default for (821, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34] INFO:     127.0.0.1:50900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:52698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:54196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:54754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:55234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:56440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:56682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:57844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:50228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:51128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:54358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:55326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:58500 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP4] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP6] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP2] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP5] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP0] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP1] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP3] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP7] [fused_moe] using default for (805, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34] INFO:     127.0.0.1:51550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:52374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:54104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:58048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:58640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:59748 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP2] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP4] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP6] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP0] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP5] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP1] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP3] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP7] [fused_moe] using default for (796, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34] INFO:     127.0.0.1:51540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:52356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:55416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:57592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:57790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:58938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:51476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:53440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:53926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:54086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:54504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:55406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:59382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:50698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:51216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:54096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:54252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:55274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:55692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:56052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:56442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:59228 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP4] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP5] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP6] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP0] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP2] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP1] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP7] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP3] [fused_moe] using default for (769, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34] INFO:     127.0.0.1:54488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:55798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:56510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:57476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:58646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:51292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:55724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:56466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:58152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:58916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:60452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:60472 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP4] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP6] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP2] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP5] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP0] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP1] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP3] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP7] [fused_moe] using default for (757, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP4] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP2] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP0] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP5] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP6] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP1] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP7] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34 TP3] [fused_moe] using default for (753, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:34] INFO:     127.0.0.1:54438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:55042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:56646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:34] INFO:     127.0.0.1:59912 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP0] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP4] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP5] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP2] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP6] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP1] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35] INFO:     127.0.0.1:50720 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP3] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35] INFO:     127.0.0.1:51324 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP7] [fused_moe] using default for (743, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35] INFO:     127.0.0.1:52322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:54684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:55972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:58796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:59006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:50238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:53922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:55576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:56800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:59434 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP4] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP2] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP6] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP0] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP5] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP1] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP3] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP7] [fused_moe] using default for (735, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:50724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:50958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:52786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:52958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:53374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:53956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:54280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:56614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:58006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:58218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:58556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:58814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:58976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:59282 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP2] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP4] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP6] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP0] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP5] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP1] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP3] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP7] [fused_moe] using default for (718, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35] INFO:     127.0.0.1:50478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:51266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:52826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:56818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:59190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:59220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:59294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:60012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:60146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP2] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP4] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP6] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP0] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP5] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP1] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP3] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP7] [fused_moe] using default for (705, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35] INFO:     127.0.0.1:50540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:53674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:53806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:54606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:54762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:54980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:55994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:56458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:56672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:58668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:59590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:59840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:60400 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP4] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP2] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP6] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP0] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP5] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP1] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP3] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP7] [fused_moe] using default for (690, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35] INFO:     127.0.0.1:50282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:51754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:51838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:52778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:54798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:55710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:56342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:58342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:59050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:59514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP4] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP2] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP6] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP0] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP5] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP1] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP3] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP7] [fused_moe] using default for (675, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35] INFO:     127.0.0.1:53084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:54732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:55200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:55304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:55450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:55526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:58744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:60630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:33334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:51220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:52828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:52944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:53188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:54932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:60438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:60764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:51248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:51570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:52916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:56310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:56790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:58452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:58666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:59096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:50792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:51618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:52406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:55706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:56716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:57348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:58422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:59060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:35] INFO:     127.0.0.1:60140 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP4] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP6] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP2] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP0] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP5] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP1] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP3] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:35 TP7] [fused_moe] using default for (635, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36] INFO:     127.0.0.1:50326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:56022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:58550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:60594 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP4] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP6] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP2] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP0] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP5] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP1] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP7] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP3] [fused_moe] using default for (628, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP0] Decode batch, #running-req: 635, #token: 96149, token usage: 0.10, cuda graph: False, gen throughput (token/s): 7646.45, #queue-req: 0, 
[2025-11-07 14:20:36] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:55970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP4] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP2] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP6] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP0] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP5] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP1] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP3] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP7] [fused_moe] using default for (624, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36] INFO:     127.0.0.1:53590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:55432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:56892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:58790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:60038 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP4] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP6] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP2] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP0] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP5] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP1] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP3] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP7] [fused_moe] using default for (618, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36] INFO:     127.0.0.1:50872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:53700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:53778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:54194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:54224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:56564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:56946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:58024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:58682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:58690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:58762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:60314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP4] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP2] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP6] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP0] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP5] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP1] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP3] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP7] [fused_moe] using default for (605, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36] INFO:     127.0.0.1:50510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:54340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:54640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:55094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:57106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59978 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP4] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP6] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP2] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP0] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP5] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP1] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP3] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP7] [fused_moe] using default for (592, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36] INFO:     127.0.0.1:50298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:50610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:51734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:51870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:54616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:55900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:56598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:57008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:57706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:58988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:60584 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP4] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP2] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP6] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP0] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP5] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP1] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP3] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP7] [fused_moe] using default for (576, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36] INFO:     127.0.0.1:51998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:53444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:53686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:55302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:57578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:57658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:60992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:33394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:55498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:55584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:57744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:58404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:59968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:60150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:32976 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP4] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP6] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP2] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP5] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP0] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP1] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP7] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP3] [fused_moe] using default for (547, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36] INFO:     127.0.0.1:53034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:55620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:58804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:53228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:54658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:55112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:60132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:60198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:60532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:32890 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP4] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP6] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP2] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP5] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP0] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP1] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP7] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36 TP3] [fused_moe] using default for (533, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:36] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:51094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:52956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:54288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:56264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:57134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:60396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:36] INFO:     127.0.0.1:60708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:52832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:53264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:54244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:56394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:56420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:56964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:60080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:50270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:51066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:52196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:52594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:54332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:54836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:56078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:57538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:51890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:55026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:56336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:57458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:58330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:58774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:60032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:60448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:60502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:33354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:51526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:51934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:52526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:55914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:58296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:60850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:52986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:57710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:57734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:60234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:60568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:53592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:54250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:56104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:56120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:57384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:32942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:53886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:54100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:54870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:54910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:60810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:33008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:52478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:52930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:53830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:56162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:57602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:57682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:60222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:52440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:52694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:55084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:57442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:60176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:33358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:50212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:50628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:56706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:32994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:50662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:52164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:53322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:53532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:53618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:54154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:55512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:55820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:56942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:58834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:33212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:33276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:53558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:54924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:55332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:58264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:33650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:51908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:53424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:53476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:57494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:57620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:57858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:33474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:51784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:53718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:54976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:58120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:59594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:60488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:60778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:60820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:32862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:33128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:37] INFO:     127.0.0.1:33164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:55688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:56786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:58704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:32978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:52998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:54898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:55228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:56578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:56774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:56834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:57048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:57290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:57850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:58366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:51624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:56248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:58660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:32926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:51814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:53870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:55350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:51080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:52248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:57478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:58486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:58564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:50348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:51202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:51776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:50838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:54342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:58190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:53408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:58596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:58900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:58958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:32768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:32996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:52758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:50376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:50642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:52240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:52576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:53678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:53388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:55598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:58456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:51048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:52176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:52544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:55012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:56208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:56240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:60468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:53234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:56178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:59944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:33732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:50892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:51980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:38] INFO:     127.0.0.1:57642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39 TP0] Decode batch, #running-req: 287, #token: 53355, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6141.43, #queue-req: 0, 
[2025-11-07 14:20:39] INFO:     127.0.0.1:51472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:51586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:59368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:59588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:50256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:56284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:59712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:53006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:53498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:54110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:56260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:59616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:59776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:32966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:51772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:52120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:53310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:58480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:59602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:32778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:50262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:51280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:52696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:53696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:57316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:59084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:50536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:51416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:57244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:59106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:58858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:54258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:53798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:56526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:56542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:50550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:51178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:55990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:52148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:54514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:55560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:59256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:52734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:51158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:54056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:55472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:59632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:32950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:33432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:56556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:39] INFO:     127.0.0.1:60662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:50356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:59826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:33102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:50294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:52394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:53366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:54908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:54996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:55884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:58146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:32984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:53218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:50972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:55366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:51356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:55248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:57254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:59704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:53942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:59900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:33374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:53100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:59916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:33418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:33598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:51062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:52682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:54366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:51900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:52488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:32818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:33400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:58074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:51858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:57490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:59662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:59762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:32794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:55372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:59398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:33272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:59690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:59736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:32810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:33030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:33482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:52042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:55814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:59924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:32954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:55660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:33058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:51580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40 TP0] Decode batch, #running-req: 112, #token: 26473, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3886.73, #queue-req: 0, 
[2025-11-07 14:20:40] INFO:     127.0.0.1:55454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:57932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:60944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:50466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:40] INFO:     127.0.0.1:59056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:52856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:55280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:59720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:60380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:33670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:50514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:56224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:60692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:60826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:54184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:59408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:59446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:32916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:33556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:50302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:51956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:59308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:51684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:59804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:53764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:60828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:54148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:57148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:57274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:58618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:33182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:32826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:51966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:33480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:33634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:60702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:57338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:60336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:53160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:60622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:33086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:33188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:55340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:60816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:32900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:59136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:59522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:60046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:41] INFO:     127.0.0.1:53790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:59806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:56408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:60930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:33246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:60142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:60762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:33232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:58974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:60704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:60980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:54776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:59180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:33704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:57472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:59330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42 TP0] Decode batch, #running-req: 38, #token: 11308, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1886.89, #queue-req: 0, 
[2025-11-07 14:20:42] INFO:     127.0.0.1:33532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:58436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:50494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:56488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:60916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:56086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:59352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:59498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:33070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:33116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:59960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:33614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:53814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:33452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:33022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:42] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:51700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:52974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:33582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:60538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:60170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:33726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:52892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:59852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:58392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:60022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:60646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43 TP0] Decode batch, #running-req: 10, #token: 3958, token usage: 0.00, cuda graph: True, gen throughput (token/s): 826.02, #queue-req: 0, 
[2025-11-07 14:20:43] INFO:     127.0.0.1:57358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:33260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:60618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:57026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:60872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:59152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:43] INFO:     127.0.0.1:59422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:44 TP0] Decode batch, #running-req: 2, #token: 1274, token usage: 0.00, cuda graph: True, gen throughput (token/s): 216.04, #queue-req: 0, 
[2025-11-07 14:20:44] INFO:     127.0.0.1:60236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:44] INFO:     127.0.0.1:60744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:57] INFO:     127.0.0.1:46450 - "GET /get_model_info HTTP/1.1" 200 OK
[2025-11-07 14:20:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 666, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:20:57] INFO:     127.0.0.1:46458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:20:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 733, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:20:57 TP0] Prefill batch, #new-seq: 41, #new-token: 41, #cached-token: 29782, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:57 TP0] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:57 TP2] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:57 TP3] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:57 TP4] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:57 TP5] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:57 TP1] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:57 TP6] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:57 TP7] [fused_moe] using default for (41, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:57 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 34880, token usage: 0.01, #running-req: 42, #queue-req: 0, 
[2025-11-07 14:20:57 TP0] Prefill batch, #new-seq: 50, #new-token: 50, #cached-token: 36420, token usage: 0.01, #running-req: 90, #queue-req: 0, 
[2025-11-07 14:20:57 TP0] Prefill batch, #new-seq: 53, #new-token: 53, #cached-token: 38815, token usage: 0.01, #running-req: 140, #queue-req: 0, 
[2025-11-07 14:20:57 TP0] Prefill batch, #new-seq: 56, #new-token: 56, #cached-token: 40671, token usage: 0.02, #running-req: 193, #queue-req: 0, 
[2025-11-07 14:20:58 TP0] Prefill batch, #new-seq: 60, #new-token: 60, #cached-token: 43632, token usage: 0.02, #running-req: 249, #queue-req: 0, 
[2025-11-07 14:20:58 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44445, token usage: 0.02, #running-req: 309, #queue-req: 0, 
[2025-11-07 14:20:58 TP0] Prefill batch, #new-seq: 72, #new-token: 72, #cached-token: 52606, token usage: 0.03, #running-req: 370, #queue-req: 0, 
[2025-11-07 14:20:58 TP0] Prefill batch, #new-seq: 67, #new-token: 67, #cached-token: 48709, token usage: 0.03, #running-req: 442, #queue-req: 0, 
[2025-11-07 14:20:58 TP0] Prefill batch, #new-seq: 78, #new-token: 78, #cached-token: 56434, token usage: 0.04, #running-req: 509, #queue-req: 0, 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP3] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP2] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP4] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP1] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP5] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP0] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP7] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP6] [fused_moe] using default for (78, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP0] Prefill batch, #new-seq: 26, #new-token: 26, #cached-token: 19005, token usage: 0.04, #running-req: 587, #queue-req: 0, 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP0] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP2] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP3] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP1] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP5] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP4] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP7] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP6] [fused_moe] using default for (26, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP0] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP4] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP1] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP5] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP2] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP3] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP6] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP7] [fused_moe] using default for (613, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:58 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5099, token usage: 0.04, #running-req: 613, #queue-req: 0, 
[2025-11-07 14:20:59 TP0] Prefill batch, #new-seq: 47, #new-token: 47, #cached-token: 34115, token usage: 0.04, #running-req: 620, #queue-req: 0, 
[2025-11-07 14:20:59 TP0] Prefill batch, #new-seq: 48, #new-token: 48, #cached-token: 34930, token usage: 0.04, #running-req: 667, #queue-req: 0, 
[2025-11-07 14:20:59 TP0] Prefill batch, #new-seq: 58, #new-token: 58, #cached-token: 42245, token usage: 0.05, #running-req: 715, #queue-req: 0, 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP3] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP2] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP0] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP1] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP4] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP5] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP6] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP7] [fused_moe] using default for (58, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP0] Prefill batch, #new-seq: 55, #new-token: 55, #cached-token: 40319, token usage: 0.05, #running-req: 773, #queue-req: 0, 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP2] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP3] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP0] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP1] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP4] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP5] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP7] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP6] [fused_moe] using default for (55, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:20:59 TP0] Prefill batch, #new-seq: 65, #new-token: 65, #cached-token: 47104, token usage: 0.06, #running-req: 828, #queue-req: 0, 
[2025-11-07 14:20:59 TP0] Prefill batch, #new-seq: 61, #new-token: 61, #cached-token: 44554, token usage: 0.06, #running-req: 893, #queue-req: 0, 
[2025-11-07 14:20:59 TP0] Prefill batch, #new-seq: 70, #new-token: 70, #cached-token: 51431, token usage: 0.06, #running-req: 954, #queue-req: 0, 
[2025-11-07 14:21:02] INFO:     127.0.0.1:49276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 711, token usage: 0.09, #running-req: 1023, #queue-req: 294, 
[2025-11-07 14:21:03 TP0] Decode batch, #running-req: 1024, #token: 94081, token usage: 0.10, cuda graph: False, gen throughput (token/s): 1674.87, #queue-req: 294, 
[2025-11-07 14:21:03] INFO:     127.0.0.1:46860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:03] INFO:     127.0.0.1:54744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:03 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1447, token usage: 0.10, #running-req: 1022, #queue-req: 292, 
[2025-11-07 14:21:03] INFO:     127.0.0.1:49298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 729, token usage: 0.10, #running-req: 1023, #queue-req: 291, 
[2025-11-07 14:21:04] INFO:     127.0.0.1:46494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:47138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:47424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:51076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 1525, token usage: 0.11, #running-req: 1022, #queue-req: 289, 
[2025-11-07 14:21:04] INFO:     127.0.0.1:48108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:48418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:49130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:49936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:52088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5173, token usage: 0.11, #running-req: 1017, #queue-req: 282, 
[2025-11-07 14:21:04] INFO:     127.0.0.1:47350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:47638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:49284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:52072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2934, token usage: 0.11, #running-req: 1020, #queue-req: 278, 
[2025-11-07 14:21:04] INFO:     127.0.0.1:46968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:47026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:47198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:50906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:51016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:51150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:51232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:54720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5795, token usage: 0.11, #running-req: 1016, #queue-req: 270, 
[2025-11-07 14:21:04] INFO:     127.0.0.1:48058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:49244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:49668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:51372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04] INFO:     127.0.0.1:53604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:04 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3623, token usage: 0.11, #running-req: 1019, #queue-req: 265, 
[2025-11-07 14:21:05] INFO:     127.0.0.1:48574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:49568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:50116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:51138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:51574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:54854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:54998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5009, token usage: 0.11, #running-req: 1017, #queue-req: 258, 
[2025-11-07 14:21:05] INFO:     127.0.0.1:46962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:53164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:54210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:54862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05 TP0] Prefill batch, #new-seq: 4, #new-token: 4, #cached-token: 2880, token usage: 0.11, #running-req: 1020, #queue-req: 254, 
[2025-11-07 14:21:05] INFO:     127.0.0.1:47090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:47606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:48534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:49762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:50946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:52376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:54458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5124, token usage: 0.11, #running-req: 1017, #queue-req: 247, 
[2025-11-07 14:21:05] INFO:     127.0.0.1:46476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:46882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:52108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:54066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:54370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:54938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4485, token usage: 0.11, #running-req: 1018, #queue-req: 241, 
[2025-11-07 14:21:05] INFO:     127.0.0.1:47452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:48732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:52616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:53918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:54180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05] INFO:     127.0.0.1:55386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:05 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4367, token usage: 0.11, #running-req: 1018, #queue-req: 235, 
[2025-11-07 14:21:06] INFO:     127.0.0.1:47670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:50658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:51046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:51272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:53470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:53898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4406, token usage: 0.12, #running-req: 1018, #queue-req: 229, 
[2025-11-07 14:21:06] INFO:     127.0.0.1:48950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:50508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:50546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:51008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:51458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:51850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:51984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:53664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:53848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:54706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8027, token usage: 0.12, #running-req: 1013, #queue-req: 218, 
[2025-11-07 14:21:06] INFO:     127.0.0.1:47632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:47794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:49460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:50280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:51580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:52168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:52598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:54236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5759, token usage: 0.12, #running-req: 1016, #queue-req: 210, 
[2025-11-07 14:21:06] INFO:     127.0.0.1:46774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:47522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:48174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:49100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:49654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:49896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:50822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:51144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:52770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:54182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:54336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7966, token usage: 0.12, #running-req: 1013, #queue-req: 199, 
[2025-11-07 14:21:06] INFO:     127.0.0.1:47278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:48654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:49856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:50860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:52288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06] INFO:     127.0.0.1:53298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:06 TP0] Prefill batch, #new-seq: 6, #new-token: 6, #cached-token: 4358, token usage: 0.12, #running-req: 1018, #queue-req: 193, 
[2025-11-07 14:21:07] INFO:     127.0.0.1:46470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:47330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:48208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:48282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:48404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:48904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:49718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:52378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:52676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:52828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:53104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7987, token usage: 0.12, #running-req: 1013, #queue-req: 182, 
[2025-11-07 14:21:07] INFO:     127.0.0.1:47114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:47896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:48428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:48988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:50838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:50902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:52494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:52674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:52818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:53214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:55052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 8032, token usage: 0.12, #running-req: 1013, #queue-req: 171, 
[2025-11-07 14:21:07] INFO:     127.0.0.1:47562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:47946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:48474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:49350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:49672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:51924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:54080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 4964, token usage: 0.12, #running-req: 1017, #queue-req: 164, 
[2025-11-07 14:21:07] INFO:     127.0.0.1:47072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:48718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:50914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:52346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:53066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07 TP0] Prefill batch, #new-seq: 5, #new-token: 5, #cached-token: 3641, token usage: 0.12, #running-req: 1019, #queue-req: 159, 
[2025-11-07 14:21:07] INFO:     127.0.0.1:48350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:48522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:49808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:50646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:51098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:51354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:52236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:52450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:52928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:54028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07] INFO:     127.0.0.1:54648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:07 TP0] Prefill batch, #new-seq: 11, #new-token: 11, #cached-token: 7983, token usage: 0.12, #running-req: 1013, #queue-req: 148, 
[2025-11-07 14:21:08] INFO:     127.0.0.1:48242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:49360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:49774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:50978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:53448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:53768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:54522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08 TP0] Prefill batch, #new-seq: 7, #new-token: 7, #cached-token: 5213, token usage: 0.12, #running-req: 1017, #queue-req: 141, 
[2025-11-07 14:21:08] INFO:     127.0.0.1:46626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:47084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:47412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:47952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:48182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:48614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:51342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:52070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:54460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08 TP0] Prefill batch, #new-seq: 9, #new-token: 9, #cached-token: 6501, token usage: 0.12, #running-req: 1015, #queue-req: 132, 
[2025-11-07 14:21:08] INFO:     127.0.0.1:46706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:47938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:48226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:51664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:51912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:51992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:53674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08 TP0] Prefill batch, #new-seq: 8, #new-token: 8, #cached-token: 5902, token usage: 0.12, #running-req: 1016, #queue-req: 124, 
[2025-11-07 14:21:08] INFO:     127.0.0.1:47698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:48676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:49096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:49706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:49836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:51794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:52694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:52968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:54262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:54356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:54380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8810, token usage: 0.12, #running-req: 1012, #queue-req: 112, 
[2025-11-07 14:21:08] INFO:     127.0.0.1:46746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:46862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:47340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:47868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:49382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:49416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:50420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:51636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:52396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:52600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:53206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:53684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08] INFO:     127.0.0.1:54286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:08 TP0] Prefill batch, #new-seq: 13, #new-token: 13, #cached-token: 9633, token usage: 0.12, #running-req: 1011, #queue-req: 99, 
[2025-11-07 14:21:09] INFO:     127.0.0.1:46838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:47398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:48052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:48220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:48694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:48794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:49248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:50092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:50668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:50990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:52278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:53282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:53418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:54156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:54310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09 TP0] Prefill batch, #new-seq: 15, #new-token: 15, #cached-token: 10809, token usage: 0.12, #running-req: 1009, #queue-req: 84, 
[2025-11-07 14:21:09] INFO:     127.0.0.1:47888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:48564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:49074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:49186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:51488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:51878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:53134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:53566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:55122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7290, token usage: 0.12, #running-req: 1014, #queue-req: 74, 
[2025-11-07 14:21:09] INFO:     127.0.0.1:46680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:47010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:47054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:47808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:47956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:48632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:49046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:49384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:51242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:53692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7393, token usage: 0.12, #running-req: 1014, #queue-req: 64, 
[2025-11-07 14:21:09] INFO:     127.0.0.1:47570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:48142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:48348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:48560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:49728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:50074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:50644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:52160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:52496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:52692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:54296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:54426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8882, token usage: 0.13, #running-req: 1012, #queue-req: 52, 
[2025-11-07 14:21:09] INFO:     127.0.0.1:47470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:48036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:50538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:50888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:51170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:51766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:51960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:52060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:52358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:52464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:53182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:53356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:54074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:54114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:54134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:09 TP0] Prefill batch, #new-seq: 18, #new-token: 18, #cached-token: 13105, token usage: 0.12, #running-req: 1006, #queue-req: 34, 
[2025-11-07 14:21:10] INFO:     127.0.0.1:48758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:49102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:49542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:49580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:49694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:50442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:51410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:51706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:51988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:52210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:53584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:55166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8753, token usage: 0.13, #running-req: 1012, #queue-req: 22, 
[2025-11-07 14:21:10 TP0] Decode batch, #running-req: 1012, #token: 120246, token usage: 0.12, cuda graph: False, gen throughput (token/s): 5679.43, #queue-req: 22, 
[2025-11-07 14:21:10] INFO:     127.0.0.1:46736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:47518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:47704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:49484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:50298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:51840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:52340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:52508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:53400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:53648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:54194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:55262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10 TP0] Prefill batch, #new-seq: 12, #new-token: 12, #cached-token: 8787, token usage: 0.13, #running-req: 1012, #queue-req: 10, 
[2025-11-07 14:21:10] INFO:     127.0.0.1:46796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:47228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:48306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:48738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:48786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:48864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:49496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:49788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:50124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:51034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:52858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:53202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:54800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10 TP0] Prefill batch, #new-seq: 10, #new-token: 10, #cached-token: 7209, token usage: 0.13, #running-req: 1011, #queue-req: 0, 
[2025-11-07 14:21:10] INFO:     127.0.0.1:47774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:47942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:49820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:50558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:53048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:54172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:55182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:55264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:46692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:47962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:48380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:54796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:55360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:48066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:48852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:49968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:50878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:50900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:52180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:52484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:52942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:53474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:53548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:54840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:10] INFO:     127.0.0.1:54964 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:10 TP6] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:10 TP4] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:10 TP2] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:10 TP7] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:10 TP5] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:10 TP0] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:10 TP3] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:10 TP1] [fused_moe] using default for (992, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11] INFO:     127.0.0.1:48750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:50594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:52412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:53190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:55032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:55200 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP6] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP4] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP2] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP5] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP7] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP0] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP3] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP1] [fused_moe] using default for (984, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11] INFO:     127.0.0.1:47324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:47498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:47732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:48124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:48774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:50862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:53256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:53700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:46756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:52150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:52198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:53994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54702 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP4] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP6] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP2] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP5] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP1] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP0] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP7] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP3] [fused_moe] using default for (965, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11] INFO:     127.0.0.1:46938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:46974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:47050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:47074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:47180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:48920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:50962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:52910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:52984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:53974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54668 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP4] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP2] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP5] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP0] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP6] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP1] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP3] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP7] [fused_moe] using default for (950, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11] INFO:     127.0.0.1:46650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:46992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:48566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:48826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:49514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:53546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:53888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54978 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP5] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP0] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP1] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP4] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP2] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP6] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP7] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP3] [fused_moe] using default for (938, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11] INFO:     127.0.0.1:47240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:47318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:47842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:55102 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP0] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP5] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP6] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP4] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP2] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP1] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP3] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP7] [fused_moe] using default for (930, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11] INFO:     127.0.0.1:47094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:49732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:52472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:52712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:52784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:55290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:46804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:47238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:47882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:48382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:48590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:49082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:50250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:50688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:52552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:53842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:54470 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP4] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP2] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP6] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP0] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP5] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP3] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP7] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP1] [fused_moe] using default for (908, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11] INFO:     127.0.0.1:47080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:47366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:48440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:49950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:50198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:53328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:53822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:46512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:46772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:46986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:47970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:49356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:50188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:50528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:50692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:50780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:51392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:11] INFO:     127.0.0.1:52666 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP2] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP4] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP6] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP0] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP5] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP1] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP3] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:11 TP7] [fused_moe] using default for (887, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12] INFO:     127.0.0.1:49278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:49744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:49798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:52270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:53504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:53790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:53930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54228 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP2] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP4] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP0] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP5] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP6] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP1] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP3] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP7] [fused_moe] using default for (874, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:47590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:48716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:55322 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP4] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP2] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP6] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP0] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP5] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP1] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP7] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP3] [fused_moe] using default for (868, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12] INFO:     127.0.0.1:47308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:47344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:48776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:48876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:49340 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:49398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:49608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:49928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:53508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP4] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP2] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP6] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP0] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP5] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP1] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP3] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP7] [fused_moe] using default for (854, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12] INFO:     127.0.0.1:47056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:47648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:47664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:48668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:48942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:49410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:52874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:53170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:53612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:55452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:49734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:55050 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP4] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP2] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP0] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP5] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP1] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP6] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP3] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP7] [fused_moe] using default for (832, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12] INFO:     127.0.0.1:47428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:47758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:49212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:49620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:52328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:52772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:53238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:46498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:47124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:48068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:52976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54636 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP4] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP2] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP6] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP5] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP0] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP1] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP3] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP7] [fused_moe] using default for (809, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12] INFO:     127.0.0.1:46956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:47046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:52842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:53272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:53432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:55156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:55872 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP4] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP2] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP6] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP1] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP0] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP5] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP3] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP7] [fused_moe] using default for (798, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12] INFO:     127.0.0.1:47178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:47716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:52442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:54390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:55254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:55570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:56096 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP0] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP4] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP5] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP2] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP1] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP6] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP3] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP7] [fused_moe] using default for (786, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12] INFO:     127.0.0.1:46502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:47168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:49540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:49874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:50388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:51836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:55514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:12] INFO:     127.0.0.1:55930 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP4] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP5] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP0] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP2] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP6] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP1] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP3] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:12 TP7] [fused_moe] using default for (774, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13] INFO:     127.0.0.1:46894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:47040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:47242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:48346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:52468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:52776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:47538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:48896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:52036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:52382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:54778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP4] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP5] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP6] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP0] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP2] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP1] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP7] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP3] [fused_moe] using default for (756, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13] INFO:     127.0.0.1:46688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:48960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:52124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:54888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:55046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:56592 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP4] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP2] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP6] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP5] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP0] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP7] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP3] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP1] [fused_moe] using default for (747, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP4] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP2] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP5] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP6] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP0] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP3] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP7] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP1] [fused_moe] using default for (738, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13] INFO:     127.0.0.1:51262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:52050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:52802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:54644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:55444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:55976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:56014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:56582 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP4] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP2] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP0] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP5] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP6] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP1] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP3] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP7] [fused_moe] using default for (725, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13] INFO:     127.0.0.1:47540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:47744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:48538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:51062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:54178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:55070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:55286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:55476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:55488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:49144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:51504 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:51868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:52400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:54928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:55222 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP4] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP5] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP0] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP1] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP2] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP6] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP3] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP7] [fused_moe] using default for (713, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13] INFO:     127.0.0.1:46574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:46912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:48974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:49566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:49730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:55162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:47382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:47576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:47620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:48940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:49016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:51022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:52970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:54044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:56278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:46824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:49860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:50854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:52576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:54400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:54784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:55186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:56508 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP2] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP4] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP6] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP0] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP5] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP3] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP1] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13 TP7] [fused_moe] using default for (676, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:13] INFO:     127.0.0.1:47922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:51180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:52794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:53928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:56176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:13] INFO:     127.0.0.1:56438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:46614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:48624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:49292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:49598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:50036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:50754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:51360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:51726 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:51846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:52046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:52658 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:53018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:53960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:55622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:56758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:57284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:57684 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP2] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP4] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP5] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP6] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP0] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP1] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP3] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP7] [fused_moe] using default for (650, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14] INFO:     127.0.0.1:46676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:47348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:47910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:48024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:48652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:48664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:49154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:49368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:51130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:54206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:55344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:56250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:56550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:56928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:47154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:47556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:49122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:51560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:54486 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP2] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP4] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP6] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP5] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP0] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP1] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP3] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP7] [fused_moe] using default for (630, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14] INFO:     127.0.0.1:47374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:48494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:49884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:50632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:50942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:52610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:52632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:53632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:55270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:56262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:46642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:47116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:47292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:48844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:48994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:49158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:49526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:52086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:52388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:53012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:54684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:54808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:55500 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP4] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP2] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP5] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP6] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP0] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP1] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP3] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP7] [fused_moe] using default for (607, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP0] Decode batch, #running-req: 620, #token: 91588, token usage: 0.09, cuda graph: False, gen throughput (token/s): 7791.79, #queue-req: 0, 
[2025-11-07 14:21:14] INFO:     127.0.0.1:47690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:48140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:50610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:52024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:52366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:54594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:55194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:56706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:57868 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP4] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP2] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP6] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP0] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP5] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP1] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP3] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP7] [fused_moe] using default for (598, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14] INFO:     127.0.0.1:51302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:52296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:56130 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP2] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP4] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP6] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP0] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP5] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP1] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP3] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP7] [fused_moe] using default for (595, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14] INFO:     127.0.0.1:49022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:49474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:50454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:50496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:51716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:53178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:57130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:57656 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP2] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP4] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP6] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP5] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP1] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP0] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP3] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP7] [fused_moe] using default for (587, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14] INFO:     127.0.0.1:46922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:47446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:48456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:48866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:50786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:53464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:54222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP2] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP4] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP0] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP6] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP5] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP1] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP3] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP7] [fused_moe] using default for (578, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14] INFO:     127.0.0.1:46896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:46970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:47458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:48424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:48438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:50830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:51742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:53290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:53358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:54142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:54342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:55136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:55276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:55678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:57142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:46602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:47782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:49434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:49594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:49644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:49890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:51258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:52254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:53914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:55686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:55778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:56080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:56832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:14] INFO:     127.0.0.1:57722 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP4] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP2] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP0] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP5] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP6] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP1] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP3] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:14 TP7] [fused_moe] using default for (548, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:15] INFO:     127.0.0.1:46790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:48084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:48188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:48706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:48934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:50580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:50806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:52304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:52906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:52998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:53782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:53838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:53990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:55170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:55632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:57348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:49502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:49770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:51894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:52248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:55550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:57426 - "POST /generate HTTP/1.1" 200 OK
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:15 TP4] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:15 TP0] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:15 TP5] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:15 TP2] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:15 TP6] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:15 TP1] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[aiter] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:15 TP3] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:15 TP7] [fused_moe] using default for (525, 7168, 256, 256, 8, 'ActivationType.Silu', 'torch.bfloat16', 'torch.float8_e4m3fnuz', 'torch.float8_e4m3fnuz', 'QuantType.per_128x128', True, False) 
[2025-11-07 14:21:15] INFO:     127.0.0.1:49168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:49316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:51928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:52642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:57268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:48158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:49748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:51328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:52900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:53430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:54454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:54768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:55716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:48202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:51678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:52564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:54920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:57028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:48292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:50770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:52738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:46952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:49978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:50090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:50570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:53734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:57380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:46548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:46864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:47186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:48884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:49036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:50474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:52474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:54552 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:46810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:48268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:49678 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:52402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:52706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:53088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:54004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:55668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:57112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:50512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:52186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:53676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:55782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:46730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:49196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:50636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:51096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:52418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:53032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:54270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:55828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:53866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:53946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:55408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:55534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:46492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:48314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:48828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:49106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:51294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:53988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:54436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:15] INFO:     127.0.0.1:56864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:46520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:48810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:54824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:55988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:48486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:49000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:51828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:52432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:52510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:52862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:55680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:55944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:55986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:58018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:58032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:49538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:50404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:51118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:52140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:52478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:54878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:49716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:49784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:51808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:53754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:53962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:54096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:54956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:47626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:48914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:49912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:51208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:54128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:47438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:51910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:53720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:54626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:55748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:49622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:51584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:47270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:47994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:49054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:50046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:53582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:47510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:48394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:48554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:49422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:50784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:52082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:52526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:53332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:54098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:54508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:54692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:46876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:51094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:52498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:52902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:53124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:55438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57892 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:48364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:51600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:55614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:46794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:48256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:49264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:52008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:53752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:53896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:55656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:50588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:53076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:55532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:55298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:55376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:54704 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:56218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:57674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:16] INFO:     127.0.0.1:58080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:49880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:53526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:48464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:58008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:58102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:48432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:48588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:51224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:58094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:58130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:47824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:48596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:49444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:50532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:49994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17 TP0] Decode batch, #running-req: 282, #token: 53403, token usage: 0.05, cuda graph: True, gen throughput (token/s): 6091.04, #queue-req: 0, 
[2025-11-07 14:21:17] INFO:     127.0.0.1:48672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:50372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:47906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:52538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:53072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:51538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:52590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:58066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:46536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:47866 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:49236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:51598 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:47004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:47992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:48692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:55510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:48492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:49886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:50084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:53588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:52314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:54984 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:58056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:47980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:48890 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56956 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:50000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:50660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:56900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:17] INFO:     127.0.0.1:57640 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:55446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:55732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:56056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:56784 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:49630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:55330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:50336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:53522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:48096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:50590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:51996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:56418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:51474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:52888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:55314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:53534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:54338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:56454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:56526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:56848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:51106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:52092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:52898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:53148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:56512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:56772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:48508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:50480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:55554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:46588 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:48636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:55762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:51882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:55596 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:55936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:46770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:51194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:55942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:56478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:49310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:50264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:52220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:52724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:46720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:55816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:55860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:51780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:48612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:50162 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:49554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:50960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:48626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:51614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:47950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:56930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:56412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:56514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:47826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:49328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:53758 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:55394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:55804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:56814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:18] INFO:     127.0.0.1:57508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:56030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:57580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:49952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:55610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:55792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:56018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:56668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:57192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:57366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:57884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:47684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:56720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:57394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:57594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:47486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:52138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:55608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:56046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:52512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:51972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:56190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:56472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:57428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19 TP0] Decode batch, #running-req: 115, #token: 27712, token usage: 0.03, cuda graph: True, gen throughput (token/s): 3949.23, #queue-req: 0, 
[2025-11-07 14:21:19] INFO:     127.0.0.1:46604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:54174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:55854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:57002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:49070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:51512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:52324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:56500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:57454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:58046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:55462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:56788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:50430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:50684 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:53554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:55490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:47098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:46798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:48098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:57296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:49090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:56690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:54610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:55234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:47256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:48944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:51274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:53362 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:47756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:47862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:53110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:54732 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:56386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:56796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:50358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:51622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:57906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:50392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:50938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:55642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:56806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:19] INFO:     127.0.0.1:55908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:55528 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:55998 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:56138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:47212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:49358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:53620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:55086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:56466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:46830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:56730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:47878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:46850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:49166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:58116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:55922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:55208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:56280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:56978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:52754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:56828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:58086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:53440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:53742 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:51748 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:53494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20 TP0] Decode batch, #running-req: 33, #token: 9339, token usage: 0.01, cuda graph: True, gen throughput (token/s): 1867.26, #queue-req: 0, 
[2025-11-07 14:21:20] INFO:     127.0.0.1:54586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:54526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:47858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:52834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:20] INFO:     127.0.0.1:57058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:57958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:50024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:56074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:49982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:48008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:55600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:57970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:55134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:56670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:56746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:57572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:57824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:55068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:56318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:56124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:55960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:47416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:55898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21 TP0] Decode batch, #running-req: 8, #token: 3293, token usage: 0.00, cuda graph: True, gen throughput (token/s): 727.51, #queue-req: 0, 
[2025-11-07 14:21:21] INFO:     127.0.0.1:53624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:21] INFO:     127.0.0.1:56744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:22] INFO:     127.0.0.1:53314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:22] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:22] INFO:     127.0.0.1:55300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:22 TP0] Decode batch, #running-req: 3, #token: 1354, token usage: 0.00, cuda graph: True, gen throughput (token/s): 214.48, #queue-req: 0, 
[2025-11-07 14:21:22] INFO:     127.0.0.1:56372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:22] INFO:     127.0.0.1:56896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:23 TP0] Decode batch, #running-req: 1, #token: 1103, token usage: 0.00, cuda graph: True, gen throughput (token/s): 62.20, #queue-req: 0, 
[2025-11-07 14:21:24 TP0] Decode batch, #running-req: 1, #token: 1143, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.63, #queue-req: 0, 
[2025-11-07 14:21:25 TP0] Decode batch, #running-req: 1, #token: 1183, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.62, #queue-req: 0, 
[2025-11-07 14:21:25 TP0] Decode batch, #running-req: 1, #token: 1223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.62, #queue-req: 0, 
[2025-11-07 14:21:26] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:21:37] INFO:     127.0.0.1:53994 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-07 14:22:34] INFO:     127.0.0.1:36202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:34 TP0] Prefill batch, #new-seq: 1, #new-token: 3200, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:22:35 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 0.58, #queue-req: 0, 
[2025-11-07 14:22:36] INFO:     127.0.0.1:41356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:22:36] INFO:     127.0.0.1:41360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41458 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-11-07 14:22:36] INFO:     127.0.0.1:41502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41562 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41632 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.02, #running-req: 6, #queue-req: 21, 
[2025-11-07 14:22:36] INFO:     127.0.0.1:41672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41716 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41768 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41876 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:41992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:36] INFO:     127.0.0.1:42144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42156 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42212 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42442 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37] INFO:     127.0.0.1:42492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:22:37 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-11-07 14:22:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-11-07 14:22:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-11-07 14:22:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.09, #running-req: 26, #queue-req: 97, 
[2025-11-07 14:22:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-11-07 14:22:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-11-07 14:22:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-11-07 14:22:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-11-07 14:22:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-11-07 14:22:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-11-07 14:22:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.20, #running-req: 61, #queue-req: 62, 
[2025-11-07 14:22:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.22, #running-req: 66, #queue-req: 57, 
[2025-11-07 14:22:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.23, #running-req: 71, #queue-req: 52, 
[2025-11-07 14:22:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.25, #running-req: 76, #queue-req: 47, 
[2025-11-07 14:22:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.27, #running-req: 81, #queue-req: 42, 
[2025-11-07 14:22:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.28, #running-req: 86, #queue-req: 37, 
[2025-11-07 14:22:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.30, #running-req: 91, #queue-req: 32, 
[2025-11-07 14:22:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.32, #running-req: 96, #queue-req: 27, 
[2025-11-07 14:22:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.33, #running-req: 101, #queue-req: 22, 
[2025-11-07 14:22:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.35, #running-req: 106, #queue-req: 17, 
[2025-11-07 14:22:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.37, #running-req: 111, #queue-req: 12, 
[2025-11-07 14:22:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.38, #running-req: 116, #queue-req: 7, 
[2025-11-07 14:22:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.40, #running-req: 121, #queue-req: 2, 
[2025-11-07 14:22:56 TP0] Prefill batch, #new-seq: 2, #new-token: 6394, #cached-token: 8, token usage: 0.41, #running-req: 126, #queue-req: 0, 
[2025-11-07 14:22:58 TP0] Decode batch, #running-req: 128, #token: 413727, token usage: 0.43, cuda graph: True, gen throughput (token/s): 179.38, #queue-req: 0, 
[2025-11-07 14:23:00 TP0] Decode batch, #running-req: 128, #token: 418847, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2531.73, #queue-req: 0, 
[2025-11-07 14:23:02 TP0] Decode batch, #running-req: 128, #token: 423967, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2510.84, #queue-req: 0, 
[2025-11-07 14:23:04 TP0] Decode batch, #running-req: 128, #token: 429087, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2498.53, #queue-req: 0, 
[2025-11-07 14:23:06 TP0] Decode batch, #running-req: 128, #token: 434207, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2489.55, #queue-req: 0, 
[2025-11-07 14:23:09 TP0] Decode batch, #running-req: 128, #token: 439327, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2481.26, #queue-req: 0, 
[2025-11-07 14:23:11 TP0] Decode batch, #running-req: 128, #token: 444447, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2473.95, #queue-req: 0, 
[2025-11-07 14:23:13 TP0] Decode batch, #running-req: 128, #token: 449567, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2467.33, #queue-req: 0, 
[2025-11-07 14:23:15 TP0] Decode batch, #running-req: 128, #token: 454687, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2459.98, #queue-req: 0, 
[2025-11-07 14:23:17 TP0] Decode batch, #running-req: 128, #token: 459807, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2452.76, #queue-req: 0, 
[2025-11-07 14:23:19 TP0] Decode batch, #running-req: 128, #token: 464927, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2449.31, #queue-req: 0, 
[2025-11-07 14:23:21 TP0] Decode batch, #running-req: 128, #token: 470047, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2447.40, #queue-req: 0, 
[2025-11-07 14:23:23 TP0] Decode batch, #running-req: 128, #token: 475167, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2441.06, #queue-req: 0, 
[2025-11-07 14:23:25 TP0] Decode batch, #running-req: 128, #token: 480287, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2432.57, #queue-req: 0, 
[2025-11-07 14:23:27 TP0] Decode batch, #running-req: 128, #token: 485407, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2428.26, #queue-req: 0, 
[2025-11-07 14:23:29 TP0] Decode batch, #running-req: 128, #token: 490527, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2422.43, #queue-req: 0, 
[2025-11-07 14:23:32 TP0] Decode batch, #running-req: 128, #token: 495647, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2418.06, #queue-req: 0, 
[2025-11-07 14:23:34 TP0] Decode batch, #running-req: 128, #token: 500767, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2412.07, #queue-req: 0, 
[2025-11-07 14:23:36 TP0] Decode batch, #running-req: 128, #token: 505887, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2406.22, #queue-req: 0, 
[2025-11-07 14:23:38 TP0] Decode batch, #running-req: 128, #token: 511007, token usage: 0.53, cuda graph: True, gen throughput (token/s): 2402.92, #queue-req: 0, 
[2025-11-07 14:23:38] INFO:     127.0.0.1:41978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:38 TP0] Prefill batch, #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:23:38] INFO:     127.0.0.1:41988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:38] INFO:     127.0.0.1:41994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:38] INFO:     127.0.0.1:42006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:38] INFO:     127.0.0.1:42020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:38] INFO:     127.0.0.1:42022 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:38] INFO:     127.0.0.1:42024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:38] INFO:     127.0.0.1:42036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:38] INFO:     127.0.0.1:42042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:38] INFO:     127.0.0.1:42050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:38] INFO:     127.0.0.1:42054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:38] INFO:     127.0.0.1:42058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:38] INFO:     127.0.0.1:42074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:38] INFO:     127.0.0.1:42082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 7, 
[2025-11-07 14:23:39] INFO:     127.0.0.1:42106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42160 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 6, #queue-req: 41, 
[2025-11-07 14:23:39] INFO:     127.0.0.1:42448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42584 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42642 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42756 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42864 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:42994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:39] INFO:     127.0.0.1:43134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:23:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-11-07 14:23:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-11-07 14:23:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-11-07 14:23:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 26, #queue-req: 97, 
[2025-11-07 14:23:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-11-07 14:23:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.12, #running-req: 36, #queue-req: 87, 
[2025-11-07 14:23:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 41, #queue-req: 82, 
[2025-11-07 14:23:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 46, #queue-req: 77, 
[2025-11-07 14:23:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.17, #running-req: 51, #queue-req: 72, 
[2025-11-07 14:23:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.18, #running-req: 56, #queue-req: 67, 
[2025-11-07 14:23:48 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.20, #running-req: 61, #queue-req: 61, 
[2025-11-07 14:23:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.22, #running-req: 67, #queue-req: 56, 
[2025-11-07 14:23:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15977, #cached-token: 28, token usage: 0.23, #running-req: 72, #queue-req: 51, 
[2025-11-07 14:23:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.25, #running-req: 77, #queue-req: 46, 
[2025-11-07 14:23:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.27, #running-req: 82, #queue-req: 41, 
[2025-11-07 14:23:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.28, #running-req: 87, #queue-req: 36, 
[2025-11-07 14:23:53 TP0] Prefill batch, #new-seq: 6, #new-token: 15994, #cached-token: 3212, token usage: 0.30, #running-req: 92, #queue-req: 30, 
[2025-11-07 14:23:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.32, #running-req: 98, #queue-req: 25, 
[2025-11-07 14:23:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.34, #running-req: 103, #queue-req: 20, 
[2025-11-07 14:23:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.35, #running-req: 108, #queue-req: 15, 
[2025-11-07 14:23:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.37, #running-req: 113, #queue-req: 10, 
[2025-11-07 14:23:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.39, #running-req: 118, #queue-req: 5, 
[2025-11-07 14:23:58 TP0] Prefill batch, #new-seq: 5, #new-token: 12792, #cached-token: 3213, token usage: 0.40, #running-req: 123, #queue-req: 0, 
[2025-11-07 14:24:01 TP0] Decode batch, #running-req: 128, #token: 407325, token usage: 0.42, cuda graph: True, gen throughput (token/s): 224.48, #queue-req: 0, 
[2025-11-07 14:24:03 TP0] Decode batch, #running-req: 128, #token: 412445, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2503.87, #queue-req: 0, 
[2025-11-07 14:24:05 TP0] Decode batch, #running-req: 128, #token: 417565, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2488.15, #queue-req: 0, 
[2025-11-07 14:24:07 TP0] Decode batch, #running-req: 128, #token: 422685, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2479.23, #queue-req: 0, 
[2025-11-07 14:24:09 TP0] Decode batch, #running-req: 128, #token: 427805, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2471.96, #queue-req: 0, 
[2025-11-07 14:24:11 TP0] Decode batch, #running-req: 128, #token: 432925, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2464.00, #queue-req: 0, 
[2025-11-07 14:24:13 TP0] Decode batch, #running-req: 128, #token: 438045, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2456.98, #queue-req: 0, 
[2025-11-07 14:24:15 TP0] Decode batch, #running-req: 128, #token: 443165, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2451.20, #queue-req: 0, 
[2025-11-07 14:24:17 TP0] Decode batch, #running-req: 128, #token: 448285, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2445.65, #queue-req: 0, 
[2025-11-07 14:24:19 TP0] Decode batch, #running-req: 128, #token: 453405, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2440.08, #queue-req: 0, 
[2025-11-07 14:24:22 TP0] Decode batch, #running-req: 128, #token: 458525, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2437.17, #queue-req: 0, 
[2025-11-07 14:24:24 TP0] Decode batch, #running-req: 128, #token: 463645, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2433.23, #queue-req: 0, 
[2025-11-07 14:24:26 TP0] Decode batch, #running-req: 128, #token: 468765, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2429.35, #queue-req: 0, 
[2025-11-07 14:24:28 TP0] Decode batch, #running-req: 128, #token: 473885, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2425.05, #queue-req: 0, 
[2025-11-07 14:24:30 TP0] Decode batch, #running-req: 128, #token: 479005, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2420.48, #queue-req: 0, 
[2025-11-07 14:24:32 TP0] Decode batch, #running-req: 128, #token: 484125, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2412.76, #queue-req: 0, 
[2025-11-07 14:24:34 TP0] Decode batch, #running-req: 128, #token: 489245, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2410.09, #queue-req: 0, 
[2025-11-07 14:24:36 TP0] Decode batch, #running-req: 128, #token: 494365, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2404.93, #queue-req: 0, 
[2025-11-07 14:24:39 TP0] Decode batch, #running-req: 128, #token: 499485, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2401.54, #queue-req: 0, 
[2025-11-07 14:24:41 TP0] Decode batch, #running-req: 128, #token: 504605, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2398.19, #queue-req: 0, 
[2025-11-07 14:24:41] INFO:     127.0.0.1:49052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:24:41] INFO:     127.0.0.1:49062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49188 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-11-07 14:24:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.02, #running-req: 6, #queue-req: 4, 
[2025-11-07 14:24:41] INFO:     127.0.0.1:49198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:41] INFO:     127.0.0.1:49220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49400 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49454 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49544 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49624 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49682 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49728 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49738 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49804 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49838 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:49990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42] INFO:     127.0.0.1:50274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:24:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.04, #running-req: 11, #queue-req: 112, 
[2025-11-07 14:24:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.05, #running-req: 16, #queue-req: 107, 
[2025-11-07 14:24:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 21, #queue-req: 102, 
[2025-11-07 14:24:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.09, #running-req: 26, #queue-req: 97, 
[2025-11-07 14:24:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.10, #running-req: 31, #queue-req: 92, 
[2025-11-07 14:24:46 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.12, #running-req: 36, #queue-req: 86, 
[2025-11-07 14:24:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.14, #running-req: 42, #queue-req: 81, 
[2025-11-07 14:24:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.15, #running-req: 47, #queue-req: 76, 
[2025-11-07 14:24:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.17, #running-req: 52, #queue-req: 71, 
[2025-11-07 14:24:50 TP0] Prefill batch, #new-seq: 6, #new-token: 15985, #cached-token: 3221, token usage: 0.19, #running-req: 57, #queue-req: 65, 
[2025-11-07 14:24:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.21, #running-req: 63, #queue-req: 60, 
[2025-11-07 14:24:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.22, #running-req: 68, #queue-req: 55, 
[2025-11-07 14:24:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.24, #running-req: 73, #queue-req: 50, 
[2025-11-07 14:24:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.26, #running-req: 78, #queue-req: 45, 
[2025-11-07 14:24:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.27, #running-req: 83, #queue-req: 40, 
[2025-11-07 14:24:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.29, #running-req: 88, #queue-req: 35, 
[2025-11-07 14:24:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.31, #running-req: 93, #queue-req: 30, 
[2025-11-07 14:24:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.32, #running-req: 98, #queue-req: 25, 
[2025-11-07 14:24:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.34, #running-req: 103, #queue-req: 20, 
[2025-11-07 14:24:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.36, #running-req: 108, #queue-req: 15, 
[2025-11-07 14:24:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.37, #running-req: 113, #queue-req: 10, 
[2025-11-07 14:24:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.39, #running-req: 118, #queue-req: 5, 
[2025-11-07 14:25:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.40, #running-req: 123, #queue-req: 0, 
[2025-11-07 14:25:03 TP0] Decode batch, #running-req: 128, #token: 413724, token usage: 0.43, cuda graph: True, gen throughput (token/s): 224.29, #queue-req: 0, 
[2025-11-07 14:25:06 TP0] Decode batch, #running-req: 128, #token: 418844, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2500.72, #queue-req: 0, 
[2025-11-07 14:25:08 TP0] Decode batch, #running-req: 128, #token: 423964, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2481.62, #queue-req: 0, 
[2025-11-07 14:25:10 TP0] Decode batch, #running-req: 128, #token: 429084, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2472.25, #queue-req: 0, 
[2025-11-07 14:25:12 TP0] Decode batch, #running-req: 128, #token: 434204, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2465.05, #queue-req: 0, 
[2025-11-07 14:25:14 TP0] Decode batch, #running-req: 128, #token: 439324, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2461.24, #queue-req: 0, 
[2025-11-07 14:25:16 TP0] Decode batch, #running-req: 128, #token: 444444, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2455.39, #queue-req: 0, 
[2025-11-07 14:25:18 TP0] Decode batch, #running-req: 128, #token: 449564, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2453.03, #queue-req: 0, 
[2025-11-07 14:25:20 TP0] Decode batch, #running-req: 128, #token: 454684, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2446.56, #queue-req: 0, 
[2025-11-07 14:25:22 TP0] Decode batch, #running-req: 128, #token: 459804, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2442.11, #queue-req: 0, 
[2025-11-07 14:25:24 TP0] Decode batch, #running-req: 128, #token: 464924, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2441.02, #queue-req: 0, 
[2025-11-07 14:25:26 TP0] Decode batch, #running-req: 128, #token: 470044, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2439.83, #queue-req: 0, 
[2025-11-07 14:25:28 TP0] Decode batch, #running-req: 128, #token: 475164, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2438.09, #queue-req: 0, 
[2025-11-07 14:25:31 TP0] Decode batch, #running-req: 128, #token: 480284, token usage: 0.49, cuda graph: True, gen throughput (token/s): 2427.57, #queue-req: 0, 
[2025-11-07 14:25:33 TP0] Decode batch, #running-req: 128, #token: 485404, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2420.87, #queue-req: 0, 
[2025-11-07 14:25:35 TP0] Decode batch, #running-req: 128, #token: 490524, token usage: 0.50, cuda graph: True, gen throughput (token/s): 2418.11, #queue-req: 0, 
[2025-11-07 14:25:37 TP0] Decode batch, #running-req: 128, #token: 495644, token usage: 0.51, cuda graph: True, gen throughput (token/s): 2410.51, #queue-req: 0, 
[2025-11-07 14:25:39 TP0] Decode batch, #running-req: 128, #token: 500764, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2407.94, #queue-req: 0, 
[2025-11-07 14:25:41 TP0] Decode batch, #running-req: 128, #token: 505884, token usage: 0.52, cuda graph: True, gen throughput (token/s): 2404.00, #queue-req: 0, 
[2025-11-07 14:25:43 TP0] Decode batch, #running-req: 128, #token: 511004, token usage: 0.53, cuda graph: True, gen throughput (token/s): 2397.47, #queue-req: 0, 
[2025-11-07 14:25:44] INFO:     127.0.0.1:40414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:25:44] INFO:     127.0.0.1:40426 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40434 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40470 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40506 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-11-07 14:25:44] INFO:     127.0.0.1:40522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40546 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40634 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40650 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40766 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40794 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40808 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40814 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 38, 
[2025-11-07 14:25:44] INFO:     127.0.0.1:40874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40928 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40944 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40952 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:40994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41024 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41186 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41202 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41230 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41266 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41294 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41314 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41328 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:44] INFO:     127.0.0.1:41376 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:25:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 100, 
[2025-11-07 14:25:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 95, 
[2025-11-07 14:25:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 21, #queue-req: 90, 
[2025-11-07 14:25:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.09, #running-req: 26, #queue-req: 85, 
[2025-11-07 14:25:48 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.10, #running-req: 31, #queue-req: 80, 
[2025-11-07 14:25:49 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 36, #queue-req: 75, 
[2025-11-07 14:25:50 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.13, #running-req: 41, #queue-req: 70, 
[2025-11-07 14:25:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 46, #queue-req: 65, 
[2025-11-07 14:25:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.17, #running-req: 51, #queue-req: 60, 
[2025-11-07 14:25:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.18, #running-req: 56, #queue-req: 55, 
[2025-11-07 14:25:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.20, #running-req: 61, #queue-req: 50, 
[2025-11-07 14:25:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.22, #running-req: 66, #queue-req: 45, 
[2025-11-07 14:25:55 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.24, #running-req: 71, #queue-req: 39, 
[2025-11-07 14:25:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.25, #running-req: 77, #queue-req: 34, 
[2025-11-07 14:25:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.27, #running-req: 82, #queue-req: 29, 
[2025-11-07 14:25:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.29, #running-req: 87, #queue-req: 24, 
[2025-11-07 14:25:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.30, #running-req: 92, #queue-req: 19, 
[2025-11-07 14:25:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.32, #running-req: 97, #queue-req: 14, 
[2025-11-07 14:26:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.34, #running-req: 102, #queue-req: 9, 
[2025-11-07 14:26:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.35, #running-req: 107, #queue-req: 4, 
[2025-11-07 14:26:01 TP0] Prefill batch, #new-seq: 4, #new-token: 12787, #cached-token: 17, token usage: 0.37, #running-req: 112, #queue-req: 0, 
[2025-11-07 14:26:04 TP0] Decode batch, #running-req: 116, #token: 374942, token usage: 0.39, cuda graph: True, gen throughput (token/s): 224.45, #queue-req: 0, 
[2025-11-07 14:26:06 TP0] Decode batch, #running-req: 116, #token: 379582, token usage: 0.39, cuda graph: True, gen throughput (token/s): 2324.59, #queue-req: 0, 
[2025-11-07 14:26:08 TP0] Decode batch, #running-req: 116, #token: 384222, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2310.76, #queue-req: 0, 
[2025-11-07 14:26:10 TP0] Decode batch, #running-req: 116, #token: 388862, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2302.35, #queue-req: 0, 
[2025-11-07 14:26:12 TP0] Decode batch, #running-req: 116, #token: 393502, token usage: 0.40, cuda graph: True, gen throughput (token/s): 2293.41, #queue-req: 0, 
[2025-11-07 14:26:14 TP0] Decode batch, #running-req: 116, #token: 398142, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2286.27, #queue-req: 0, 
[2025-11-07 14:26:16 TP0] Decode batch, #running-req: 116, #token: 402782, token usage: 0.41, cuda graph: True, gen throughput (token/s): 2284.56, #queue-req: 0, 
[2025-11-07 14:26:19 TP0] Decode batch, #running-req: 116, #token: 407422, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2280.30, #queue-req: 0, 
[2025-11-07 14:26:21 TP0] Decode batch, #running-req: 116, #token: 412062, token usage: 0.42, cuda graph: True, gen throughput (token/s): 2275.27, #queue-req: 0, 
[2025-11-07 14:26:23 TP0] Decode batch, #running-req: 116, #token: 416702, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2272.88, #queue-req: 0, 
[2025-11-07 14:26:25 TP0] Decode batch, #running-req: 116, #token: 421342, token usage: 0.43, cuda graph: True, gen throughput (token/s): 2266.84, #queue-req: 0, 
[2025-11-07 14:26:27 TP0] Decode batch, #running-req: 116, #token: 425982, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2262.54, #queue-req: 0, 
[2025-11-07 14:26:29 TP0] Decode batch, #running-req: 116, #token: 430622, token usage: 0.44, cuda graph: True, gen throughput (token/s): 2259.21, #queue-req: 0, 
[2025-11-07 14:26:31 TP0] Decode batch, #running-req: 116, #token: 435262, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2255.10, #queue-req: 0, 
[2025-11-07 14:26:33 TP0] Decode batch, #running-req: 116, #token: 439902, token usage: 0.45, cuda graph: True, gen throughput (token/s): 2246.32, #queue-req: 0, 
[2025-11-07 14:26:35 TP0] Decode batch, #running-req: 116, #token: 444542, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2242.22, #queue-req: 0, 
[2025-11-07 14:26:37 TP0] Decode batch, #running-req: 116, #token: 449182, token usage: 0.46, cuda graph: True, gen throughput (token/s): 2239.18, #queue-req: 0, 
[2025-11-07 14:26:39 TP0] Decode batch, #running-req: 116, #token: 453822, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2238.52, #queue-req: 0, 
[2025-11-07 14:26:41 TP0] Decode batch, #running-req: 116, #token: 458462, token usage: 0.47, cuda graph: True, gen throughput (token/s): 2230.82, #queue-req: 0, 
[2025-11-07 14:26:43 TP0] Decode batch, #running-req: 116, #token: 463102, token usage: 0.48, cuda graph: True, gen throughput (token/s): 2230.49, #queue-req: 0, 
[2025-11-07 14:26:44] INFO:     127.0.0.1:38406 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-07 14:26:45] INFO:     127.0.0.1:56166 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-07 14:27:01] INFO:     127.0.0.1:37920 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-07 14:27:07] INFO:     127.0.0.1:52914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:07 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:27:09] INFO:     127.0.0.1:52916 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:52932 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:27:09] INFO:     127.0.0.1:52948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:52954 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:52960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:52962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:52966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:52978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:52990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:52994 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:53000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:53002 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:53010 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:53016 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:53028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:53036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 8, 
[2025-11-07 14:27:09] INFO:     127.0.0.1:53038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:53040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:53054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:53056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:53058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:09] INFO:     127.0.0.1:53066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53096 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53110 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 22, 
[2025-11-07 14:27:10] INFO:     127.0.0.1:53168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53220 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53394 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10] INFO:     127.0.0.1:53408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-07 14:27:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-07 14:27:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-07 14:27:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-07 14:27:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-07 14:27:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-11-07 14:27:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-11-07 14:27:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-11-07 14:27:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-11-07 14:27:18 TP0] Prefill batch, #new-seq: 6, #new-token: 15989, #cached-token: 3217, token usage: 0.19, #running-req: 56, #queue-req: 2, 
[2025-11-07 14:27:19 TP0] Prefill batch, #new-seq: 2, #new-token: 6398, #cached-token: 4, token usage: 0.20, #running-req: 62, #queue-req: 0, 
[2025-11-07 14:27:20 TP0] Decode batch, #running-req: 64, #token: 204840, token usage: 0.21, cuda graph: True, gen throughput (token/s): 24.94, #queue-req: 0, 
[2025-11-07 14:27:21 TP0] Decode batch, #running-req: 64, #token: 207400, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1547.31, #queue-req: 0, 
[2025-11-07 14:27:23 TP0] Decode batch, #running-req: 64, #token: 209960, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1521.75, #queue-req: 0, 
[2025-11-07 14:27:25 TP0] Decode batch, #running-req: 64, #token: 212520, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1509.65, #queue-req: 0, 
[2025-11-07 14:27:26 TP0] Decode batch, #running-req: 64, #token: 215080, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1501.72, #queue-req: 0, 
[2025-11-07 14:27:28 TP0] Decode batch, #running-req: 64, #token: 217640, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1498.88, #queue-req: 0, 
[2025-11-07 14:27:30 TP0] Decode batch, #running-req: 64, #token: 220200, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1494.28, #queue-req: 0, 
[2025-11-07 14:27:32 TP0] Decode batch, #running-req: 64, #token: 222760, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1491.23, #queue-req: 0, 
[2025-11-07 14:27:33 TP0] Decode batch, #running-req: 64, #token: 225320, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1487.04, #queue-req: 0, 
[2025-11-07 14:27:35 TP0] Decode batch, #running-req: 64, #token: 227880, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1481.39, #queue-req: 0, 
[2025-11-07 14:27:37 TP0] Decode batch, #running-req: 64, #token: 230440, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1480.59, #queue-req: 0, 
[2025-11-07 14:27:38 TP0] Decode batch, #running-req: 64, #token: 233000, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1480.68, #queue-req: 0, 
[2025-11-07 14:27:40 TP0] Decode batch, #running-req: 64, #token: 235560, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1484.45, #queue-req: 0, 
[2025-11-07 14:27:42 TP0] Decode batch, #running-req: 64, #token: 238120, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1481.97, #queue-req: 0, 
[2025-11-07 14:27:44 TP0] Decode batch, #running-req: 64, #token: 240680, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1482.40, #queue-req: 0, 
[2025-11-07 14:27:45 TP0] Decode batch, #running-req: 64, #token: 243240, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1477.46, #queue-req: 0, 
[2025-11-07 14:27:47 TP0] Decode batch, #running-req: 64, #token: 245800, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1477.59, #queue-req: 0, 
[2025-11-07 14:27:49 TP0] Decode batch, #running-req: 64, #token: 248360, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1474.10, #queue-req: 0, 
[2025-11-07 14:27:51 TP0] Decode batch, #running-req: 64, #token: 250920, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1469.66, #queue-req: 0, 
[2025-11-07 14:27:52 TP0] Decode batch, #running-req: 64, #token: 253480, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1470.36, #queue-req: 0, 
[2025-11-07 14:27:54] INFO:     127.0.0.1:32934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54 TP0] Prefill batch, #new-seq: 1, #new-token: 3195, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:27:54] INFO:     127.0.0.1:32938 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:32946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:32960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:32972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:32988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-07 14:27:54] INFO:     127.0.0.1:33100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33124 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33140 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33150 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33214 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33302 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33348 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33358 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 43, 
[2025-11-07 14:27:54] INFO:     127.0.0.1:33472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33498 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33514 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:54] INFO:     127.0.0.1:33530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:27:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-07 14:27:56 TP0] Prefill batch, #new-seq: 6, #new-token: 15981, #cached-token: 3225, token usage: 0.06, #running-req: 16, #queue-req: 42, 
[2025-11-07 14:27:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 22, #queue-req: 37, 
[2025-11-07 14:27:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.09, #running-req: 27, #queue-req: 32, 
[2025-11-07 14:27:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.11, #running-req: 32, #queue-req: 27, 
[2025-11-07 14:27:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.12, #running-req: 37, #queue-req: 22, 
[2025-11-07 14:28:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.14, #running-req: 42, #queue-req: 17, 
[2025-11-07 14:28:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 47, #queue-req: 12, 
[2025-11-07 14:28:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.17, #running-req: 52, #queue-req: 7, 
[2025-11-07 14:28:03 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.19, #running-req: 57, #queue-req: 2, 
[2025-11-07 14:28:03 TP0] Prefill batch, #new-seq: 2, #new-token: 6393, #cached-token: 9, token usage: 0.20, #running-req: 62, #queue-req: 0, 
[2025-11-07 14:28:05 TP0] Decode batch, #running-req: 64, #token: 204823, token usage: 0.21, cuda graph: True, gen throughput (token/s): 208.95, #queue-req: 0, 
[2025-11-07 14:28:06 TP0] Decode batch, #running-req: 64, #token: 207383, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1547.37, #queue-req: 0, 
[2025-11-07 14:28:08 TP0] Decode batch, #running-req: 64, #token: 209943, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1513.96, #queue-req: 0, 
[2025-11-07 14:28:10 TP0] Decode batch, #running-req: 64, #token: 212503, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1507.65, #queue-req: 0, 
[2025-11-07 14:28:11 TP0] Decode batch, #running-req: 64, #token: 215063, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1499.93, #queue-req: 0, 
[2025-11-07 14:28:13 TP0] Decode batch, #running-req: 64, #token: 217623, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1494.79, #queue-req: 0, 
[2025-11-07 14:28:15 TP0] Decode batch, #running-req: 64, #token: 220183, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1491.90, #queue-req: 0, 
[2025-11-07 14:28:16 TP0] Decode batch, #running-req: 64, #token: 222743, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1487.28, #queue-req: 0, 
[2025-11-07 14:28:18 TP0] Decode batch, #running-req: 64, #token: 225303, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1487.19, #queue-req: 0, 
[2025-11-07 14:28:20 TP0] Decode batch, #running-req: 64, #token: 227863, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1485.84, #queue-req: 0, 
[2025-11-07 14:28:22 TP0] Decode batch, #running-req: 64, #token: 230423, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1481.20, #queue-req: 0, 
[2025-11-07 14:28:23 TP0] Decode batch, #running-req: 64, #token: 232983, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1483.78, #queue-req: 0, 
[2025-11-07 14:28:25 TP0] Decode batch, #running-req: 64, #token: 235543, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1484.45, #queue-req: 0, 
[2025-11-07 14:28:27 TP0] Decode batch, #running-req: 64, #token: 238103, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1482.66, #queue-req: 0, 
[2025-11-07 14:28:29 TP0] Decode batch, #running-req: 64, #token: 240663, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1481.31, #queue-req: 0, 
[2025-11-07 14:28:30 TP0] Decode batch, #running-req: 64, #token: 243223, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1476.85, #queue-req: 0, 
[2025-11-07 14:28:32 TP0] Decode batch, #running-req: 64, #token: 245783, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1476.87, #queue-req: 0, 
[2025-11-07 14:28:34 TP0] Decode batch, #running-req: 64, #token: 248343, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1470.42, #queue-req: 0, 
[2025-11-07 14:28:35 TP0] Decode batch, #running-req: 64, #token: 250903, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1470.98, #queue-req: 0, 
[2025-11-07 14:28:37 TP0] Decode batch, #running-req: 64, #token: 253463, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1470.76, #queue-req: 0, 
[2025-11-07 14:28:39] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39 TP0] Prefill batch, #new-seq: 1, #new-token: 3191, #cached-token: 10, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:28:39] INFO:     127.0.0.1:32792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32798 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32812 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32816 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32878 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32900 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 9, 
[2025-11-07 14:28:39] INFO:     127.0.0.1:32922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32962 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32970 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32974 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32986 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:32990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33066 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33094 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33134 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33194 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 40, 
[2025-11-07 14:28:39] INFO:     127.0.0.1:33218 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33228 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33304 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:39] INFO:     127.0.0.1:33324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:28:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-07 14:28:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-07 14:28:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-07 14:28:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-07 14:28:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-07 14:28:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-11-07 14:28:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-11-07 14:28:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-11-07 14:28:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-11-07 14:28:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-11-07 14:28:48 TP0] Prefill batch, #new-seq: 3, #new-token: 9592, #cached-token: 11, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-11-07 14:28:50 TP0] Decode batch, #running-req: 64, #token: 204837, token usage: 0.21, cuda graph: True, gen throughput (token/s): 208.03, #queue-req: 0, 
[2025-11-07 14:28:51 TP0] Decode batch, #running-req: 64, #token: 207397, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1537.82, #queue-req: 0, 
[2025-11-07 14:28:53 TP0] Decode batch, #running-req: 64, #token: 209957, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1515.11, #queue-req: 0, 
[2025-11-07 14:28:55 TP0] Decode batch, #running-req: 64, #token: 212517, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1506.86, #queue-req: 0, 
[2025-11-07 14:28:56 TP0] Decode batch, #running-req: 64, #token: 215077, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1497.01, #queue-req: 0, 
[2025-11-07 14:28:58 TP0] Decode batch, #running-req: 64, #token: 217637, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1491.41, #queue-req: 0, 
[2025-11-07 14:29:00 TP0] Decode batch, #running-req: 64, #token: 220197, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1487.05, #queue-req: 0, 
[2025-11-07 14:29:01 TP0] Decode batch, #running-req: 64, #token: 222757, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1481.08, #queue-req: 0, 
[2025-11-07 14:29:03 TP0] Decode batch, #running-req: 64, #token: 225317, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1477.01, #queue-req: 0, 
[2025-11-07 14:29:05 TP0] Decode batch, #running-req: 64, #token: 227877, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1478.27, #queue-req: 0, 
[2025-11-07 14:29:07 TP0] Decode batch, #running-req: 64, #token: 230437, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1477.08, #queue-req: 0, 
[2025-11-07 14:29:08 TP0] Decode batch, #running-req: 64, #token: 232997, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1478.74, #queue-req: 0, 
[2025-11-07 14:29:10 TP0] Decode batch, #running-req: 64, #token: 235557, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1477.47, #queue-req: 0, 
[2025-11-07 14:29:12 TP0] Decode batch, #running-req: 64, #token: 238117, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1479.23, #queue-req: 0, 
[2025-11-07 14:29:14 TP0] Decode batch, #running-req: 64, #token: 240677, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1478.62, #queue-req: 0, 
[2025-11-07 14:29:15 TP0] Decode batch, #running-req: 64, #token: 243237, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1473.23, #queue-req: 0, 
[2025-11-07 14:29:17 TP0] Decode batch, #running-req: 64, #token: 245797, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1472.15, #queue-req: 0, 
[2025-11-07 14:29:19 TP0] Decode batch, #running-req: 64, #token: 248357, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1468.45, #queue-req: 0, 
[2025-11-07 14:29:21 TP0] Decode batch, #running-req: 64, #token: 250917, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1471.27, #queue-req: 0, 
[2025-11-07 14:29:22 TP0] Decode batch, #running-req: 64, #token: 253477, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1467.72, #queue-req: 0, 
[2025-11-07 14:29:24] INFO:     127.0.0.1:44226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:29:24] INFO:     127.0.0.1:44236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44254 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44268 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44276 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44298 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44306 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44356 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44372 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24 TP0] Prefill batch, #new-seq: 6, #new-token: 15987, #cached-token: 3219, token usage: 0.01, #running-req: 1, #queue-req: 9, 
[2025-11-07 14:29:24] INFO:     127.0.0.1:44384 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44388 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44428 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44484 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44566 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44586 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44644 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44656 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44666 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44694 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44706 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44710 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.02, #running-req: 7, #queue-req: 39, 
[2025-11-07 14:29:24] INFO:     127.0.0.1:44720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44754 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44764 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44790 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44806 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:24] INFO:     127.0.0.1:44820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:29:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.04, #running-req: 12, #queue-req: 47, 
[2025-11-07 14:29:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.06, #running-req: 17, #queue-req: 42, 
[2025-11-07 14:29:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 22, #queue-req: 37, 
[2025-11-07 14:29:28 TP0] Prefill batch, #new-seq: 7, #new-token: 15993, #cached-token: 6414, token usage: 0.10, #running-req: 27, #queue-req: 30, 
[2025-11-07 14:29:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.11, #running-req: 34, #queue-req: 25, 
[2025-11-07 14:29:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.13, #running-req: 39, #queue-req: 20, 
[2025-11-07 14:29:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.14, #running-req: 44, #queue-req: 15, 
[2025-11-07 14:29:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.16, #running-req: 49, #queue-req: 10, 
[2025-11-07 14:29:32 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.18, #running-req: 54, #queue-req: 5, 
[2025-11-07 14:29:33 TP0] Prefill batch, #new-seq: 5, #new-token: 12792, #cached-token: 3213, token usage: 0.19, #running-req: 59, #queue-req: 0, 
[2025-11-07 14:29:34 TP0] Decode batch, #running-req: 64, #token: 201626, token usage: 0.21, cuda graph: True, gen throughput (token/s): 218.97, #queue-req: 0, 
[2025-11-07 14:29:36 TP0] Decode batch, #running-req: 64, #token: 204186, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1533.29, #queue-req: 0, 
[2025-11-07 14:29:37 TP0] Decode batch, #running-req: 64, #token: 206746, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1503.25, #queue-req: 0, 
[2025-11-07 14:29:39 TP0] Decode batch, #running-req: 64, #token: 209306, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1495.84, #queue-req: 0, 
[2025-11-07 14:29:41 TP0] Decode batch, #running-req: 64, #token: 211866, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1497.13, #queue-req: 0, 
[2025-11-07 14:29:42 TP0] Decode batch, #running-req: 64, #token: 214426, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1495.96, #queue-req: 0, 
[2025-11-07 14:29:44 TP0] Decode batch, #running-req: 64, #token: 216986, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1494.57, #queue-req: 0, 
[2025-11-07 14:29:46 TP0] Decode batch, #running-req: 64, #token: 219546, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1491.46, #queue-req: 0, 
[2025-11-07 14:29:48 TP0] Decode batch, #running-req: 64, #token: 222106, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1488.95, #queue-req: 0, 
[2025-11-07 14:29:49 TP0] Decode batch, #running-req: 64, #token: 224666, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1486.65, #queue-req: 0, 
[2025-11-07 14:29:51 TP0] Decode batch, #running-req: 64, #token: 227226, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1484.00, #queue-req: 0, 
[2025-11-07 14:29:53 TP0] Decode batch, #running-req: 64, #token: 229786, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1480.55, #queue-req: 0, 
[2025-11-07 14:29:55 TP0] Decode batch, #running-req: 64, #token: 232346, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1485.11, #queue-req: 0, 
[2025-11-07 14:29:56 TP0] Decode batch, #running-req: 64, #token: 234906, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1484.78, #queue-req: 0, 
[2025-11-07 14:29:58 TP0] Decode batch, #running-req: 64, #token: 237466, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1483.48, #queue-req: 0, 
[2025-11-07 14:30:00 TP0] Decode batch, #running-req: 64, #token: 240026, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1482.59, #queue-req: 0, 
[2025-11-07 14:30:01 TP0] Decode batch, #running-req: 64, #token: 242586, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1479.69, #queue-req: 0, 
[2025-11-07 14:30:03 TP0] Decode batch, #running-req: 64, #token: 245146, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1480.26, #queue-req: 0, 
[2025-11-07 14:30:05 TP0] Decode batch, #running-req: 64, #token: 247706, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1476.61, #queue-req: 0, 
[2025-11-07 14:30:07 TP0] Decode batch, #running-req: 64, #token: 250266, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1478.13, #queue-req: 0, 
[2025-11-07 14:30:08] INFO:     127.0.0.1:59770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:30:08] INFO:     127.0.0.1:59778 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59796 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59834 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59844 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59902 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.00, #running-req: 1, #queue-req: 11, 
[2025-11-07 14:30:08] INFO:     127.0.0.1:59942 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:59988 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:60000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:60006 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:60020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:60034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:08] INFO:     127.0.0.1:60042 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60072 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60224 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60226 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.02, #running-req: 6, #queue-req: 39, 
[2025-11-07 14:30:09] INFO:     127.0.0.1:60242 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60256 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09] INFO:     127.0.0.1:60312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:09 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-07 14:30:10 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-07 14:30:11 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-07 14:30:12 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-07 14:30:13 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-07 14:30:14 TP0] Prefill batch, #new-seq: 6, #new-token: 15991, #cached-token: 3215, token usage: 0.12, #running-req: 36, #queue-req: 22, 
[2025-11-07 14:30:14 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.14, #running-req: 42, #queue-req: 17, 
[2025-11-07 14:30:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.15, #running-req: 47, #queue-req: 12, 
[2025-11-07 14:30:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15980, #cached-token: 25, token usage: 0.17, #running-req: 52, #queue-req: 7, 
[2025-11-07 14:30:17 TP0] Prefill batch, #new-seq: 6, #new-token: 15985, #cached-token: 3221, token usage: 0.19, #running-req: 57, #queue-req: 1, 
[2025-11-07 14:30:18 TP0] Prefill batch, #new-seq: 1, #new-token: 3193, #cached-token: 8, token usage: 0.21, #running-req: 63, #queue-req: 0, 
[2025-11-07 14:30:19 TP0] Decode batch, #running-req: 64, #token: 204826, token usage: 0.21, cuda graph: True, gen throughput (token/s): 212.55, #queue-req: 0, 
[2025-11-07 14:30:20 TP0] Decode batch, #running-req: 64, #token: 207386, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1533.48, #queue-req: 0, 
[2025-11-07 14:30:22 TP0] Decode batch, #running-req: 64, #token: 209946, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1507.49, #queue-req: 0, 
[2025-11-07 14:30:24 TP0] Decode batch, #running-req: 64, #token: 212506, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1503.67, #queue-req: 0, 
[2025-11-07 14:30:25 TP0] Decode batch, #running-req: 64, #token: 215066, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1497.31, #queue-req: 0, 
[2025-11-07 14:30:27 TP0] Decode batch, #running-req: 64, #token: 217626, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1492.26, #queue-req: 0, 
[2025-11-07 14:30:29 TP0] Decode batch, #running-req: 64, #token: 220186, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1488.64, #queue-req: 0, 
[2025-11-07 14:30:31 TP0] Decode batch, #running-req: 64, #token: 222746, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1485.41, #queue-req: 0, 
[2025-11-07 14:30:32 TP0] Decode batch, #running-req: 64, #token: 225306, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1483.23, #queue-req: 0, 
[2025-11-07 14:30:34 TP0] Decode batch, #running-req: 64, #token: 227866, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1483.37, #queue-req: 0, 
[2025-11-07 14:30:36 TP0] Decode batch, #running-req: 64, #token: 230426, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1481.66, #queue-req: 0, 
[2025-11-07 14:30:38 TP0] Decode batch, #running-req: 64, #token: 232986, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1483.68, #queue-req: 0, 
[2025-11-07 14:30:39 TP0] Decode batch, #running-req: 64, #token: 235546, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1488.11, #queue-req: 0, 
[2025-11-07 14:30:41 TP0] Decode batch, #running-req: 64, #token: 238106, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1490.41, #queue-req: 0, 
[2025-11-07 14:30:43 TP0] Decode batch, #running-req: 64, #token: 240666, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1486.73, #queue-req: 0, 
[2025-11-07 14:30:44 TP0] Decode batch, #running-req: 64, #token: 243226, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1484.05, #queue-req: 0, 
[2025-11-07 14:30:46 TP0] Decode batch, #running-req: 64, #token: 245786, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1482.85, #queue-req: 0, 
[2025-11-07 14:30:48 TP0] Decode batch, #running-req: 64, #token: 248346, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1477.64, #queue-req: 0, 
[2025-11-07 14:30:50 TP0] Decode batch, #running-req: 64, #token: 250906, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1478.80, #queue-req: 0, 
[2025-11-07 14:30:51 TP0] Decode batch, #running-req: 64, #token: 253466, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1473.86, #queue-req: 0, 
[2025-11-07 14:30:53] INFO:     127.0.0.1:54468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:30:53] INFO:     127.0.0.1:54474 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54490 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54512 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54516 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54532 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54558 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54574 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54592 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-07 14:30:53] INFO:     127.0.0.1:54622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54626 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54638 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54686 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54696 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54718 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54724 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54734 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54736 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54746 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54760 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54788 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54832 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54872 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54898 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54906 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54940 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54950 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 40, 
[2025-11-07 14:30:53] INFO:     127.0.0.1:54982 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:54996 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:55008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:55014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:55026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:55032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:55040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:55050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:55056 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:55064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:55074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:53] INFO:     127.0.0.1:55090 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:30:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-07 14:30:55 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-07 14:30:56 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-07 14:30:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-07 14:30:57 TP0] Prefill batch, #new-seq: 5, #new-token: 15994, #cached-token: 11, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-07 14:30:58 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-11-07 14:30:59 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-11-07 14:31:00 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-11-07 14:31:01 TP0] Prefill batch, #new-seq: 5, #new-token: 15984, #cached-token: 21, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-11-07 14:31:02 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-11-07 14:31:02 TP0] Prefill batch, #new-seq: 3, #new-token: 9597, #cached-token: 6, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-11-07 14:31:04 TP0] Decode batch, #running-req: 64, #token: 204840, token usage: 0.21, cuda graph: True, gen throughput (token/s): 206.84, #queue-req: 0, 
[2025-11-07 14:31:05 TP0] Decode batch, #running-req: 64, #token: 207400, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1541.66, #queue-req: 0, 
[2025-11-07 14:31:07 TP0] Decode batch, #running-req: 64, #token: 209960, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1510.70, #queue-req: 0, 
[2025-11-07 14:31:09 TP0] Decode batch, #running-req: 64, #token: 212520, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1500.92, #queue-req: 0, 
[2025-11-07 14:31:10 TP0] Decode batch, #running-req: 64, #token: 215080, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1495.19, #queue-req: 0, 
[2025-11-07 14:31:12 TP0] Decode batch, #running-req: 64, #token: 217640, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1494.07, #queue-req: 0, 
[2025-11-07 14:31:14 TP0] Decode batch, #running-req: 64, #token: 220200, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1488.15, #queue-req: 0, 
[2025-11-07 14:31:16 TP0] Decode batch, #running-req: 64, #token: 222760, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1486.30, #queue-req: 0, 
[2025-11-07 14:31:17 TP0] Decode batch, #running-req: 64, #token: 225320, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1487.47, #queue-req: 0, 
[2025-11-07 14:31:19 TP0] Decode batch, #running-req: 64, #token: 227880, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1487.43, #queue-req: 0, 
[2025-11-07 14:31:21 TP0] Decode batch, #running-req: 64, #token: 230440, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1485.91, #queue-req: 0, 
[2025-11-07 14:31:23 TP0] Decode batch, #running-req: 64, #token: 233000, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1487.13, #queue-req: 0, 
[2025-11-07 14:31:24 TP0] Decode batch, #running-req: 64, #token: 235560, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1491.28, #queue-req: 0, 
[2025-11-07 14:31:26 TP0] Decode batch, #running-req: 64, #token: 238120, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1491.52, #queue-req: 0, 
[2025-11-07 14:31:28 TP0] Decode batch, #running-req: 64, #token: 240680, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1488.16, #queue-req: 0, 
[2025-11-07 14:31:29 TP0] Decode batch, #running-req: 64, #token: 243240, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1484.58, #queue-req: 0, 
[2025-11-07 14:31:31 TP0] Decode batch, #running-req: 64, #token: 245800, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1482.97, #queue-req: 0, 
[2025-11-07 14:31:33 TP0] Decode batch, #running-req: 64, #token: 248360, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1474.26, #queue-req: 0, 
[2025-11-07 14:31:35 TP0] Decode batch, #running-req: 64, #token: 250920, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1481.03, #queue-req: 0, 
[2025-11-07 14:31:36 TP0] Decode batch, #running-req: 64, #token: 253480, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1476.61, #queue-req: 0, 
[2025-11-07 14:31:38] INFO:     127.0.0.1:48028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:31:38] INFO:     127.0.0.1:48034 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48040 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48130 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48136 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48152 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48158 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48172 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 11, 
[2025-11-07 14:31:38] INFO:     127.0.0.1:48182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48216 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48232 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48246 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48260 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48278 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48286 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48300 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48368 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48380 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48404 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48412 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48478 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48496 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 40, 
[2025-11-07 14:31:38] INFO:     127.0.0.1:48502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48534 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48540 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48570 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48576 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:38] INFO:     127.0.0.1:48578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:31:39 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 48, 
[2025-11-07 14:31:40 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.05, #running-req: 16, #queue-req: 43, 
[2025-11-07 14:31:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.07, #running-req: 21, #queue-req: 38, 
[2025-11-07 14:31:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.09, #running-req: 26, #queue-req: 33, 
[2025-11-07 14:31:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.10, #running-req: 31, #queue-req: 28, 
[2025-11-07 14:31:43 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.12, #running-req: 36, #queue-req: 23, 
[2025-11-07 14:31:44 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.13, #running-req: 41, #queue-req: 18, 
[2025-11-07 14:31:45 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.15, #running-req: 46, #queue-req: 13, 
[2025-11-07 14:31:46 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.17, #running-req: 51, #queue-req: 8, 
[2025-11-07 14:31:47 TP0] Prefill batch, #new-seq: 5, #new-token: 15983, #cached-token: 22, token usage: 0.18, #running-req: 56, #queue-req: 3, 
[2025-11-07 14:31:47 TP0] Prefill batch, #new-seq: 3, #new-token: 9593, #cached-token: 10, token usage: 0.20, #running-req: 61, #queue-req: 0, 
[2025-11-07 14:31:49 TP0] Decode batch, #running-req: 64, #token: 204830, token usage: 0.21, cuda graph: True, gen throughput (token/s): 206.56, #queue-req: 0, 
[2025-11-07 14:31:50 TP0] Decode batch, #running-req: 64, #token: 207390, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1539.23, #queue-req: 0, 
[2025-11-07 14:31:52 TP0] Decode batch, #running-req: 64, #token: 209950, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1514.84, #queue-req: 0, 
[2025-11-07 14:31:54 TP0] Decode batch, #running-req: 64, #token: 212510, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1503.94, #queue-req: 0, 
[2025-11-07 14:31:55 TP0] Decode batch, #running-req: 64, #token: 215070, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1495.93, #queue-req: 0, 
[2025-11-07 14:31:57 TP0] Decode batch, #running-req: 64, #token: 217630, token usage: 0.22, cuda graph: True, gen throughput (token/s): 1489.62, #queue-req: 0, 
[2025-11-07 14:31:59 TP0] Decode batch, #running-req: 64, #token: 220190, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1485.67, #queue-req: 0, 
[2025-11-07 14:32:01 TP0] Decode batch, #running-req: 64, #token: 222750, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1478.80, #queue-req: 0, 
[2025-11-07 14:32:02 TP0] Decode batch, #running-req: 64, #token: 225310, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1479.53, #queue-req: 0, 
[2025-11-07 14:32:04 TP0] Decode batch, #running-req: 64, #token: 227870, token usage: 0.23, cuda graph: True, gen throughput (token/s): 1481.20, #queue-req: 0, 
[2025-11-07 14:32:06 TP0] Decode batch, #running-req: 64, #token: 230430, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1478.85, #queue-req: 0, 
[2025-11-07 14:32:08 TP0] Decode batch, #running-req: 64, #token: 232990, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1482.32, #queue-req: 0, 
[2025-11-07 14:32:09 TP0] Decode batch, #running-req: 64, #token: 235550, token usage: 0.24, cuda graph: True, gen throughput (token/s): 1484.99, #queue-req: 0, 
[2025-11-07 14:32:11 TP0] Decode batch, #running-req: 64, #token: 238110, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1486.34, #queue-req: 0, 
[2025-11-07 14:32:13 TP0] Decode batch, #running-req: 64, #token: 240670, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1481.95, #queue-req: 0, 
[2025-11-07 14:32:14 TP0] Decode batch, #running-req: 64, #token: 243230, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1480.83, #queue-req: 0, 
[2025-11-07 14:32:16 TP0] Decode batch, #running-req: 64, #token: 245790, token usage: 0.25, cuda graph: True, gen throughput (token/s): 1482.96, #queue-req: 0, 
[2025-11-07 14:32:18 TP0] Decode batch, #running-req: 64, #token: 248350, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1479.38, #queue-req: 0, 
[2025-11-07 14:32:20 TP0] Decode batch, #running-req: 64, #token: 250910, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1476.16, #queue-req: 0, 
[2025-11-07 14:32:21 TP0] Decode batch, #running-req: 64, #token: 253470, token usage: 0.26, cuda graph: True, gen throughput (token/s): 1477.45, #queue-req: 0, 
[2025-11-07 14:32:23] INFO:     127.0.0.1:36028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:32:23] INFO:     127.0.0.1:36030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36054 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36070 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36082 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36084 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36086 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36100 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36112 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36138 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36154 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36164 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36168 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-07 14:32:23] INFO:     127.0.0.1:36176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36178 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36184 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36196 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36208 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36222 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36236 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36252 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36258 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36280 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36282 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36284 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36292 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36296 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36324 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36334 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36378 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36414 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36430 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23] INFO:     127.0.0.1:36476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:32:23 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 6, #queue-req: 40, 
[2025-11-07 14:32:24 TP0] Prefill batch, #new-seq: 6, #new-token: 15988, #cached-token: 3218, token usage: 0.04, #running-req: 11, #queue-req: 35, 
[2025-11-07 14:32:25 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.06, #running-req: 17, #queue-req: 30, 
[2025-11-07 14:32:26 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.07, #running-req: 22, #queue-req: 25, 
[2025-11-07 14:32:27 TP0] Prefill batch, #new-seq: 5, #new-token: 15982, #cached-token: 23, token usage: 0.09, #running-req: 27, #queue-req: 20, 
[2025-11-07 14:32:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.11, #running-req: 32, #queue-req: 15, 
[2025-11-07 14:32:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.12, #running-req: 37, #queue-req: 10, 
[2025-11-07 14:32:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.14, #running-req: 42, #queue-req: 5, 
[2025-11-07 14:32:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15981, #cached-token: 24, token usage: 0.15, #running-req: 47, #queue-req: 0, 
[2025-11-07 14:32:32 TP0] Decode batch, #running-req: 52, #token: 166426, token usage: 0.17, cuda graph: True, gen throughput (token/s): 251.06, #queue-req: 0, 
[2025-11-07 14:32:33 TP0] Decode batch, #running-req: 52, #token: 168506, token usage: 0.17, cuda graph: True, gen throughput (token/s): 1290.04, #queue-req: 0, 
[2025-11-07 14:32:35 TP0] Decode batch, #running-req: 52, #token: 170586, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1271.88, #queue-req: 0, 
[2025-11-07 14:32:36 TP0] Decode batch, #running-req: 52, #token: 172666, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1266.34, #queue-req: 0, 
[2025-11-07 14:32:38 TP0] Decode batch, #running-req: 52, #token: 174746, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1263.66, #queue-req: 0, 
[2025-11-07 14:32:40 TP0] Decode batch, #running-req: 52, #token: 176826, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1262.51, #queue-req: 0, 
[2025-11-07 14:32:41 TP0] Decode batch, #running-req: 52, #token: 178906, token usage: 0.18, cuda graph: True, gen throughput (token/s): 1262.40, #queue-req: 0, 
[2025-11-07 14:32:43 TP0] Decode batch, #running-req: 52, #token: 180986, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1261.50, #queue-req: 0, 
[2025-11-07 14:32:45 TP0] Decode batch, #running-req: 52, #token: 183066, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1258.85, #queue-req: 0, 
[2025-11-07 14:32:46 TP0] Decode batch, #running-req: 52, #token: 185146, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1259.28, #queue-req: 0, 
[2025-11-07 14:32:48 TP0] Decode batch, #running-req: 52, #token: 187226, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1260.78, #queue-req: 0, 
[2025-11-07 14:32:50 TP0] Decode batch, #running-req: 52, #token: 189306, token usage: 0.19, cuda graph: True, gen throughput (token/s): 1261.55, #queue-req: 0, 
[2025-11-07 14:32:51 TP0] Decode batch, #running-req: 52, #token: 191386, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1256.10, #queue-req: 0, 
[2025-11-07 14:32:53 TP0] Decode batch, #running-req: 52, #token: 193466, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1254.82, #queue-req: 0, 
[2025-11-07 14:32:55 TP0] Decode batch, #running-req: 52, #token: 195546, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1253.16, #queue-req: 0, 
[2025-11-07 14:32:56 TP0] Decode batch, #running-req: 52, #token: 197626, token usage: 0.20, cuda graph: True, gen throughput (token/s): 1251.44, #queue-req: 0, 
[2025-11-07 14:32:58 TP0] Decode batch, #running-req: 52, #token: 199706, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1250.63, #queue-req: 0, 
[2025-11-07 14:33:00 TP0] Decode batch, #running-req: 52, #token: 201786, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1247.21, #queue-req: 0, 
[2025-11-07 14:33:01 TP0] Decode batch, #running-req: 52, #token: 203866, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1248.19, #queue-req: 0, 
[2025-11-07 14:33:03 TP0] Decode batch, #running-req: 52, #token: 205946, token usage: 0.21, cuda graph: True, gen throughput (token/s): 1245.83, #queue-req: 0, 
[2025-11-07 14:33:05] INFO:     127.0.0.1:49576 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-07 14:33:05] INFO:     127.0.0.1:49582 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-07 14:33:22] INFO:     127.0.0.1:46286 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-07 14:33:28] INFO:     127.0.0.1:46538 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:28 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:33:28 TP0] Decode batch, #running-req: 1, #token: 3203, token usage: 0.00, cuda graph: True, gen throughput (token/s): 81.28, #queue-req: 0, 
[2025-11-07 14:33:30] INFO:     127.0.0.1:46550 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46564 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:33:30] INFO:     127.0.0.1:46580 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46608 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46614 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46620 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46622 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30] INFO:     127.0.0.1:46674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15995, #cached-token: 10, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-07 14:33:30 TP0] Prefill batch, #new-seq: 5, #new-token: 15993, #cached-token: 12, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-07 14:33:31 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-07 14:33:32 TP0] Decode batch, #running-req: 16, #token: 51346, token usage: 0.05, cuda graph: True, gen throughput (token/s): 39.81, #queue-req: 0, 
[2025-11-07 14:33:33 TP0] Decode batch, #running-req: 16, #token: 51986, token usage: 0.05, cuda graph: True, gen throughput (token/s): 620.36, #queue-req: 0, 
[2025-11-07 14:33:34 TP0] Decode batch, #running-req: 16, #token: 52626, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.70, #queue-req: 0, 
[2025-11-07 14:33:35 TP0] Decode batch, #running-req: 16, #token: 53266, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.31, #queue-req: 0, 
[2025-11-07 14:33:36 TP0] Decode batch, #running-req: 16, #token: 53906, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.60, #queue-req: 0, 
[2025-11-07 14:33:37 TP0] Decode batch, #running-req: 16, #token: 54546, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.69, #queue-req: 0, 
[2025-11-07 14:33:39 TP0] Decode batch, #running-req: 16, #token: 55186, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.15, #queue-req: 0, 
[2025-11-07 14:33:40 TP0] Decode batch, #running-req: 16, #token: 55826, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.68, #queue-req: 0, 
[2025-11-07 14:33:41 TP0] Decode batch, #running-req: 16, #token: 56466, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.95, #queue-req: 0, 
[2025-11-07 14:33:42 TP0] Decode batch, #running-req: 16, #token: 57106, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.39, #queue-req: 0, 
[2025-11-07 14:33:43 TP0] Decode batch, #running-req: 16, #token: 57746, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.71, #queue-req: 0, 
[2025-11-07 14:33:44 TP0] Decode batch, #running-req: 16, #token: 58386, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.81, #queue-req: 0, 
[2025-11-07 14:33:45 TP0] Decode batch, #running-req: 16, #token: 59026, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.11, #queue-req: 0, 
[2025-11-07 14:33:46 TP0] Decode batch, #running-req: 16, #token: 59666, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.30, #queue-req: 0, 
[2025-11-07 14:33:47 TP0] Decode batch, #running-req: 16, #token: 60306, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.50, #queue-req: 0, 
[2025-11-07 14:33:48 TP0] Decode batch, #running-req: 16, #token: 60946, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.87, #queue-req: 0, 
[2025-11-07 14:33:49 TP0] Decode batch, #running-req: 16, #token: 61586, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.38, #queue-req: 0, 
[2025-11-07 14:33:50 TP0] Decode batch, #running-req: 16, #token: 62226, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.11, #queue-req: 0, 
[2025-11-07 14:33:51 TP0] Decode batch, #running-req: 16, #token: 62866, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.56, #queue-req: 0, 
[2025-11-07 14:33:52 TP0] Decode batch, #running-req: 16, #token: 63506, token usage: 0.07, cuda graph: True, gen throughput (token/s): 608.25, #queue-req: 0, 
[2025-11-07 14:33:53] INFO:     127.0.0.1:33036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:33:53] INFO:     127.0.0.1:33046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33048 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33078 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33088 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33098 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33106 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33122 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33144 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53] INFO:     127.0.0.1:33148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:33:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-07 14:33:53 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-07 14:33:54 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-07 14:33:56 TP0] Decode batch, #running-req: 16, #token: 51340, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.90, #queue-req: 0, 
[2025-11-07 14:33:57 TP0] Decode batch, #running-req: 16, #token: 51980, token usage: 0.05, cuda graph: True, gen throughput (token/s): 617.03, #queue-req: 0, 
[2025-11-07 14:33:58 TP0] Decode batch, #running-req: 16, #token: 52620, token usage: 0.05, cuda graph: True, gen throughput (token/s): 610.82, #queue-req: 0, 
[2025-11-07 14:33:59 TP0] Decode batch, #running-req: 16, #token: 53260, token usage: 0.05, cuda graph: True, gen throughput (token/s): 608.41, #queue-req: 0, 
[2025-11-07 14:34:00 TP0] Decode batch, #running-req: 16, #token: 53900, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.20, #queue-req: 0, 
[2025-11-07 14:34:01 TP0] Decode batch, #running-req: 16, #token: 54540, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.95, #queue-req: 0, 
[2025-11-07 14:34:02 TP0] Decode batch, #running-req: 16, #token: 55180, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.91, #queue-req: 0, 
[2025-11-07 14:34:03 TP0] Decode batch, #running-req: 16, #token: 55820, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.73, #queue-req: 0, 
[2025-11-07 14:34:04 TP0] Decode batch, #running-req: 16, #token: 56460, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.07, #queue-req: 0, 
[2025-11-07 14:34:05 TP0] Decode batch, #running-req: 16, #token: 57100, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.35, #queue-req: 0, 
[2025-11-07 14:34:06 TP0] Decode batch, #running-req: 16, #token: 57740, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.02, #queue-req: 0, 
[2025-11-07 14:34:08 TP0] Decode batch, #running-req: 16, #token: 58380, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.90, #queue-req: 0, 
[2025-11-07 14:34:09 TP0] Decode batch, #running-req: 16, #token: 59020, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.66, #queue-req: 0, 
[2025-11-07 14:34:10 TP0] Decode batch, #running-req: 16, #token: 59660, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.81, #queue-req: 0, 
[2025-11-07 14:34:11 TP0] Decode batch, #running-req: 16, #token: 60300, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.13, #queue-req: 0, 
[2025-11-07 14:34:12 TP0] Decode batch, #running-req: 16, #token: 60940, token usage: 0.06, cuda graph: True, gen throughput (token/s): 603.51, #queue-req: 0, 
[2025-11-07 14:34:13 TP0] Decode batch, #running-req: 16, #token: 61580, token usage: 0.06, cuda graph: True, gen throughput (token/s): 602.85, #queue-req: 0, 
[2025-11-07 14:34:14 TP0] Decode batch, #running-req: 16, #token: 62220, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.36, #queue-req: 0, 
[2025-11-07 14:34:15 TP0] Decode batch, #running-req: 16, #token: 62860, token usage: 0.06, cuda graph: True, gen throughput (token/s): 602.84, #queue-req: 0, 
[2025-11-07 14:34:16 TP0] Decode batch, #running-req: 16, #token: 63500, token usage: 0.07, cuda graph: True, gen throughput (token/s): 602.06, #queue-req: 0, 
[2025-11-07 14:34:17] INFO:     127.0.0.1:51308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:34:17] INFO:     127.0.0.1:51332 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51336 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51344 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51360 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51390 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17] INFO:     127.0.0.1:51450 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-07 14:34:17 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-07 14:34:18 TP0] Prefill batch, #new-seq: 5, #new-token: 15985, #cached-token: 20, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-07 14:34:20 TP0] Decode batch, #running-req: 16, #token: 51342, token usage: 0.05, cuda graph: True, gen throughput (token/s): 171.43, #queue-req: 0, 
[2025-11-07 14:34:21 TP0] Decode batch, #running-req: 16, #token: 51982, token usage: 0.05, cuda graph: True, gen throughput (token/s): 622.71, #queue-req: 0, 
[2025-11-07 14:34:22 TP0] Decode batch, #running-req: 16, #token: 52622, token usage: 0.05, cuda graph: True, gen throughput (token/s): 623.08, #queue-req: 0, 
[2025-11-07 14:34:23 TP0] Decode batch, #running-req: 16, #token: 53262, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.35, #queue-req: 0, 
[2025-11-07 14:34:24 TP0] Decode batch, #running-req: 16, #token: 53902, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.32, #queue-req: 0, 
[2025-11-07 14:34:25 TP0] Decode batch, #running-req: 16, #token: 54542, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.76, #queue-req: 0, 
[2025-11-07 14:34:26 TP0] Decode batch, #running-req: 16, #token: 55182, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.32, #queue-req: 0, 
[2025-11-07 14:34:27 TP0] Decode batch, #running-req: 16, #token: 55822, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.18, #queue-req: 0, 
[2025-11-07 14:34:28 TP0] Decode batch, #running-req: 16, #token: 56462, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.39, #queue-req: 0, 
[2025-11-07 14:34:29 TP0] Decode batch, #running-req: 16, #token: 57102, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.29, #queue-req: 0, 
[2025-11-07 14:34:30 TP0] Decode batch, #running-req: 16, #token: 57742, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.38, #queue-req: 0, 
[2025-11-07 14:34:31 TP0] Decode batch, #running-req: 16, #token: 58382, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.75, #queue-req: 0, 
[2025-11-07 14:34:32 TP0] Decode batch, #running-req: 16, #token: 59022, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.47, #queue-req: 0, 
[2025-11-07 14:34:33 TP0] Decode batch, #running-req: 16, #token: 59662, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.91, #queue-req: 0, 
[2025-11-07 14:34:34 TP0] Decode batch, #running-req: 16, #token: 60302, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.76, #queue-req: 0, 
[2025-11-07 14:34:35 TP0] Decode batch, #running-req: 16, #token: 60942, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.35, #queue-req: 0, 
[2025-11-07 14:34:37 TP0] Decode batch, #running-req: 16, #token: 61582, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.67, #queue-req: 0, 
[2025-11-07 14:34:38 TP0] Decode batch, #running-req: 16, #token: 62222, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.81, #queue-req: 0, 
[2025-11-07 14:34:39 TP0] Decode batch, #running-req: 16, #token: 62862, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.38, #queue-req: 0, 
[2025-11-07 14:34:40 TP0] Decode batch, #running-req: 16, #token: 63502, token usage: 0.07, cuda graph: True, gen throughput (token/s): 606.32, #queue-req: 0, 
[2025-11-07 14:34:40] INFO:     127.0.0.1:41308 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41316 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:34:41] INFO:     127.0.0.1:41322 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41352 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41364 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41396 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41402 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41406 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41420 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41] INFO:     127.0.0.1:41436 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:34:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-07 14:34:41 TP0] Prefill batch, #new-seq: 5, #new-token: 15989, #cached-token: 16, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-07 14:34:42 TP0] Prefill batch, #new-seq: 5, #new-token: 15992, #cached-token: 13, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-07 14:34:43 TP0] Decode batch, #running-req: 16, #token: 51345, token usage: 0.05, cuda graph: True, gen throughput (token/s): 172.28, #queue-req: 0, 
[2025-11-07 14:34:44 TP0] Decode batch, #running-req: 16, #token: 51985, token usage: 0.05, cuda graph: True, gen throughput (token/s): 616.92, #queue-req: 0, 
[2025-11-07 14:34:45 TP0] Decode batch, #running-req: 16, #token: 52625, token usage: 0.05, cuda graph: True, gen throughput (token/s): 612.54, #queue-req: 0, 
[2025-11-07 14:34:47 TP0] Decode batch, #running-req: 16, #token: 53265, token usage: 0.05, cuda graph: True, gen throughput (token/s): 611.29, #queue-req: 0, 
[2025-11-07 14:34:48 TP0] Decode batch, #running-req: 16, #token: 53905, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.42, #queue-req: 0, 
[2025-11-07 14:34:49 TP0] Decode batch, #running-req: 16, #token: 54545, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.78, #queue-req: 0, 
[2025-11-07 14:34:50 TP0] Decode batch, #running-req: 16, #token: 55185, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.36, #queue-req: 0, 
[2025-11-07 14:34:51 TP0] Decode batch, #running-req: 16, #token: 55825, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.44, #queue-req: 0, 
[2025-11-07 14:34:52 TP0] Decode batch, #running-req: 16, #token: 56465, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.05, #queue-req: 0, 
[2025-11-07 14:34:53 TP0] Decode batch, #running-req: 16, #token: 57105, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.24, #queue-req: 0, 
[2025-11-07 14:34:54 TP0] Decode batch, #running-req: 16, #token: 57745, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.83, #queue-req: 0, 
[2025-11-07 14:34:55 TP0] Decode batch, #running-req: 16, #token: 58385, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.81, #queue-req: 0, 
[2025-11-07 14:34:56 TP0] Decode batch, #running-req: 16, #token: 59025, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.61, #queue-req: 0, 
[2025-11-07 14:34:57 TP0] Decode batch, #running-req: 16, #token: 59665, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.48, #queue-req: 0, 
[2025-11-07 14:34:58 TP0] Decode batch, #running-req: 16, #token: 60305, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.84, #queue-req: 0, 
[2025-11-07 14:34:59 TP0] Decode batch, #running-req: 16, #token: 60945, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.20, #queue-req: 0, 
[2025-11-07 14:35:00 TP0] Decode batch, #running-req: 16, #token: 61585, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.71, #queue-req: 0, 
[2025-11-07 14:35:01 TP0] Decode batch, #running-req: 16, #token: 62225, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.43, #queue-req: 0, 
[2025-11-07 14:35:02 TP0] Decode batch, #running-req: 16, #token: 62865, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.37, #queue-req: 0, 
[2025-11-07 14:35:03 TP0] Decode batch, #running-req: 16, #token: 63505, token usage: 0.07, cuda graph: True, gen throughput (token/s): 610.04, #queue-req: 0, 
[2025-11-07 14:35:04] INFO:     127.0.0.1:44350 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44354 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04 TP0] Prefill batch, #new-seq: 1, #new-token: 3195, #cached-token: 6, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:35:04] INFO:     127.0.0.1:44366 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44382 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44398 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44410 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44422 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44448 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44462 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44472 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44488 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04] INFO:     127.0.0.1:44508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:04 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-07 14:35:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-07 14:35:05 TP0] Prefill batch, #new-seq: 5, #new-token: 15991, #cached-token: 14, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-07 14:35:07 TP0] Decode batch, #running-req: 16, #token: 51340, token usage: 0.05, cuda graph: True, gen throughput (token/s): 171.69, #queue-req: 0, 
[2025-11-07 14:35:08 TP0] Decode batch, #running-req: 16, #token: 51980, token usage: 0.05, cuda graph: True, gen throughput (token/s): 623.92, #queue-req: 0, 
[2025-11-07 14:35:09 TP0] Decode batch, #running-req: 16, #token: 52620, token usage: 0.05, cuda graph: True, gen throughput (token/s): 621.92, #queue-req: 0, 
[2025-11-07 14:35:10 TP0] Decode batch, #running-req: 16, #token: 53260, token usage: 0.05, cuda graph: True, gen throughput (token/s): 621.45, #queue-req: 0, 
[2025-11-07 14:35:11 TP0] Decode batch, #running-req: 16, #token: 53900, token usage: 0.06, cuda graph: True, gen throughput (token/s): 616.43, #queue-req: 0, 
[2025-11-07 14:35:12 TP0] Decode batch, #running-req: 16, #token: 54540, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.73, #queue-req: 0, 
[2025-11-07 14:35:13 TP0] Decode batch, #running-req: 16, #token: 55180, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.15, #queue-req: 0, 
[2025-11-07 14:35:14 TP0] Decode batch, #running-req: 16, #token: 55820, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.53, #queue-req: 0, 
[2025-11-07 14:35:15 TP0] Decode batch, #running-req: 16, #token: 56460, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.18, #queue-req: 0, 
[2025-11-07 14:35:16 TP0] Decode batch, #running-req: 16, #token: 57100, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.22, #queue-req: 0, 
[2025-11-07 14:35:18 TP0] Decode batch, #running-req: 16, #token: 57740, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.60, #queue-req: 0, 
[2025-11-07 14:35:19 TP0] Decode batch, #running-req: 16, #token: 58380, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.88, #queue-req: 0, 
[2025-11-07 14:35:20 TP0] Decode batch, #running-req: 16, #token: 59020, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.47, #queue-req: 0, 
[2025-11-07 14:35:21 TP0] Decode batch, #running-req: 16, #token: 59660, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.55, #queue-req: 0, 
[2025-11-07 14:35:22 TP0] Decode batch, #running-req: 16, #token: 60300, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.17, #queue-req: 0, 
[2025-11-07 14:35:23 TP0] Decode batch, #running-req: 16, #token: 60940, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.81, #queue-req: 0, 
[2025-11-07 14:35:24 TP0] Decode batch, #running-req: 16, #token: 61580, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.31, #queue-req: 0, 
[2025-11-07 14:35:25 TP0] Decode batch, #running-req: 16, #token: 62220, token usage: 0.06, cuda graph: True, gen throughput (token/s): 610.81, #queue-req: 0, 
[2025-11-07 14:35:26 TP0] Decode batch, #running-req: 16, #token: 62860, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.14, #queue-req: 0, 
[2025-11-07 14:35:27 TP0] Decode batch, #running-req: 16, #token: 63500, token usage: 0.07, cuda graph: True, gen throughput (token/s): 613.40, #queue-req: 0, 
[2025-11-07 14:35:28] INFO:     127.0.0.1:38846 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38854 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28 TP0] Prefill batch, #new-seq: 1, #new-token: 3196, #cached-token: 5, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:35:28] INFO:     127.0.0.1:38856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38870 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38896 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38912 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38936 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28] INFO:     127.0.0.1:38992 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15978, #cached-token: 27, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-07 14:35:28 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-07 14:35:29 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-07 14:35:31 TP0] Decode batch, #running-req: 16, #token: 51340, token usage: 0.05, cuda graph: True, gen throughput (token/s): 171.45, #queue-req: 0, 
[2025-11-07 14:35:32 TP0] Decode batch, #running-req: 16, #token: 51980, token usage: 0.05, cuda graph: True, gen throughput (token/s): 621.51, #queue-req: 0, 
[2025-11-07 14:35:33 TP0] Decode batch, #running-req: 16, #token: 52620, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.09, #queue-req: 0, 
[2025-11-07 14:35:34 TP0] Decode batch, #running-req: 16, #token: 53260, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.07, #queue-req: 0, 
[2025-11-07 14:35:35 TP0] Decode batch, #running-req: 16, #token: 53900, token usage: 0.06, cuda graph: True, gen throughput (token/s): 613.55, #queue-req: 0, 
[2025-11-07 14:35:36 TP0] Decode batch, #running-req: 16, #token: 54540, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.10, #queue-req: 0, 
[2025-11-07 14:35:37 TP0] Decode batch, #running-req: 16, #token: 55180, token usage: 0.06, cuda graph: True, gen throughput (token/s): 609.32, #queue-req: 0, 
[2025-11-07 14:35:38 TP0] Decode batch, #running-req: 16, #token: 55820, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.50, #queue-req: 0, 
[2025-11-07 14:35:39 TP0] Decode batch, #running-req: 16, #token: 56460, token usage: 0.06, cuda graph: True, gen throughput (token/s): 616.15, #queue-req: 0, 
[2025-11-07 14:35:40 TP0] Decode batch, #running-req: 16, #token: 57100, token usage: 0.06, cuda graph: True, gen throughput (token/s): 616.52, #queue-req: 0, 
[2025-11-07 14:35:41 TP0] Decode batch, #running-req: 16, #token: 57740, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.60, #queue-req: 0, 
[2025-11-07 14:35:42 TP0] Decode batch, #running-req: 16, #token: 58380, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.16, #queue-req: 0, 
[2025-11-07 14:35:43 TP0] Decode batch, #running-req: 16, #token: 59020, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.68, #queue-req: 0, 
[2025-11-07 14:35:44 TP0] Decode batch, #running-req: 16, #token: 59660, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.86, #queue-req: 0, 
[2025-11-07 14:35:45 TP0] Decode batch, #running-req: 16, #token: 60300, token usage: 0.06, cuda graph: True, gen throughput (token/s): 611.99, #queue-req: 0, 
[2025-11-07 14:35:46 TP0] Decode batch, #running-req: 16, #token: 60940, token usage: 0.06, cuda graph: True, gen throughput (token/s): 612.14, #queue-req: 0, 
[2025-11-07 14:35:47 TP0] Decode batch, #running-req: 16, #token: 61580, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.07, #queue-req: 0, 
[2025-11-07 14:35:48 TP0] Decode batch, #running-req: 16, #token: 62220, token usage: 0.06, cuda graph: True, gen throughput (token/s): 614.54, #queue-req: 0, 
[2025-11-07 14:35:49 TP0] Decode batch, #running-req: 16, #token: 62860, token usage: 0.06, cuda graph: True, gen throughput (token/s): 615.41, #queue-req: 0, 
[2025-11-07 14:35:50 TP0] Decode batch, #running-req: 16, #token: 63500, token usage: 0.07, cuda graph: True, gen throughput (token/s): 613.91, #queue-req: 0, 
[2025-11-07 14:35:51] INFO:     127.0.0.1:53762 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51 TP0] Prefill batch, #new-seq: 1, #new-token: 3197, #cached-token: 4, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:35:51] INFO:     127.0.0.1:53780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53792 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53824 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53826 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53828 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53830 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53842 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53858 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53882 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51] INFO:     127.0.0.1:53894 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:35:51 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-07 14:35:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-07 14:35:52 TP0] Prefill batch, #new-seq: 5, #new-token: 15987, #cached-token: 18, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-07 14:35:54 TP0] Decode batch, #running-req: 16, #token: 51342, token usage: 0.05, cuda graph: True, gen throughput (token/s): 171.70, #queue-req: 0, 
[2025-11-07 14:35:55 TP0] Decode batch, #running-req: 16, #token: 51982, token usage: 0.05, cuda graph: True, gen throughput (token/s): 615.89, #queue-req: 0, 
[2025-11-07 14:35:56 TP0] Decode batch, #running-req: 16, #token: 52622, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.75, #queue-req: 0, 
[2025-11-07 14:35:57 TP0] Decode batch, #running-req: 16, #token: 53262, token usage: 0.05, cuda graph: True, gen throughput (token/s): 606.32, #queue-req: 0, 
[2025-11-07 14:35:58 TP0] Decode batch, #running-req: 16, #token: 53902, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.48, #queue-req: 0, 
[2025-11-07 14:35:59 TP0] Decode batch, #running-req: 16, #token: 54542, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.18, #queue-req: 0, 
[2025-11-07 14:36:01 TP0] Decode batch, #running-req: 16, #token: 55182, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.27, #queue-req: 0, 
[2025-11-07 14:36:02 TP0] Decode batch, #running-req: 16, #token: 55822, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.15, #queue-req: 0, 
[2025-11-07 14:36:03 TP0] Decode batch, #running-req: 16, #token: 56462, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.05, #queue-req: 0, 
[2025-11-07 14:36:04 TP0] Decode batch, #running-req: 16, #token: 57102, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.85, #queue-req: 0, 
[2025-11-07 14:36:05 TP0] Decode batch, #running-req: 16, #token: 57742, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.37, #queue-req: 0, 
[2025-11-07 14:36:06 TP0] Decode batch, #running-req: 16, #token: 58382, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.58, #queue-req: 0, 
[2025-11-07 14:36:07 TP0] Decode batch, #running-req: 16, #token: 59022, token usage: 0.06, cuda graph: True, gen throughput (token/s): 606.89, #queue-req: 0, 
[2025-11-07 14:36:08 TP0] Decode batch, #running-req: 16, #token: 59662, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.52, #queue-req: 0, 
[2025-11-07 14:36:09 TP0] Decode batch, #running-req: 16, #token: 60302, token usage: 0.06, cuda graph: True, gen throughput (token/s): 603.98, #queue-req: 0, 
[2025-11-07 14:36:10 TP0] Decode batch, #running-req: 16, #token: 60942, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.24, #queue-req: 0, 
[2025-11-07 14:36:11 TP0] Decode batch, #running-req: 16, #token: 61582, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.06, #queue-req: 0, 
[2025-11-07 14:36:12 TP0] Decode batch, #running-req: 16, #token: 62222, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.99, #queue-req: 0, 
[2025-11-07 14:36:13 TP0] Decode batch, #running-req: 16, #token: 62862, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.34, #queue-req: 0, 
[2025-11-07 14:36:14 TP0] Decode batch, #running-req: 16, #token: 63502, token usage: 0.07, cuda graph: True, gen throughput (token/s): 604.31, #queue-req: 0, 
[2025-11-07 14:36:15] INFO:     127.0.0.1:49922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:49930 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15 TP0] Prefill batch, #new-seq: 1, #new-token: 3199, #cached-token: 2, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:36:15] INFO:     127.0.0.1:49946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:49958 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:49960 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:49966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:49980 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:49990 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:50004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:50020 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:50026 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:50030 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:50044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:50046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:50060 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15] INFO:     127.0.0.1:50064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:36:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15986, #cached-token: 19, token usage: 0.00, #running-req: 1, #queue-req: 10, 
[2025-11-07 14:36:15 TP0] Prefill batch, #new-seq: 5, #new-token: 15990, #cached-token: 15, token usage: 0.02, #running-req: 6, #queue-req: 5, 
[2025-11-07 14:36:16 TP0] Prefill batch, #new-seq: 5, #new-token: 15988, #cached-token: 17, token usage: 0.04, #running-req: 11, #queue-req: 0, 
[2025-11-07 14:36:18 TP0] Decode batch, #running-req: 16, #token: 51343, token usage: 0.05, cuda graph: True, gen throughput (token/s): 171.28, #queue-req: 0, 
[2025-11-07 14:36:19 TP0] Decode batch, #running-req: 16, #token: 51983, token usage: 0.05, cuda graph: True, gen throughput (token/s): 614.88, #queue-req: 0, 
[2025-11-07 14:36:20 TP0] Decode batch, #running-req: 16, #token: 52623, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.83, #queue-req: 0, 
[2025-11-07 14:36:21 TP0] Decode batch, #running-req: 16, #token: 53263, token usage: 0.05, cuda graph: True, gen throughput (token/s): 609.25, #queue-req: 0, 
[2025-11-07 14:36:22 TP0] Decode batch, #running-req: 16, #token: 53903, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.03, #queue-req: 0, 
[2025-11-07 14:36:23 TP0] Decode batch, #running-req: 16, #token: 54543, token usage: 0.06, cuda graph: True, gen throughput (token/s): 608.17, #queue-req: 0, 
[2025-11-07 14:36:24 TP0] Decode batch, #running-req: 16, #token: 55183, token usage: 0.06, cuda graph: True, gen throughput (token/s): 607.97, #queue-req: 0, 
[2025-11-07 14:36:25 TP0] Decode batch, #running-req: 16, #token: 55823, token usage: 0.06, cuda graph: True, gen throughput (token/s): 605.94, #queue-req: 0, 
[2025-11-07 14:36:26 TP0] Decode batch, #running-req: 16, #token: 56463, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.76, #queue-req: 0, 
[2025-11-07 14:36:27 TP0] Decode batch, #running-req: 16, #token: 57103, token usage: 0.06, cuda graph: True, gen throughput (token/s): 604.93, #queue-req: 0, 
[2025-11-07 14:36:29 TP0] Decode batch, #running-req: 16, #token: 57743, token usage: 0.06, cuda graph: True, gen throughput (token/s): 602.77, #queue-req: 0, 
[2025-11-07 14:36:30 TP0] Decode batch, #running-req: 16, #token: 58383, token usage: 0.06, cuda graph: True, gen throughput (token/s): 603.64, #queue-req: 0, 
[2025-11-07 14:36:31 TP0] Decode batch, #running-req: 16, #token: 59023, token usage: 0.06, cuda graph: True, gen throughput (token/s): 602.85, #queue-req: 0, 
[2025-11-07 14:36:32 TP0] Decode batch, #running-req: 16, #token: 59663, token usage: 0.06, cuda graph: True, gen throughput (token/s): 603.51, #queue-req: 0, 
[2025-11-07 14:36:33 TP0] Decode batch, #running-req: 16, #token: 60303, token usage: 0.06, cuda graph: True, gen throughput (token/s): 601.75, #queue-req: 0, 
[2025-11-07 14:36:34 TP0] Decode batch, #running-req: 16, #token: 60943, token usage: 0.06, cuda graph: True, gen throughput (token/s): 602.34, #queue-req: 0, 
[2025-11-07 14:36:35 TP0] Decode batch, #running-req: 16, #token: 61583, token usage: 0.06, cuda graph: True, gen throughput (token/s): 602.74, #queue-req: 0, 
[2025-11-07 14:36:36 TP0] Decode batch, #running-req: 16, #token: 62223, token usage: 0.06, cuda graph: True, gen throughput (token/s): 601.71, #queue-req: 0, 
[2025-11-07 14:36:37 TP0] Decode batch, #running-req: 16, #token: 62863, token usage: 0.06, cuda graph: True, gen throughput (token/s): 601.78, #queue-req: 0, 
[2025-11-07 14:36:38 TP0] Decode batch, #running-req: 16, #token: 63503, token usage: 0.07, cuda graph: True, gen throughput (token/s): 603.00, #queue-req: 0, 
[2025-11-07 14:36:39] INFO:     127.0.0.1:53244 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-07 14:36:39] INFO:     127.0.0.1:53246 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-07 14:36:56] INFO:     127.0.0.1:38902 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-07 14:37:02] INFO:     127.0.0.1:38904 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:37:02 TP0] Decode batch, #running-req: 1, #token: 3211, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.23, #queue-req: 0, 
[2025-11-07 14:37:03] INFO:     127.0.0.1:38910 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:03] INFO:     127.0.0.1:38920 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:37:03] INFO:     127.0.0.1:38934 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:03] INFO:     127.0.0.1:38946 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:03 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-07 14:37:04 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 45.77, #queue-req: 0, 
[2025-11-07 14:37:05 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.62, #queue-req: 0, 
[2025-11-07 14:37:06 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-07 14:37:06 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-07 14:37:07 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:37:08 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-07 14:37:09 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:37:10 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:37:10 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:37:11 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-07 14:37:12 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:37:13 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:37:14 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:37:15 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:37:15 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:37:16 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-07 14:37:17 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:37:18 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:37:19 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:37:20 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:37:20] INFO:     127.0.0.1:45966 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:20] INFO:     127.0.0.1:45972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:20] INFO:     127.0.0.1:45976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:20 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:37:20] INFO:     127.0.0.1:45978 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:20 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:37:21 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 158.90, #queue-req: 0, 
[2025-11-07 14:37:21 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-07 14:37:22 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-07 14:37:23 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-07 14:37:24 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-07 14:37:25 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:37:26 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:37:26 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:37:27 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:37:28 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:37:29 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:37:30 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:37:31 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-07 14:37:31 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:37:32 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:37:33 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:37:34 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:37:35 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:37:35 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:37:36 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:37:37] INFO:     127.0.0.1:58582 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:37] INFO:     127.0.0.1:58590 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:37] INFO:     127.0.0.1:58602 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:37 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:37:37] INFO:     127.0.0.1:58610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:37 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:37:37 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.57, #queue-req: 0, 
[2025-11-07 14:37:38 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-07 14:37:39 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-07 14:37:40 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-07 14:37:41 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:37:41 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:37:42 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:37:43 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:37:44 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:37:45 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:37:46 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-07 14:37:46 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:37:47 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:37:48 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:37:49 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:37:50 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:37:51 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:37:51 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:37:52 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:37:53 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:37:53] INFO:     127.0.0.1:35076 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:53] INFO:     127.0.0.1:35092 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:53] INFO:     127.0.0.1:35102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:53 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:37:53] INFO:     127.0.0.1:35104 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:37:54 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:37:54 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.55, #queue-req: 0, 
[2025-11-07 14:37:55 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-07 14:37:56 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-07 14:37:56 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-07 14:37:57 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-07 14:37:58 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-07 14:37:59 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:38:00 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:38:01 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-07 14:38:01 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:38:02 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:38:03 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:38:04 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:38:05 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:38:06 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:38:06 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:38:07 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-07 14:38:08 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-07 14:38:09 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:38:10 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-07 14:38:10] INFO:     127.0.0.1:59008 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:38:10] INFO:     127.0.0.1:59018 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:38:10] INFO:     127.0.0.1:59032 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:38:10 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:38:10] INFO:     127.0.0.1:59038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:38:10 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:38:11 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.60, #queue-req: 0, 
[2025-11-07 14:38:12 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-07 14:38:12 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.57, #queue-req: 0, 
[2025-11-07 14:38:13 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-07 14:38:14 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:38:15 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-07 14:38:16 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:38:17 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:38:17 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:38:18 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:38:19 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:38:20 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:38:21 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:38:21 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:38:22 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:38:23 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:38:24 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:38:25 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:38:26 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:38:26 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:38:27] INFO:     127.0.0.1:57652 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:38:27] INFO:     127.0.0.1:57660 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:38:27] INFO:     127.0.0.1:57668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:38:27 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:38:27] INFO:     127.0.0.1:57670 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:38:27 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:38:27 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.52, #queue-req: 0, 
[2025-11-07 14:38:28 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.65, #queue-req: 0, 
[2025-11-07 14:38:29 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-07 14:38:30 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-07 14:38:31 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:38:32 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:38:32 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:38:33 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:38:34 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:38:35 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:38:36 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-07 14:38:37 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:38:37 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:38:38 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:38:39 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:38:40 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-07 14:38:41 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:38:41 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:38:42 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:38:43 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:38:44] INFO:     127.0.0.1:39182 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:38:44] INFO:     127.0.0.1:39192 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:38:44] INFO:     127.0.0.1:39200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:38:44 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:38:44] INFO:     127.0.0.1:39206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:38:44 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:38:44 TP0] Decode batch, #running-req: 4, #token: 12868, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.54, #queue-req: 0, 
[2025-11-07 14:38:45 TP0] Decode batch, #running-req: 4, #token: 13028, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-07 14:38:46 TP0] Decode batch, #running-req: 4, #token: 13188, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-07 14:38:47 TP0] Decode batch, #running-req: 4, #token: 13348, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-07 14:38:47 TP0] Decode batch, #running-req: 4, #token: 13508, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-07 14:38:48 TP0] Decode batch, #running-req: 4, #token: 13668, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:38:49 TP0] Decode batch, #running-req: 4, #token: 13828, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-07 14:38:50 TP0] Decode batch, #running-req: 4, #token: 13988, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:38:51 TP0] Decode batch, #running-req: 4, #token: 14148, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:38:52 TP0] Decode batch, #running-req: 4, #token: 14308, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:38:52 TP0] Decode batch, #running-req: 4, #token: 14468, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:38:53 TP0] Decode batch, #running-req: 4, #token: 14628, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:38:54 TP0] Decode batch, #running-req: 4, #token: 14788, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-07 14:38:55 TP0] Decode batch, #running-req: 4, #token: 14948, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-07 14:38:56 TP0] Decode batch, #running-req: 4, #token: 15108, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:38:57 TP0] Decode batch, #running-req: 4, #token: 15268, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:38:57 TP0] Decode batch, #running-req: 4, #token: 15428, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-07 14:38:58 TP0] Decode batch, #running-req: 4, #token: 15588, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:38:59 TP0] Decode batch, #running-req: 4, #token: 15748, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:39:00 TP0] Decode batch, #running-req: 4, #token: 15908, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:39:00] INFO:     127.0.0.1:58262 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:00] INFO:     127.0.0.1:58264 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:00] INFO:     127.0.0.1:58272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:00 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:39:00] INFO:     127.0.0.1:58274 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:00 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:39:01 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.54, #queue-req: 0, 
[2025-11-07 14:39:02 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-07 14:39:03 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-07 14:39:03 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-07 14:39:04 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-07 14:39:05 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:39:06 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:39:07 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-07 14:39:07 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:39:08 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:39:09 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:39:10 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:39:11 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:39:12 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:39:12 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:39:13 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-07 14:39:14 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:39:15 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:39:16 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-07 14:39:17 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:39:17] INFO:     127.0.0.1:52166 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:17] INFO:     127.0.0.1:52170 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:17] INFO:     127.0.0.1:52174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:17 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:39:17] INFO:     127.0.0.1:52176 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:17 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:39:18 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.51, #queue-req: 0, 
[2025-11-07 14:39:18 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.69, #queue-req: 0, 
[2025-11-07 14:39:19 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.67, #queue-req: 0, 
[2025-11-07 14:39:20 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-07 14:39:21 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:39:22 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:39:23 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:39:23 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:39:24 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:39:25 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:39:26 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-07 14:39:27 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:39:27 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:39:28 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:39:29 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-07 14:39:30 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-07 14:39:31 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-07 14:39:32 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:39:32 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:39:33 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:39:34] INFO:     127.0.0.1:32770 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:34] INFO:     127.0.0.1:32776 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:34] INFO:     127.0.0.1:32780 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:34 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:39:34] INFO:     127.0.0.1:32782 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:34 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:39:34 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.52, #queue-req: 0, 
[2025-11-07 14:39:35 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-07 14:39:36 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-07 14:39:37 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:39:38 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:39:38 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:39:39 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:39:40 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:39:41 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:39:42 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:39:43 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:39:43 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:39:44 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:39:45 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:39:46 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:39:47 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:39:48 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:39:48 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:39:49 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-07 14:39:50 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:39:50] INFO:     127.0.0.1:51464 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:50] INFO:     127.0.0.1:51480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:50] INFO:     127.0.0.1:51486 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:50 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:39:50] INFO:     127.0.0.1:51500 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:39:51 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:39:51 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 158.73, #queue-req: 0, 
[2025-11-07 14:39:52 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-07 14:39:53 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.53, #queue-req: 0, 
[2025-11-07 14:39:53 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:39:54 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:39:55 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:39:56 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:39:57 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:39:58 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-07 14:39:58 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:39:59 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:40:00 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:40:01 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-07 14:40:02 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:40:03 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-07 14:40:03 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-07 14:40:04 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:40:05 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:40:06 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:40:07 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:40:07] INFO:     127.0.0.1:52038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:07] INFO:     127.0.0.1:52046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:07] INFO:     127.0.0.1:52058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:07 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:40:07] INFO:     127.0.0.1:52068 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:07 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:40:08 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.58, #queue-req: 0, 
[2025-11-07 14:40:09 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.71, #queue-req: 0, 
[2025-11-07 14:40:09 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-07 14:40:10 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-07 14:40:11 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.59, #queue-req: 0, 
[2025-11-07 14:40:12 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-07 14:40:13 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:40:14 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:40:14 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:40:15 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:40:16 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:40:17 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-07 14:40:18 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:40:18 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:40:19 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-07 14:40:20 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:40:21 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-07 14:40:22 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:40:23 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:40:23 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:40:24] INFO:     127.0.0.1:41692 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:24] INFO:     127.0.0.1:41700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:24] INFO:     127.0.0.1:41714 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:40:24] INFO:     127.0.0.1:41730 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:24 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:40:24 TP0] Decode batch, #running-req: 4, #token: 12869, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.48, #queue-req: 0, 
[2025-11-07 14:40:25 TP0] Decode batch, #running-req: 4, #token: 13029, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:40:26 TP0] Decode batch, #running-req: 4, #token: 13189, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:40:27 TP0] Decode batch, #running-req: 4, #token: 13349, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:40:28 TP0] Decode batch, #running-req: 4, #token: 13509, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-07 14:40:29 TP0] Decode batch, #running-req: 4, #token: 13669, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-07 14:40:29 TP0] Decode batch, #running-req: 4, #token: 13829, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:40:30 TP0] Decode batch, #running-req: 4, #token: 13989, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-07 14:40:31 TP0] Decode batch, #running-req: 4, #token: 14149, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:40:32 TP0] Decode batch, #running-req: 4, #token: 14309, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:40:33 TP0] Decode batch, #running-req: 4, #token: 14469, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:40:34 TP0] Decode batch, #running-req: 4, #token: 14629, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:40:34 TP0] Decode batch, #running-req: 4, #token: 14789, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:40:35 TP0] Decode batch, #running-req: 4, #token: 14949, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:40:36 TP0] Decode batch, #running-req: 4, #token: 15109, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:40:37 TP0] Decode batch, #running-req: 4, #token: 15269, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:40:38 TP0] Decode batch, #running-req: 4, #token: 15429, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:40:38 TP0] Decode batch, #running-req: 4, #token: 15589, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-11-07 14:40:39 TP0] Decode batch, #running-req: 4, #token: 15749, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:40:40 TP0] Decode batch, #running-req: 4, #token: 15909, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-11-07 14:40:41] INFO:     127.0.0.1:40856 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:41] INFO:     127.0.0.1:40860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:41] INFO:     127.0.0.1:40874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:40:41] INFO:     127.0.0.1:40886 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:41 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-07 14:40:41 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.30, #queue-req: 0, 
[2025-11-07 14:40:42 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.66, #queue-req: 0, 
[2025-11-07 14:40:43 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-07 14:40:44 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:40:44 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:40:45 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:40:46 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:40:47 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:40:48 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:40:49 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:40:49 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:40:50 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:40:51 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-07 14:40:52 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:40:53 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-07 14:40:54 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-07 14:40:54 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-07 14:40:55 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-07 14:40:56 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:40:57 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:40:57] INFO:     127.0.0.1:46852 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:57] INFO:     127.0.0.1:46862 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:57] INFO:     127.0.0.1:46868 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:40:57] INFO:     127.0.0.1:46884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:40:57 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-07 14:40:58 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.51, #queue-req: 0, 
[2025-11-07 14:40:59 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:41:00 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:41:00 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:41:01 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:41:02 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:41:03 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:41:04 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:41:04 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-07 14:41:05 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:41:06 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:41:07 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:41:08 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:41:09 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:41:09 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:41:10 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:41:11 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:41:12 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-11-07 14:41:13 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:41:14 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:41:14] INFO:     127.0.0.1:33810 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:41:14] INFO:     127.0.0.1:33818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:41:14] INFO:     127.0.0.1:33820 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:41:14 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:41:14] INFO:     127.0.0.1:33822 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:41:14 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:41:15 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 158.70, #queue-req: 0, 
[2025-11-07 14:41:15 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.44, #queue-req: 0, 
[2025-11-07 14:41:16 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:41:17 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:41:18 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:41:19 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:41:20 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:41:20 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:41:21 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-07 14:41:22 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:41:23 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:41:24 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:41:25 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:41:25 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:41:26 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:41:27 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:41:28 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:41:29 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:41:29 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:41:30 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:41:31] INFO:     127.0.0.1:47554 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:41:31] INFO:     127.0.0.1:47568 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:41:31] INFO:     127.0.0.1:47578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:41:31 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:41:31] INFO:     127.0.0.1:47594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:41:31 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:41:31 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 158.62, #queue-req: 0, 
[2025-11-07 14:41:32 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:41:33 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:41:34 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:41:35 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:41:35 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-07 14:41:36 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-07 14:41:37 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:41:38 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-07 14:41:39 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:41:40 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-11-07 14:41:40 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-11-07 14:41:41 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:41:42 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:41:43 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-11-07 14:41:44 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:41:45 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-11-07 14:41:45 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-11-07 14:41:46 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-11-07 14:41:47 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:41:48] INFO:     127.0.0.1:41508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:41:48] INFO:     127.0.0.1:41510 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:41:48] INFO:     127.0.0.1:41520 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:41:48 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:41:48] INFO:     127.0.0.1:41530 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:41:48 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:41:48 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.70, #queue-req: 0, 
[2025-11-07 14:41:49 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:41:50 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:41:51 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-07 14:41:51 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:41:52 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-07 14:41:53 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:41:54 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:41:55 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:41:56 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-07 14:41:56 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:41:57 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-07 14:41:58 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-07 14:41:59 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:42:00 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:42:00 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:42:01 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-07 14:42:02 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-07 14:42:03 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:42:04 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:42:04] INFO:     127.0.0.1:38518 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:04] INFO:     127.0.0.1:38526 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:04] INFO:     127.0.0.1:38542 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:04 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:42:04] INFO:     127.0.0.1:38556 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:04 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:42:05 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.24, #queue-req: 0, 
[2025-11-07 14:42:06 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:42:06 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:42:07 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-07 14:42:08 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:42:09 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:42:10 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:42:11 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:42:11 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:42:12 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:42:13 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:42:14 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-07 14:42:15 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:42:16 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-07 14:42:16 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-07 14:42:17 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:42:18 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:42:19 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-11-07 14:42:20 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-11-07 14:42:21 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-11-07 14:42:21] INFO:     127.0.0.1:44600 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:21] INFO:     127.0.0.1:44606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:42:21] INFO:     127.0.0.1:44616 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:21] INFO:     127.0.0.1:44630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:21 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-07 14:42:22 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.31, #queue-req: 0, 
[2025-11-07 14:42:22 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:42:23 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:42:24 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:42:25 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:42:26 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:42:26 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-07 14:42:27 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-07 14:42:28 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:42:29 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:42:30 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-07 14:42:31 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:42:31 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:42:32 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:42:33 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:42:34 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:42:35 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:42:36 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-11-07 14:42:36 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:42:37 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-11-07 14:42:38] INFO:     127.0.0.1:59036 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:38] INFO:     127.0.0.1:59046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:38] INFO:     127.0.0.1:59062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:38 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:42:38] INFO:     127.0.0.1:59064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:38 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:42:38 TP0] Decode batch, #running-req: 4, #token: 12869, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.16, #queue-req: 0, 
[2025-11-07 14:42:39 TP0] Decode batch, #running-req: 4, #token: 13029, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.61, #queue-req: 0, 
[2025-11-07 14:42:40 TP0] Decode batch, #running-req: 4, #token: 13189, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-07 14:42:41 TP0] Decode batch, #running-req: 4, #token: 13349, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-07 14:42:42 TP0] Decode batch, #running-req: 4, #token: 13509, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.55, #queue-req: 0, 
[2025-11-07 14:42:42 TP0] Decode batch, #running-req: 4, #token: 13669, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-07 14:42:43 TP0] Decode batch, #running-req: 4, #token: 13829, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:42:44 TP0] Decode batch, #running-req: 4, #token: 13989, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:42:45 TP0] Decode batch, #running-req: 4, #token: 14149, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:42:46 TP0] Decode batch, #running-req: 4, #token: 14309, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:42:46 TP0] Decode batch, #running-req: 4, #token: 14469, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:42:47 TP0] Decode batch, #running-req: 4, #token: 14629, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-07 14:42:48 TP0] Decode batch, #running-req: 4, #token: 14789, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:42:49 TP0] Decode batch, #running-req: 4, #token: 14949, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:42:50 TP0] Decode batch, #running-req: 4, #token: 15109, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:42:51 TP0] Decode batch, #running-req: 4, #token: 15269, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-07 14:42:51 TP0] Decode batch, #running-req: 4, #token: 15429, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:42:52 TP0] Decode batch, #running-req: 4, #token: 15589, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:42:53 TP0] Decode batch, #running-req: 4, #token: 15749, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:42:54 TP0] Decode batch, #running-req: 4, #token: 15909, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-07 14:42:54] INFO:     127.0.0.1:37174 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:54] INFO:     127.0.0.1:37180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:54] INFO:     127.0.0.1:37190 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:54 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:42:54] INFO:     127.0.0.1:37204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:42:55 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:42:55 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.23, #queue-req: 0, 
[2025-11-07 14:42:56 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.77, #queue-req: 0, 
[2025-11-07 14:42:57 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.70, #queue-req: 0, 
[2025-11-07 14:42:57 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.63, #queue-req: 0, 
[2025-11-07 14:42:58 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-07 14:42:59 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-07 14:43:00 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:43:01 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.56, #queue-req: 0, 
[2025-11-07 14:43:02 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:43:02 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:43:03 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:43:04 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:43:05 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:43:06 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-07 14:43:07 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-07 14:43:07 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:43:08 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:43:09 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:43:10 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:43:11 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:43:11] INFO:     127.0.0.1:45508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:43:11] INFO:     127.0.0.1:45522 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:43:11] INFO:     127.0.0.1:45536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:43:11 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:43:11] INFO:     127.0.0.1:45548 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:43:11 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:43:12 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 158.69, #queue-req: 0, 
[2025-11-07 14:43:12 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:43:13 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:43:14 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:43:15 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:43:16 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-07 14:43:17 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:43:17 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:43:18 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:43:19 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:43:20 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:43:21 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-11-07 14:43:22 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:43:22 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-11-07 14:43:23 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:43:24 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-11-07 14:43:25 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-11-07 14:43:26 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-11-07 14:43:27 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-11-07 14:43:27 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-11-07 14:43:28] INFO:     127.0.0.1:43416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:43:28] INFO:     127.0.0.1:43432 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:43:28] INFO:     127.0.0.1:43440 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:43:28 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:43:28] INFO:     127.0.0.1:43456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:43:28 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:43:28 TP0] Decode batch, #running-req: 4, #token: 12869, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.10, #queue-req: 0, 
[2025-11-07 14:43:29 TP0] Decode batch, #running-req: 4, #token: 13029, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.54, #queue-req: 0, 
[2025-11-07 14:43:30 TP0] Decode batch, #running-req: 4, #token: 13189, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.47, #queue-req: 0, 
[2025-11-07 14:43:31 TP0] Decode batch, #running-req: 4, #token: 13349, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:43:32 TP0] Decode batch, #running-req: 4, #token: 13509, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:43:33 TP0] Decode batch, #running-req: 4, #token: 13669, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:43:33 TP0] Decode batch, #running-req: 4, #token: 13829, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-07 14:43:34 TP0] Decode batch, #running-req: 4, #token: 13989, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-07 14:43:35 TP0] Decode batch, #running-req: 4, #token: 14149, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-07 14:43:36 TP0] Decode batch, #running-req: 4, #token: 14309, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:43:37 TP0] Decode batch, #running-req: 4, #token: 14469, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:43:37 TP0] Decode batch, #running-req: 4, #token: 14629, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-07 14:43:38 TP0] Decode batch, #running-req: 4, #token: 14789, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:43:39 TP0] Decode batch, #running-req: 4, #token: 14949, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:43:40 TP0] Decode batch, #running-req: 4, #token: 15109, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:43:41 TP0] Decode batch, #running-req: 4, #token: 15269, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:43:42 TP0] Decode batch, #running-req: 4, #token: 15429, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-07 14:43:42 TP0] Decode batch, #running-req: 4, #token: 15589, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:43:43 TP0] Decode batch, #running-req: 4, #token: 15749, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-07 14:43:44 TP0] Decode batch, #running-req: 4, #token: 15909, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:43:45] INFO:     127.0.0.1:47310 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:43:45] INFO:     127.0.0.1:47326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:43:45] INFO:     127.0.0.1:47338 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:43:45 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:43:45] INFO:     127.0.0.1:47342 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:43:45 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:43:45 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 158.54, #queue-req: 0, 
[2025-11-07 14:43:46 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.58, #queue-req: 0, 
[2025-11-07 14:43:47 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:43:48 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:43:48 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:43:49 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:43:50 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:43:51 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:43:52 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:43:53 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:43:53 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:43:54 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:43:55 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:43:56 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:43:57 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-07 14:43:58 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:43:58 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:43:59 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:44:00 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:44:01 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:44:01] INFO:     127.0.0.1:36668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:01] INFO:     127.0.0.1:36674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:01] INFO:     127.0.0.1:36688 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:01 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:44:01] INFO:     127.0.0.1:36698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:01 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:44:02 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.09, #queue-req: 0, 
[2025-11-07 14:44:03 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.60, #queue-req: 0, 
[2025-11-07 14:44:03 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.52, #queue-req: 0, 
[2025-11-07 14:44:04 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:44:05 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:44:06 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.45, #queue-req: 0, 
[2025-11-07 14:44:07 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:44:08 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:44:08 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:44:09 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-07 14:44:10 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:44:11 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:44:12 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-07 14:44:13 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:44:13 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:44:14 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:44:15 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:44:16 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:44:17 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-07 14:44:18 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:44:18] INFO:     127.0.0.1:52836 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:18] INFO:     127.0.0.1:52840 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:18] INFO:     127.0.0.1:52848 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:18 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:44:18] INFO:     127.0.0.1:52850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:18 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:44:19 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.27, #queue-req: 0, 
[2025-11-07 14:44:19 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-07 14:44:20 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-07 14:44:21 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:44:22 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-11-07 14:44:23 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-11-07 14:44:24 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-11-07 14:44:24 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-11-07 14:44:25 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-11-07 14:44:26 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-11-07 14:44:27 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-11-07 14:44:28 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-11-07 14:44:28 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-11-07 14:44:29 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-11-07 14:44:30 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-11-07 14:44:31 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-11-07 14:44:32 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-11-07 14:44:33 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.14, #queue-req: 0, 
[2025-11-07 14:44:33 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-11-07 14:44:34 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.16, #queue-req: 0, 
[2025-11-07 14:44:35] INFO:     127.0.0.1:34646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:35] INFO:     127.0.0.1:34662 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:35] INFO:     127.0.0.1:34676 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:35 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:44:35] INFO:     127.0.0.1:34680 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:35 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:44:35 TP0] Decode batch, #running-req: 4, #token: 12869, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.19, #queue-req: 0, 
[2025-11-07 14:44:36 TP0] Decode batch, #running-req: 4, #token: 13029, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.49, #queue-req: 0, 
[2025-11-07 14:44:37 TP0] Decode batch, #running-req: 4, #token: 13189, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.42, #queue-req: 0, 
[2025-11-07 14:44:38 TP0] Decode batch, #running-req: 4, #token: 13349, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:44:39 TP0] Decode batch, #running-req: 4, #token: 13509, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:44:39 TP0] Decode batch, #running-req: 4, #token: 13669, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:44:40 TP0] Decode batch, #running-req: 4, #token: 13829, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:44:41 TP0] Decode batch, #running-req: 4, #token: 13989, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:44:42 TP0] Decode batch, #running-req: 4, #token: 14149, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:44:43 TP0] Decode batch, #running-req: 4, #token: 14309, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-07 14:44:44 TP0] Decode batch, #running-req: 4, #token: 14469, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:44:44 TP0] Decode batch, #running-req: 4, #token: 14629, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:44:45 TP0] Decode batch, #running-req: 4, #token: 14789, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.08, #queue-req: 0, 
[2025-11-07 14:44:46 TP0] Decode batch, #running-req: 4, #token: 14949, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:44:47 TP0] Decode batch, #running-req: 4, #token: 15109, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-11-07 14:44:48 TP0] Decode batch, #running-req: 4, #token: 15269, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-11-07 14:44:49 TP0] Decode batch, #running-req: 4, #token: 15429, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-11-07 14:44:49 TP0] Decode batch, #running-req: 4, #token: 15589, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-11-07 14:44:50 TP0] Decode batch, #running-req: 4, #token: 15749, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.21, #queue-req: 0, 
[2025-11-07 14:44:51 TP0] Decode batch, #running-req: 4, #token: 15909, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-11-07 14:44:51] INFO:     127.0.0.1:57046 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:51] INFO:     127.0.0.1:57052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:51] INFO:     127.0.0.1:57062 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:44:51] INFO:     127.0.0.1:57064 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:44:52 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-07 14:44:52 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.21, #queue-req: 0, 
[2025-11-07 14:44:53 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.48, #queue-req: 0, 
[2025-11-07 14:44:54 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:44:54 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-07 14:44:55 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:44:56 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:44:57 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-07 14:44:58 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:44:59 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:44:59 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.37, #queue-req: 0, 
[2025-11-07 14:45:00 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:45:01 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:45:02 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:45:03 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-11-07 14:45:04 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:45:04 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-11-07 14:45:05 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-11-07 14:45:06 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.20, #queue-req: 0, 
[2025-11-07 14:45:07 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-11-07 14:45:08 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-11-07 14:45:08] INFO:     127.0.0.1:59860 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:45:08] INFO:     127.0.0.1:59874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:45:08] INFO:     127.0.0.1:59884 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:45:08 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:45:08] INFO:     127.0.0.1:59888 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:45:08 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:45:09 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.15, #queue-req: 0, 
[2025-11-07 14:45:10 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:45:10 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.43, #queue-req: 0, 
[2025-11-07 14:45:11 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.39, #queue-req: 0, 
[2025-11-07 14:45:12 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-07 14:45:13 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.35, #queue-req: 0, 
[2025-11-07 14:45:14 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:45:15 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.32, #queue-req: 0, 
[2025-11-07 14:45:15 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:45:16 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:45:17 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:45:18 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:45:19 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-07 14:45:19 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:45:20 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:45:21 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:45:22 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:45:23 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:45:24 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.24, #queue-req: 0, 
[2025-11-07 14:45:24 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.23, #queue-req: 0, 
[2025-11-07 14:45:25] INFO:     127.0.0.1:34014 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:45:25] INFO:     127.0.0.1:34028 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:45:25] INFO:     127.0.0.1:34038 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:45:25 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:45:25] INFO:     127.0.0.1:34044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:45:25 TP0] Prefill batch, #new-seq: 2, #new-token: 2, #cached-token: 6400, token usage: 0.01, #running-req: 2, #queue-req: 0, 
[2025-11-07 14:45:25 TP0] Decode batch, #running-req: 4, #token: 12870, token usage: 0.01, cuda graph: True, gen throughput (token/s): 159.99, #queue-req: 0, 
[2025-11-07 14:45:26 TP0] Decode batch, #running-req: 4, #token: 13030, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.50, #queue-req: 0, 
[2025-11-07 14:45:27 TP0] Decode batch, #running-req: 4, #token: 13190, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-07 14:45:28 TP0] Decode batch, #running-req: 4, #token: 13350, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.38, #queue-req: 0, 
[2025-11-07 14:45:29 TP0] Decode batch, #running-req: 4, #token: 13510, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:45:30 TP0] Decode batch, #running-req: 4, #token: 13670, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:45:30 TP0] Decode batch, #running-req: 4, #token: 13830, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:45:31 TP0] Decode batch, #running-req: 4, #token: 13990, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.26, #queue-req: 0, 
[2025-11-07 14:45:32 TP0] Decode batch, #running-req: 4, #token: 14150, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.22, #queue-req: 0, 
[2025-11-07 14:45:33 TP0] Decode batch, #running-req: 4, #token: 14310, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-11-07 14:45:34 TP0] Decode batch, #running-req: 4, #token: 14470, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-11-07 14:45:35 TP0] Decode batch, #running-req: 4, #token: 14630, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-11-07 14:45:35 TP0] Decode batch, #running-req: 4, #token: 14790, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-11-07 14:45:36 TP0] Decode batch, #running-req: 4, #token: 14950, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.17, #queue-req: 0, 
[2025-11-07 14:45:37 TP0] Decode batch, #running-req: 4, #token: 15110, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.19, #queue-req: 0, 
[2025-11-07 14:45:38 TP0] Decode batch, #running-req: 4, #token: 15270, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-11-07 14:45:39 TP0] Decode batch, #running-req: 4, #token: 15430, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.14, #queue-req: 0, 
[2025-11-07 14:45:40 TP0] Decode batch, #running-req: 4, #token: 15590, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-11-07 14:45:40 TP0] Decode batch, #running-req: 4, #token: 15750, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.18, #queue-req: 0, 
[2025-11-07 14:45:41 TP0] Decode batch, #running-req: 4, #token: 15910, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.15, #queue-req: 0, 
[2025-11-07 14:45:42] INFO:     127.0.0.1:55118 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:45:42] INFO:     127.0.0.1:55128 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:45:42 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:45:42] INFO:     127.0.0.1:55142 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:45:42] INFO:     127.0.0.1:55146 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:45:42 TP0] Prefill batch, #new-seq: 3, #new-token: 3, #cached-token: 9600, token usage: 0.01, #running-req: 1, #queue-req: 0, 
[2025-11-07 14:45:42 TP0] Decode batch, #running-req: 4, #token: 12868, token usage: 0.01, cuda graph: True, gen throughput (token/s): 160.23, #queue-req: 0, 
[2025-11-07 14:45:43 TP0] Decode batch, #running-req: 4, #token: 13028, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:45:44 TP0] Decode batch, #running-req: 4, #token: 13188, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:45:45 TP0] Decode batch, #running-req: 4, #token: 13348, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.46, #queue-req: 0, 
[2025-11-07 14:45:46 TP0] Decode batch, #running-req: 4, #token: 13508, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.40, #queue-req: 0, 
[2025-11-07 14:45:46 TP0] Decode batch, #running-req: 4, #token: 13668, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.41, #queue-req: 0, 
[2025-11-07 14:45:47 TP0] Decode batch, #running-req: 4, #token: 13828, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.25, #queue-req: 0, 
[2025-11-07 14:45:48 TP0] Decode batch, #running-req: 4, #token: 13988, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.51, #queue-req: 0, 
[2025-11-07 14:45:49 TP0] Decode batch, #running-req: 4, #token: 14148, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:45:50 TP0] Decode batch, #running-req: 4, #token: 14308, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:45:50 TP0] Decode batch, #running-req: 4, #token: 14468, token usage: 0.01, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:45:51 TP0] Decode batch, #running-req: 4, #token: 14628, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.34, #queue-req: 0, 
[2025-11-07 14:45:52 TP0] Decode batch, #running-req: 4, #token: 14788, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.36, #queue-req: 0, 
[2025-11-07 14:45:53 TP0] Decode batch, #running-req: 4, #token: 14948, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.33, #queue-req: 0, 
[2025-11-07 14:45:54 TP0] Decode batch, #running-req: 4, #token: 15108, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-07 14:45:55 TP0] Decode batch, #running-req: 4, #token: 15268, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.31, #queue-req: 0, 
[2025-11-07 14:45:55 TP0] Decode batch, #running-req: 4, #token: 15428, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.27, #queue-req: 0, 
[2025-11-07 14:45:56 TP0] Decode batch, #running-req: 4, #token: 15588, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.28, #queue-req: 0, 
[2025-11-07 14:45:57 TP0] Decode batch, #running-req: 4, #token: 15748, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.29, #queue-req: 0, 
[2025-11-07 14:45:58 TP0] Decode batch, #running-req: 4, #token: 15908, token usage: 0.02, cuda graph: True, gen throughput (token/s): 193.30, #queue-req: 0, 
[2025-11-07 14:45:58] INFO:     127.0.0.1:35966 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-07 14:45:59] INFO:     127.0.0.1:35972 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-07 14:46:15] INFO:     127.0.0.1:51396 - "GET /v1/models HTTP/1.1" 200 OK
[2025-11-07 14:46:21] INFO:     127.0.0.1:51408 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:46:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:46:21 TP0] Decode batch, #running-req: 1, #token: 3219, token usage: 0.00, cuda graph: True, gen throughput (token/s): 4.65, #queue-req: 0, 
[2025-11-07 14:46:23] INFO:     127.0.0.1:51418 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:46:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:46:23 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 21.00, #queue-req: 0, 
[2025-11-07 14:46:24 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:46:25 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:46:26 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:46:27 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:46:27 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:46:28 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:46:29 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:46:30 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:46:31 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:46:31 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:46:32 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:46:33 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:46:34 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:46:35 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:46:36 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:46:36 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:46:37 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:46:38 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:46:39 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:46:39] INFO:     127.0.0.1:50674 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:46:39 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:46:40 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-07 14:46:41 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 14:46:41 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:46:42 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:46:43 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:46:44 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:46:45 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:46:45 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:46:46 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:46:47 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:46:48 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:46:49 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:46:50 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:46:50 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:46:51 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:46:52 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:46:53 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:46:54 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:46:54 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:46:55 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:46:56] INFO:     127.0.0.1:51370 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:46:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:46:56 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-07 14:46:57 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 14:46:58 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:46:59 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:46:59 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:47:00 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:47:01 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:47:02 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:47:03 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:47:04 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:47:04 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:05 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:06 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:47:07 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:08 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:47:08 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:09 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:47:10 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:11 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:12 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:12] INFO:     127.0.0.1:56444 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:47:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:47:13 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-07 14:47:13 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:47:14 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:47:15 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:47:16 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:47:17 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:47:18 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:47:18 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:47:19 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:47:20 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:47:21 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:47:22 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:47:22 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:47:23 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:47:24 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:47:25 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:47:26 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:47:27 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:47:27 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:47:28 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:47:28] INFO:     127.0.0.1:33618 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:47:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:47:29 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 14:47:30 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:47:31 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:32 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:47:32 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:47:33 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:47:34 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:47:35 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:47:36 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:47:36 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:47:37 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:47:38 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:47:39 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:47:40 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:47:41 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:47:41 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:47:42 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:47:43 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:47:44 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:47:45 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:47:45] INFO:     127.0.0.1:34648 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:47:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:47:46 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-07 14:47:46 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-07 14:47:47 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 14:47:48 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:47:49 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:47:50 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:47:50 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:47:51 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:47:52 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:53 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:54 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:47:54 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:55 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:56 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:47:57 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:58 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:47:59 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:47:59 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:48:00 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:48:01 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:48:01] INFO:     127.0.0.1:40052 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:48:01 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:48:02 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-07 14:48:03 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:48:04 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:48:04 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:48:05 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:48:06 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:48:07 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:48:08 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:48:08 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:48:09 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:48:10 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:48:11 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:48:12 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:48:13 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:48:13 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:48:14 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:48:15 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:48:16 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:48:17 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:48:17 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:48:18] INFO:     127.0.0.1:57012 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:48:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:48:18 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-07 14:48:19 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:48:20 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:48:21 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:48:22 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:48:22 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:48:23 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:48:24 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:48:25 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:48:26 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:48:27 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:48:27 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:48:28 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:48:29 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:48:30 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:48:31 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:48:31 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:48:32 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:48:33 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:48:34 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:48:34] INFO:     127.0.0.1:44604 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:48:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:48:35 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.82, #queue-req: 0, 
[2025-11-07 14:48:36 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:48:36 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:48:37 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:48:38 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:48:39 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:48:40 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:48:41 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:48:41 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:48:42 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:48:43 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:48:44 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:48:45 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:48:45 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:48:46 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:48:47 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:48:48 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:48:49 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:48:50 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:48:50 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:48:51] INFO:     127.0.0.1:56326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:48:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:48:51 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 14:48:52 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 14:48:53 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:48:54 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:48:55 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:48:55 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:48:56 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:48:57 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:48:58 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:48:59 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:48:59 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:49:00 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:49:01 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:49:02 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:49:03 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:49:04 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:49:04 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:49:05 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:49:06 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:49:07 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:49:07] INFO:     127.0.0.1:50690 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:49:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:49:08 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-07 14:49:09 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:49:09 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:49:10 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:49:11 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:49:12 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:49:13 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:49:13 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:49:14 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:49:15 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:49:16 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:49:17 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:49:18 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:49:18 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:49:19 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:49:20 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:49:21 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:49:22 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:49:22 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:49:23 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:49:24] INFO:     127.0.0.1:40198 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:49:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:49:24 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-07 14:49:25 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 14:49:26 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:49:27 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:49:27 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:49:28 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:49:29 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:49:30 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:49:31 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:49:32 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:49:32 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:49:33 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:49:34 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:49:35 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:49:36 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:49:36 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:49:37 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:49:38 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:49:39 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:49:40 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:49:40] INFO:     127.0.0.1:59880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:49:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:49:41 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-07 14:49:41 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 14:49:42 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:49:43 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:49:44 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:49:45 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:49:46 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:49:46 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:49:47 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:49:48 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:49:49 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:49:50 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:49:50 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:49:51 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:49:52 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:49:53 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:49:54 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:49:55 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:49:55 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:49:56 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:49:56] INFO:     127.0.0.1:47132 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:49:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:49:57 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 14:49:58 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:49:59 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:50:00 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:50:00 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:50:01 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:50:02 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:50:03 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:50:04 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:50:04 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:50:05 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:50:06 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:07 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:50:08 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:09 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:50:09 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:10 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:50:11 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:50:12 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:50:13 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:50:13] INFO:     127.0.0.1:39698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:50:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:50:14 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.21, #queue-req: 0, 
[2025-11-07 14:50:14 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:50:15 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:50:16 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:50:17 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:50:18 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:50:18 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:50:19 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:50:20 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:21 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:50:22 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:50:23 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:23 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:50:24 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:25 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:26 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:50:27 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:50:27 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:50:28 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:50:29 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:50:29] INFO:     127.0.0.1:33004 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:50:29 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:50:30 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.22, #queue-req: 0, 
[2025-11-07 14:50:31 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:50:32 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:50:32 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:50:33 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:50:34 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:50:35 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:50:36 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:50:37 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:50:37 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:50:38 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:50:39 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:50:40 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:41 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:41 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:42 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:50:43 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:50:44 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:50:45 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:50:46 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:50:46] INFO:     127.0.0.1:45908 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:50:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:50:46 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-07 14:50:47 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:50:48 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:50:49 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:50:50 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:50:51 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:50:51 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:50:52 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:50:53 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:54 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:55 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:55 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:56 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:50:57 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:50:58 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:50:59 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:51:00 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:51:00 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:51:01 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:51:02 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:51:02] INFO:     127.0.0.1:51918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:51:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:51:03 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 14:51:04 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:51:05 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:51:05 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:51:06 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:51:07 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:51:08 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:51:09 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:51:09 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:51:10 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:51:11 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:51:12 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:51:13 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:51:14 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:51:14 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:51:15 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:51:16 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:51:17 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:51:18 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:51:18 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:51:19] INFO:     127.0.0.1:33964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:51:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:51:19 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-07 14:51:20 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:51:21 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:51:22 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:51:23 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:51:23 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:51:24 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:51:25 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:51:26 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:51:27 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:51:28 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:51:28 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:51:29 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:51:30 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:51:31 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:51:32 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:51:32 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:51:33 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:51:34 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:51:35 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:51:35] INFO:     127.0.0.1:41628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:51:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:51:36 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-07 14:51:37 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-07 14:51:37 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 14:51:38 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 14:51:39 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:51:40 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:51:41 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:51:41 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:51:42 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:51:43 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:51:44 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:51:45 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:51:46 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:51:46 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:51:47 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:51:48 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:51:49 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:51:50 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:51:50 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:51:51 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:51:52] INFO:     127.0.0.1:44044 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:51:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:51:52 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-07 14:51:53 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:51:54 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:51:55 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:51:55 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:51:56 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:51:57 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:51:58 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:51:59 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:52:00 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:52:00 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:52:01 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:52:02 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:52:03 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:52:04 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:52:04 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 14:52:05 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:52:06 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:52:07 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:52:08 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:52:08] INFO:     127.0.0.1:48628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:52:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:52:09 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-07 14:52:09 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:52:10 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:52:11 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:52:12 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:52:13 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:52:14 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:52:14 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:52:15 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:52:16 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:52:17 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:52:18 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:52:18 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:52:19 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:52:20 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:52:21 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:52:22 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:52:23 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:52:23 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:52:24 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:52:24] INFO:     127.0.0.1:41438 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:52:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:52:25 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-07 14:52:26 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-07 14:52:27 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:52:28 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:52:28 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:52:29 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:52:30 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:52:31 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:52:32 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:52:32 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:52:33 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:52:34 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:52:35 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:52:36 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:52:37 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:52:37 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:52:38 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:52:39 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:52:40 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:52:41 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:52:41] INFO:     127.0.0.1:34612 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:52:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:52:42 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.41, #queue-req: 0, 
[2025-11-07 14:52:42 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 14:52:43 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:52:44 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:52:45 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:52:46 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:52:46 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:52:47 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:52:48 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:52:49 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:52:50 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:52:51 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:52:51 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:52:52 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:52:53 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:52:54 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:52:55 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:52:55 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:52:56 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:52:57 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:52:57] INFO:     127.0.0.1:32914 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:52:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:52:58 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-07 14:52:59 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:53:00 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:53:00 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:53:01 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:53:02 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:53:03 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:53:04 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:53:05 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:53:05 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:53:06 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:53:07 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:53:08 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:53:09 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:53:09 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:53:10 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:53:11 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:53:12 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:53:13 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:53:14 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:53:14] INFO:     127.0.0.1:55116 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:53:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:53:14 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-07 14:53:15 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:53:16 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:53:17 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:53:18 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:53:19 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:53:19 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:53:20 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:53:21 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:53:22 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:53:23 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:53:23 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:53:24 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:53:25 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:53:26 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:53:27 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:53:28 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:53:28 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:53:29 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:53:30 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:53:30] INFO:     127.0.0.1:40740 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:53:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:53:31 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-07 14:53:32 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:53:33 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:53:33 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:53:34 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:53:35 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:53:36 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:53:37 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:53:37 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:53:38 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:53:39 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:53:40 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:53:41 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:53:42 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:53:42 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:53:43 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:53:44 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:53:45 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:53:46 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:53:46 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:53:47] INFO:     127.0.0.1:48628 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:53:47 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:53:47 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-07 14:53:48 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:53:49 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:53:50 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:53:51 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:53:51 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:53:52 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:53:53 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:53:54 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:53:55 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:53:56 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:53:56 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:53:57 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:53:58 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:53:59 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:54:00 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:54:00 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:54:01 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:54:02 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:54:03 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:54:03] INFO:     127.0.0.1:33698 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:54:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:54:04 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 14:54:05 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:54:05 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:54:06 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:54:07 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:54:08 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:54:09 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:54:10 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:10 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:11 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:12 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:13 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:54:14 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:54:14 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:54:15 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:54:16 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:54:17 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:54:18 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:54:19 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:54:19 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:54:20] INFO:     127.0.0.1:55374 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:54:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:54:20 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-07 14:54:21 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 14:54:22 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 14:54:23 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:54:23 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:54:24 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:54:25 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:54:26 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:54:27 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:54:28 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:54:28 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:29 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:30 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:54:31 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:32 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:54:32 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:54:33 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:34 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:54:35 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:54:36 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:54:36] INFO:     127.0.0.1:55606 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:54:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:54:37 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-11-07 14:54:37 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:54:38 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:54:39 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:54:40 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:54:41 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:54:42 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:54:42 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:54:43 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:54:44 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:54:45 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:54:46 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:46 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:47 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:54:48 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:54:49 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:54:50 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:51 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:51 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:52 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:54:52] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:54:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:54:53 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-07 14:54:54 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:54:55 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:54:56 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:54:56 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:54:57 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:54:58 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:54:59 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:55:00 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:55:00 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:55:01 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:55:02 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:55:03 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:55:04 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:55:05 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:55:05 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:55:06 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:55:07 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:55:08 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:55:09 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:55:09] INFO:     127.0.0.1:57346 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:55:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:55:10 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-07 14:55:10 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:55:11 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:55:12 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:55:13 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:55:14 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:55:14 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:55:15 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:55:16 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:55:17 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:55:18 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:55:19 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:55:19 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:55:20 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:55:21 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:55:22 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:55:23 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:55:23 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:55:24 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:55:25 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:55:25] INFO:     127.0.0.1:47480 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:55:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:55:26 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-07 14:55:27 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:55:28 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:55:28 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:55:29 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 14:55:30 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:55:31 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 14:55:32 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:55:33 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 14:55:33 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 14:55:34 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:55:35 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 14:55:36 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:55:37 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:55:37 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 14:55:38 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 14:55:39 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 14:55:40 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 14:55:41 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 14:55:42 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 14:55:42] INFO:     127.0.0.1:49594 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:55:42 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:55:42 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-07 14:55:43 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-07 14:55:44 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 14:55:45 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:55:46 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:55:47 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:55:47 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:55:48 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:55:49 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:55:50 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:55:51 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:55:51 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:55:52 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:55:53 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:55:54 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:55:55 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:55:56 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:55:56 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:55:57 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:55:58 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:55:58] INFO:     127.0.0.1:51460 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:55:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:55:59 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.42, #queue-req: 0, 
[2025-11-07 14:56:00 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-07 14:56:01 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-07 14:56:01 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 14:56:02 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 14:56:03 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:56:04 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 14:56:05 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 14:56:05 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:56:06 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:56:07 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:56:08 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:56:09 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:56:10 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:56:10 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:56:11 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:56:12 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:56:13 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:56:14 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:56:14 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:56:15] INFO:     127.0.0.1:57126 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:56:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:56:15 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-07 14:56:16 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:56:17 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:56:18 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:56:19 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:56:19 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:56:20 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:56:21 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:56:22 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:56:23 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:56:24 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:56:24 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:56:25 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:56:26 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:56:27 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:56:28 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:56:28 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:56:29 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:56:30 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:56:31 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:56:31] INFO:     127.0.0.1:58058 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:56:31 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:56:32 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-07 14:56:33 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:56:33 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:56:34 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:56:35 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:56:36 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:56:37 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:56:38 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:56:38 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:56:39 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:56:40 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:56:41 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:56:42 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:56:42 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:56:43 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:56:44 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:56:45 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:56:46 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:56:47 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:56:47 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:56:48] INFO:     127.0.0.1:48664 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:56:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:56:48 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.85, #queue-req: 0, 
[2025-11-07 14:56:49 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:56:50 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:56:51 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:56:52 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:56:52 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:56:53 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:56:54 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:56:55 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:56:56 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:56:56 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:56:57 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:56:58 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:56:59 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:57:00 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:57:01 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:57:01 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:57:02 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:57:03 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:57:04 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:57:04] INFO:     127.0.0.1:46800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:57:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:57:05 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.41, #queue-req: 0, 
[2025-11-07 14:57:06 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-07 14:57:06 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 14:57:07 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:57:08 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 14:57:09 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:57:10 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:57:10 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:57:11 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:57:12 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:57:13 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:57:14 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:57:15 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:57:15 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:57:16 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:57:17 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:57:18 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:57:19 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:57:19 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:57:20 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:57:21] INFO:     127.0.0.1:40572 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:57:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:57:21 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-11-07 14:57:22 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:57:23 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:57:24 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:57:24 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:57:25 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:57:26 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:57:27 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:57:28 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:57:29 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:57:29 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:57:30 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:57:31 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:57:32 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:57:33 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:57:33 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:57:34 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:57:35 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:57:36 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 14:57:37 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:57:37] INFO:     127.0.0.1:52972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:57:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:57:38 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-07 14:57:38 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:57:39 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:57:40 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:57:41 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:57:42 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:57:43 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:57:43 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:57:44 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:57:45 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:57:46 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:57:47 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:57:47 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:57:48 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:57:49 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:57:50 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:57:51 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:57:52 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:57:52 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:57:53 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:57:53] INFO:     127.0.0.1:43206 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:57:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:57:54 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-07 14:57:55 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:57:56 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:57:57 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:57:57 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:57:58 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:57:59 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:58:00 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:58:01 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:58:01 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:58:02 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:58:03 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:58:04 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:58:05 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:58:06 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:58:06 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:58:07 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:58:08 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:58:09 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:58:10 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:58:10] INFO:     127.0.0.1:59926 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:58:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:58:11 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-07 14:58:11 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:58:12 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:58:13 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:58:14 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:58:15 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:58:15 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:58:16 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:58:17 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:58:18 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:58:19 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:58:20 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:58:20 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:58:21 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:58:22 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:58:23 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:58:24 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:58:24 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:58:25 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:58:26 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:58:26] INFO:     127.0.0.1:59416 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:58:26 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:58:27 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-07 14:58:28 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-07 14:58:29 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 14:58:29 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:58:30 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:58:31 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:58:32 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:58:33 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:58:34 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:58:34 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:58:35 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:58:36 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:58:37 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:58:38 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:58:38 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:58:39 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:58:40 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:58:41 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:58:42 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:58:43 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:58:43] INFO:     127.0.0.1:52850 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:58:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:58:43 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-07 14:58:44 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-07 14:58:45 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.99, #queue-req: 0, 
[2025-11-07 14:58:46 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 14:58:47 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 14:58:47 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 14:58:48 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 14:58:49 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:58:50 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 14:58:51 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:58:52 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 14:58:52 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:58:53 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:58:54 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:58:55 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 14:58:56 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:58:56 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:58:57 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:58:58 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 14:58:59 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:58:59] INFO:     127.0.0.1:60080 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:58:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:59:00 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-07 14:59:01 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 14:59:01 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:59:02 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:59:03 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 14:59:04 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:59:05 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:59:06 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:59:06 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:59:07 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:59:08 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:59:09 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:59:10 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:59:10 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:59:11 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:59:12 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:59:13 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:59:14 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:59:15 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:59:15 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:59:16] INFO:     127.0.0.1:50672 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:59:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:59:16 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-07 14:59:17 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:59:18 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:59:19 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:59:20 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:59:20 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:59:21 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:59:22 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:59:23 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:59:24 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:59:24 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:59:25 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:59:26 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:59:27 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:59:28 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:59:29 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:59:29 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:59:30 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:59:31 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:59:32 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:59:32] INFO:     127.0.0.1:51560 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:59:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:59:33 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 14:59:34 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 14:59:34 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 14:59:35 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:59:36 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 14:59:37 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:59:38 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:59:38 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:59:39 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:59:40 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:59:41 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:59:42 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:59:43 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:59:43 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:59:44 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:59:45 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:59:46 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 14:59:47 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 14:59:47 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 14:59:48 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 14:59:49] INFO:     127.0.0.1:39386 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 14:59:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 14:59:49 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.75, #queue-req: 0, 
[2025-11-07 14:59:50 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 14:59:51 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 14:59:52 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:59:52 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:59:53 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:59:54 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:59:55 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 14:59:56 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:59:57 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 14:59:57 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 14:59:58 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 14:59:59 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:00:00 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:00:01 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:00:02 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:00:02 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:00:03 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:00:04 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:00:05 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:00:05] INFO:     127.0.0.1:55250 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:00:05 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:00:06 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-11-07 15:00:07 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:00:07 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:08 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:09 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:10 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:11 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:11 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:00:12 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:13 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:00:14 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:15 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:00:16 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:16 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:00:17 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:18 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:00:19 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:00:20 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:00:20 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:00:21 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:00:22] INFO:     127.0.0.1:60102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:00:22 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:00:22 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-07 15:00:23 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:00:24 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:00:25 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:00:25 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:00:26 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:00:27 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:00:28 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:00:29 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:00:30 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:30 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:00:31 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:32 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:33 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:34 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:00:34 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:00:35 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:00:36 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:37 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:00:38 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:00:38] INFO:     127.0.0.1:38102 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:00:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:00:39 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-07 15:00:39 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:00:40 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:00:41 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:00:42 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:00:43 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:00:44 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:00:44 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:00:45 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:00:46 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:00:47 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:00:48 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:00:48 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:00:49 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:00:50 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:00:51 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:00:52 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:00:53 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:00:53 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:00:54 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:54] INFO:     127.0.0.1:39922 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:00:54 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:00:55 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-07 15:00:56 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:00:57 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:00:58 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:00:58 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:00:59 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:01:00 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:01:01 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:01:02 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:01:02 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:01:03 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:01:04 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:01:05 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:01:06 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:01:07 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:01:07 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:01:08 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:01:09 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:01:10 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:01:11 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:01:11] INFO:     127.0.0.1:49712 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:01:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:01:12 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.43, #queue-req: 0, 
[2025-11-07 15:01:12 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.06, #queue-req: 0, 
[2025-11-07 15:01:13 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.05, #queue-req: 0, 
[2025-11-07 15:01:14 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.04, #queue-req: 0, 
[2025-11-07 15:01:15 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.02, #queue-req: 0, 
[2025-11-07 15:01:16 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.05, #queue-req: 0, 
[2025-11-07 15:01:16 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.04, #queue-req: 0, 
[2025-11-07 15:01:17 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.04, #queue-req: 0, 
[2025-11-07 15:01:18 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.02, #queue-req: 0, 
[2025-11-07 15:01:19 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.01, #queue-req: 0, 
[2025-11-07 15:01:20 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.01, #queue-req: 0, 
[2025-11-07 15:01:20 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.01, #queue-req: 0, 
[2025-11-07 15:01:21 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-07 15:01:22 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-07 15:01:23 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-07 15:01:24 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 15:01:25 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 15:01:25 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 15:01:26 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 15:01:27 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 15:01:27] INFO:     127.0.0.1:54248 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:01:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:01:28 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-07 15:01:29 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:01:30 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:01:30 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:01:31 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:01:32 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:01:33 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:01:34 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:01:34 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:01:35 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:01:36 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:01:37 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:01:38 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:01:39 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:01:39 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:01:40 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:01:41 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:01:42 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:01:43 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:01:43 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:01:44] INFO:     127.0.0.1:35000 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:01:44 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:01:44 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-07 15:01:45 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:01:46 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:01:47 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:01:48 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:01:48 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:01:49 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:01:50 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:01:51 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:01:52 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:01:53 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:01:53 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:01:54 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:01:55 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:01:56 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:01:57 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:01:57 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:01:58 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:01:59 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:02:00 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:02:00] INFO:     127.0.0.1:36774 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:02:00 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:02:01 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-07 15:02:02 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:02:02 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:02:03 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:02:04 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:02:05 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:02:06 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:02:07 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:02:07 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:02:08 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:02:09 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:02:10 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:02:11 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:02:11 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:02:12 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:02:13 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:02:14 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:02:15 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:02:16 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:02:16 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:02:17] INFO:     127.0.0.1:52312 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:02:17 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:02:17 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-07 15:02:18 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:02:19 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:02:20 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:02:21 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:02:21 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:02:22 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:02:23 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:02:24 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:02:25 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:02:26 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:02:26 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:02:27 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:02:28 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:02:29 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:02:30 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:02:30 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:02:31 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:02:32 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:02:33 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:02:33] INFO:     127.0.0.1:57700 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:02:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:02:34 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-11-07 15:02:35 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:02:35 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:02:36 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:02:37 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:02:38 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:02:39 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:02:40 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:02:40 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:02:41 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:02:42 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:02:43 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:02:44 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:02:44 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:02:45 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:02:46 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:02:47 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:02:48 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:02:49 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:02:49 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:02:50] INFO:     127.0.0.1:55918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:02:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:02:50 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 15:02:51 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 15:02:52 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:02:53 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:02:54 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:02:54 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:02:55 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:02:56 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:02:57 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:02:58 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:02:58 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:02:59 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:03:00 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:03:01 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:03:02 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:03:03 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:03:03 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:03:04 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:03:05 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:03:06 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:03:06] INFO:     127.0.0.1:51114 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:03:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:03:07 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 15:03:08 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:03:08 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:03:09 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:03:10 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:03:11 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:03:12 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:03:12 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:03:13 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:03:14 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:03:15 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:03:16 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:03:17 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:03:17 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:03:18 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:03:19 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:03:20 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:03:21 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:03:21 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:03:22 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:03:23] INFO:     127.0.0.1:50772 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:03:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:03:23 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-07 15:03:24 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:03:25 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:03:26 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:03:26 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:03:27 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:03:28 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:03:29 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:03:30 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:03:31 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:03:31 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:03:32 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:03:33 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:03:34 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:03:35 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:03:35 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:03:36 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:03:37 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:03:38 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:03:39 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:03:39] INFO:     127.0.0.1:46668 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:03:39 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:03:40 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-07 15:03:40 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:03:41 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:03:42 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:03:43 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:03:44 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:03:45 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:03:45 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:03:46 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:03:47 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:03:48 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:03:49 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:03:49 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:03:50 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:03:51 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:03:52 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:03:53 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:03:54 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:03:54 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:03:55 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:03:55] INFO:     127.0.0.1:39320 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:03:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:03:56 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 15:03:57 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:03:58 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:03:59 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:03:59 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:04:00 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:04:01 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:04:02 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:04:03 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:04:03 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:04:04 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:04:05 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:04:06 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:04:07 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:04:08 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:04:08 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:04:09 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:04:10 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:04:11 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:04:12 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:04:12] INFO:     127.0.0.1:54802 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:04:12 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:04:13 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 15:04:13 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:04:14 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:04:15 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:04:16 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:04:17 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:04:17 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:04:18 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:04:19 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:04:20 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:04:21 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:04:22 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:04:22 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:04:23 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:04:24 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:04:25 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:04:26 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:04:26 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:04:27 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:04:28 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:04:28] INFO:     127.0.0.1:37050 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:04:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:04:29 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-11-07 15:04:30 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:04:31 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:04:31 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:04:32 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:04:33 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:04:34 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:04:35 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:04:36 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:04:36 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:04:37 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:04:38 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:04:39 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:04:40 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:04:40 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:04:41 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:04:42 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:04:43 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:04:44 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:04:45 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:04:45] INFO:     127.0.0.1:54818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:04:45 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:04:45 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-07 15:04:46 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:04:47 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:04:48 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:04:49 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:04:50 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:04:50 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:04:51 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:04:52 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:04:53 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:04:54 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:04:54 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:04:55 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:04:56 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:04:57 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:04:58 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:04:59 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:04:59 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:00 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:01 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:01] INFO:     127.0.0.1:40744 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:05:01 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:05:02 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-07 15:05:03 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:05:04 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:04 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:05 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:06 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:07 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:08 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:08 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:05:09 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:10 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:05:11 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:05:12 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:05:13 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:05:13 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:14 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:05:15 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:05:16 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:05:17 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:05:18 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:18] INFO:     127.0.0.1:36244 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:05:18 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:05:18 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-11-07 15:05:19 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:05:20 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:05:21 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:05:22 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:23 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:23 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:05:24 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:25 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:26 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:27 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:05:27 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:28 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:29 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:05:30 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:31 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:05:32 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:05:32 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:33 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:05:34 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:05:34] INFO:     127.0.0.1:35972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:05:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:05:35 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-07 15:05:36 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:05:37 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:05:37 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:05:38 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:05:39 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:05:40 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:41 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:05:41 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:05:42 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:05:43 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:44 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:45 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:46 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:05:46 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:47 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:48 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:49 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:50 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:05:50 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:05:51] INFO:     127.0.0.1:42424 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:05:51 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:05:51 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-07 15:05:52 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:05:53 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:05:54 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:05:55 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:05:55 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:05:56 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:05:57 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:05:58 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:05:59 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:00 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:06:00 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:06:01 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:06:02 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:03 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:06:04 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:06:04 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:06:05 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:06:06 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:06:07 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:06:07] INFO:     127.0.0.1:57288 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:06:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:06:08 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-07 15:06:09 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:06:09 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:06:10 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:06:11 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:06:12 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:06:13 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:06:14 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:06:14 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:15 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:06:16 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:06:17 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:18 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:18 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:06:19 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:20 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:21 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:22 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:06:23 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:23 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:24] INFO:     127.0.0.1:56446 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:06:24 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:06:24 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 15:06:25 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:06:26 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:06:27 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:06:28 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:28 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:29 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:06:30 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:31 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:32 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:32 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:06:33 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:34 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:35 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:36 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:37 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:37 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:06:38 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:06:39 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:06:40 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:06:40] INFO:     127.0.0.1:35272 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:06:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:06:41 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 15:06:42 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:06:42 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:06:43 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:06:44 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:06:45 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:06:46 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:06:46 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:06:47 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:06:48 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:49 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:50 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:51 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:06:51 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:52 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:53 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:06:54 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:55 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:55 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:56 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:06:57] INFO:     127.0.0.1:41610 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:06:57 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:06:57 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.89, #queue-req: 0, 
[2025-11-07 15:06:58 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:06:59 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:07:00 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:07:00 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:07:01 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:07:02 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:07:03 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:07:04 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:07:05 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:07:05 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:07:06 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:07:07 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:07:08 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:09 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:07:09 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:10 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:07:11 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:07:12 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:07:13 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:07:13] INFO:     127.0.0.1:35468 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:07:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:07:14 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 15:07:14 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:07:15 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:07:16 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:07:17 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:07:18 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:07:19 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:07:19 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:07:20 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:07:21 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:07:22 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:07:23 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:07:23 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:07:24 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:07:25 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:07:26 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:07:27 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:07:28 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:28 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:29 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:29] INFO:     127.0.0.1:35968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:07:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:07:30 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-07 15:07:31 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:07:32 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:33 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:33 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:07:34 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:07:35 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:36 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:07:37 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:07:37 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:07:38 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:07:39 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:07:40 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:41 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:07:42 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:07:42 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:07:43 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:07:44 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:07:45 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:07:46 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:07:46] INFO:     127.0.0.1:56452 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:07:46 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:07:47 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-07 15:07:47 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:07:48 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:07:49 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:07:50 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:07:51 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:07:51 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:07:52 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:07:53 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:07:54 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:55 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:56 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:07:56 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:57 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:58 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:07:59 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:08:00 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:08:00 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:01 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:02 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:02] INFO:     127.0.0.1:58630 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:08:02 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:08:03 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-07 15:08:04 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:08:05 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:05 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:06 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:08:07 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:08 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:09 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:10 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:10 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:08:11 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:08:12 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:08:13 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:08:14 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:08:15 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:08:15 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:08:16 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:08:17 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:08:18 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:08:19 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:08:19] INFO:     127.0.0.1:44318 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:08:19 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:08:20 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-07 15:08:20 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:08:21 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:08:22 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:08:23 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:08:24 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:08:24 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:08:25 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:08:26 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:27 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:08:28 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:08:29 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:29 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:08:30 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:31 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:08:32 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:33 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:08:33 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:08:34 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:08:35 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:08:35] INFO:     127.0.0.1:59750 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:08:35 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:08:36 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-07 15:08:37 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 15:08:38 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:08:38 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:08:39 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:08:40 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:08:41 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:08:42 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:08:43 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:08:43 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:08:44 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:08:45 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:08:46 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:08:47 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:08:47 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:08:48 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:08:49 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:08:50 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:08:51 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:08:52 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:08:52] INFO:     127.0.0.1:39180 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:08:52 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:08:52 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 15:08:53 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:08:54 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:08:55 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:08:56 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:08:57 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:08:57 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:08:58 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:08:59 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:09:00 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:09:01 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:09:01 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:09:02 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:09:03 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:09:04 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:09:05 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:09:06 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:09:06 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:09:07 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:09:08 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:09:08] INFO:     127.0.0.1:45234 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:09:08 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:09:09 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.35, #queue-req: 0, 
[2025-11-07 15:09:10 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:09:11 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:09:11 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:09:12 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:09:13 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:09:14 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:09:15 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:09:15 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:09:16 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:09:17 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:09:18 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:09:19 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:09:20 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:09:20 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:09:21 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:09:22 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:09:23 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:09:24 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:09:24 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:09:25] INFO:     127.0.0.1:32948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:09:25 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:09:25 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-07 15:09:26 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.01, #queue-req: 0, 
[2025-11-07 15:09:27 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 15:09:28 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 15:09:29 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 15:09:29 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 15:09:30 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:09:31 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:09:32 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:09:33 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:09:34 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:09:34 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:09:35 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:09:36 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:09:37 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:09:38 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:09:38 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:09:39 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:09:40 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:09:41 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:09:41] INFO:     127.0.0.1:47702 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:09:41 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:09:42 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-07 15:09:43 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 15:09:43 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:09:44 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:09:45 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:09:46 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:09:47 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:09:47 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:09:48 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:09:49 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:09:50 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:09:51 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:09:52 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:09:52 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:09:53 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:09:54 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:09:55 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:09:56 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:09:56 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:09:57 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:09:58] INFO:     127.0.0.1:45476 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:09:58 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:09:58 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-07 15:09:59 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 49.00, #queue-req: 0, 
[2025-11-07 15:10:00 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 15:10:01 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 15:10:01 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 15:10:02 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 15:10:03 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 15:10:04 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:10:05 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:10:06 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:10:06 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:10:07 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:10:08 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 15:10:09 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:10:10 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:10:10 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:10:11 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:10:12 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:10:13 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:10:14 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:10:14] INFO:     127.0.0.1:58536 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:10:14 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:10:15 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-07 15:10:15 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:10:16 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:10:17 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:10:18 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:10:19 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:10:20 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:10:20 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:10:21 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:10:22 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:10:23 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:10:24 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:10:24 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:10:25 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:10:26 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:10:27 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:10:28 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:10:29 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:10:29 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:10:30 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:10:30] INFO:     127.0.0.1:34204 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:10:30 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:10:31 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.40, #queue-req: 0, 
[2025-11-07 15:10:32 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 15:10:33 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:10:34 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:10:34 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:10:35 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:10:36 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:10:37 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:10:38 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:10:38 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:10:39 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:10:40 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:10:41 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:10:42 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:10:43 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:10:43 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:10:44 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:10:45 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:10:46 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:10:47 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:10:47] INFO:     127.0.0.1:57456 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:10:47 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:10:48 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.39, #queue-req: 0, 
[2025-11-07 15:10:48 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.98, #queue-req: 0, 
[2025-11-07 15:10:49 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 15:10:50 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 15:10:51 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 15:10:52 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 15:10:52 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 15:10:53 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 15:10:54 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 15:10:55 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:10:56 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:10:57 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:10:57 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:10:58 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:10:59 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:11:00 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:11:01 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:11:01 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:11:02 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:11:03 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:11:03] INFO:     127.0.0.1:52240 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:11:03 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:11:04 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-07 15:11:05 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:11:06 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:11:06 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:11:07 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:11:08 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:11:09 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:11:10 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:11:11 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:11:11 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:11:12 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:11:13 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:11:14 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:11:15 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:11:15 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:11:16 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:11:17 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:11:18 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:11:19 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:11:20 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:11:20] INFO:     127.0.0.1:60972 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:11:20 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:11:20 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-07 15:11:21 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:11:22 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:11:23 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:11:24 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:11:25 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:11:25 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:11:26 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:11:27 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:11:28 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:11:29 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:11:29 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:11:30 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:11:31 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:11:32 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:11:33 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-07 15:11:34 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-07 15:11:34 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-07 15:11:35 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-07 15:11:36 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-07 15:11:36] INFO:     127.0.0.1:48918 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:11:36 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:11:37 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.27, #queue-req: 0, 
[2025-11-07 15:11:38 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:11:39 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:11:39 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:11:40 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:11:41 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:11:42 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:11:43 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:11:43 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:11:44 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:11:45 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:11:46 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:11:47 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:11:48 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:11:48 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:11:49 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:11:50 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:11:51 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:11:52 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:11:52 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:11:53] INFO:     127.0.0.1:60800 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:11:53 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:11:53 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-07 15:11:54 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:11:55 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:11:56 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:11:57 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:11:57 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:11:58 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:11:59 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:12:00 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:12:01 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:12:02 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:12:02 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:12:03 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:12:04 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:12:05 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:12:06 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:12:06 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:12:07 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:12:08 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:12:09 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:12:09] INFO:     127.0.0.1:43502 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:12:09 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:12:10 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-07 15:12:11 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:12:11 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:12:12 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:12:13 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:12:14 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:12:15 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:12:16 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:12:16 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:12:17 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:12:18 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:12:19 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:12:20 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:12:20 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:12:21 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:12:22 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:12:23 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:12:24 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:12:25 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:12:25 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:12:26] INFO:     127.0.0.1:43200 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:12:26 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:12:26 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.97, #queue-req: 0, 
[2025-11-07 15:12:27 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:12:28 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:12:29 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:12:30 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:12:30 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:12:31 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:12:32 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:12:33 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:12:34 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:12:34 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:12:35 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:12:36 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:12:37 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:12:38 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:12:39 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:12:39 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:12:40 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:12:41 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:12:42 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:12:42] INFO:     127.0.0.1:35636 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:12:42 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:12:43 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.37, #queue-req: 0, 
[2025-11-07 15:12:44 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:12:44 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:12:45 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:12:46 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:12:47 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:12:48 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:12:48 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:12:49 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:12:50 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:12:51 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:12:52 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:12:53 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-11-07 15:12:53 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:12:54 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:12:55 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:12:56 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:12:57 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:12:58 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:12:58 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:12:59] INFO:     127.0.0.1:33708 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:12:59 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:12:59 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-07 15:13:00 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:13:01 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:13:02 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:13:03 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.58, #queue-req: 0, 
[2025-11-07 15:13:03 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:13:04 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:13:05 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:13:06 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:13:07 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:13:07 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:13:08 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:13:09 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:13:10 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:13:11 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:13:12 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:13:12 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:13:13 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.65, #queue-req: 0, 
[2025-11-07 15:13:14 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:13:15 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:13:15] INFO:     127.0.0.1:54874 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:13:15 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:13:16 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-07 15:13:17 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:13:17 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:13:18 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:13:19 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:13:20 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:13:21 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:13:21 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:13:22 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:13:23 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:13:24 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:13:25 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:13:26 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:13:26 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:13:27 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:13:28 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:13:29 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:13:30 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:13:30 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:13:31 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:13:32] INFO:     127.0.0.1:60752 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:13:32 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:13:32 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.36, #queue-req: 0, 
[2025-11-07 15:13:33 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.97, #queue-req: 0, 
[2025-11-07 15:13:34 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:13:35 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:13:35 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:13:36 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:13:37 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:13:38 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:13:39 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:13:40 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:13:40 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:13:41 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:13:42 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:13:43 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:13:44 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:13:44 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:13:45 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:13:46 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:13:47 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:13:48 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:13:48] INFO:     127.0.0.1:50964 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:13:48 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:13:49 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.38, #queue-req: 0, 
[2025-11-07 15:13:49 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.96, #queue-req: 0, 
[2025-11-07 15:13:50 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:13:51 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:13:52 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:13:53 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:13:53 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:13:54 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:13:55 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:13:56 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:13:57 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:13:58 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:13:58 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:13:59 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:14:00 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:14:01 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:14:02 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:14:02 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:14:03 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:14:04 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:14:04] INFO:     127.0.0.1:41578 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:14:04 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:14:05 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-11-07 15:14:06 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:14:07 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:14:07 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:14:08 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:14:09 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:14:10 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:14:11 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:14:12 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:14:12 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:14:13 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:14:14 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:14:15 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:14:16 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:14:16 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:14:17 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:14:18 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:14:19 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:14:20 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:14:21 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:14:21] INFO:     127.0.0.1:53120 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:14:21 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:14:21 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-07 15:14:22 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:14:23 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:14:24 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:14:25 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:14:26 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:14:26 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:14:27 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:14:28 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:14:29 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:14:30 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:14:30 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:14:31 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:14:32 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:14:33 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:14:34 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:14:35 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:14:35 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:14:36 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:14:37 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:14:37] INFO:     127.0.0.1:44786 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:14:37 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:14:38 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 15:14:39 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:14:40 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:14:40 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:14:41 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:14:42 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:14:43 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:14:44 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:14:44 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:14:45 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:14:46 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:14:47 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:14:48 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:14:49 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:14:49 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:14:50 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:14:51 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:14:52 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:14:53 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:14:54 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:14:54] INFO:     127.0.0.1:52924 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:14:54 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:14:54 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-11-07 15:14:55 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:14:56 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:14:57 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:14:58 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:14:59 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:14:59 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:15:00 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:15:01 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:15:02 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:15:03 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:15:03 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:15:04 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:15:05 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:15:06 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:15:07 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:15:08 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:15:08 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:15:09 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:15:10 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:15:10] INFO:     127.0.0.1:52392 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:15:10 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:15:11 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-11-07 15:15:12 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:15:13 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:15:13 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:15:14 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:15:15 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:15:16 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:15:17 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:15:17 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:15:18 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:15:19 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:15:20 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:15:21 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:15:22 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:15:22 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:15:23 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:15:24 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:15:25 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:15:26 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:15:26 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:15:27] INFO:     127.0.0.1:59108 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:15:27 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:15:27 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-07 15:15:28 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:15:29 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:15:30 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:15:31 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:15:31 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:15:32 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:15:33 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:15:34 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:15:35 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:15:36 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:15:36 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:15:37 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:15:38 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:15:39 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:15:40 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:15:40 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:15:41 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:15:42 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:15:43 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:15:43] INFO:     127.0.0.1:55148 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:15:43 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:15:44 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-07 15:15:45 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:15:45 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:15:46 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:15:47 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:15:48 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:15:49 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:15:50 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:15:50 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:15:51 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:15:52 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:15:53 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:15:54 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:15:54 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:15:55 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:15:56 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:15:57 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:15:58 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:15:59 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:15:59 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:16:00] INFO:     127.0.0.1:38720 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:16:00 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:16:00 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-07 15:16:01 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:16:02 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:16:03 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:16:04 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:16:04 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:16:05 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:16:06 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:16:07 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:08 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:08 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:09 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:10 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:16:11 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:12 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:13 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:16:13 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:16:14 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:16:15 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:16:16 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:16:16] INFO:     127.0.0.1:57880 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:16:16 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:16:17 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-07 15:16:18 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:16:18 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:16:19 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:16:20 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:16:21 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:16:22 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:16:22 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:16:23 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:16:24 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:16:25 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:16:26 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:27 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:16:27 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:28 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:16:29 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:16:30 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:16:31 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:16:32 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:16:32 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:16:33] INFO:     127.0.0.1:44330 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:16:33 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:16:33 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-07 15:16:34 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:16:35 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:16:36 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:16:37 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:16:37 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:16:38 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:16:39 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:16:40 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:16:41 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:16:41 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:42 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:43 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:44 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:45 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:16:46 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:46 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:47 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:16:48 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:16:49 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:16:49] INFO:     127.0.0.1:60968 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:16:49 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:16:50 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.31, #queue-req: 0, 
[2025-11-07 15:16:51 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:16:51 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:16:52 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:16:53 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:16:54 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:16:55 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:16:55 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:16:56 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:16:57 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:16:58 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:16:59 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:17:00 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:17:00 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:17:01 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:17:02 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:17:03 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:17:04 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:17:04 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:17:05 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:17:06] INFO:     127.0.0.1:52976 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:17:06 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:17:06 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-07 15:17:07 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:17:08 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:17:09 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:17:09 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:17:10 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:17:11 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:17:12 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:17:13 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:17:14 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:17:14 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:17:15 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:17:16 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:17:17 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:17:18 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:17:18 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:17:19 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:17:20 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:17:21 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:17:22 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:17:22] INFO:     127.0.0.1:33646 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:17:22 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:17:23 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-07 15:17:23 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 15:17:24 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:17:25 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:17:26 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:17:27 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:17:28 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:17:28 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:17:29 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:17:30 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:17:31 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:17:32 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:17:32 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:17:33 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:17:34 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:17:35 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:17:36 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:17:37 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:17:37 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:17:38 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:17:38] INFO:     127.0.0.1:47270 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:17:38 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:17:39 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-07 15:17:40 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:17:41 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:17:42 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:17:42 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:17:43 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:17:44 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:17:45 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:17:46 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:17:46 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:17:47 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:17:48 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:17:49 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:17:50 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:17:51 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:17:51 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:17:52 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:17:53 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:17:54 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-07 15:17:55 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:17:55] INFO:     127.0.0.1:41494 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:17:55 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:17:56 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-11-07 15:17:56 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:17:57 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:17:58 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:17:59 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:18:00 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:18:00 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:18:01 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:18:02 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:18:03 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:18:04 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:18:05 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:05 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:18:06 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:07 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:08 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:09 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:18:09 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:10 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:11 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:11] INFO:     127.0.0.1:42074 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:18:11 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:18:12 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-07 15:18:13 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:18:14 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:18:14 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:18:15 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:18:16 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:18:17 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:18:18 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:18:19 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:18:19 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:18:20 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:18:21 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:18:22 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:18:23 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:18:23 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:18:24 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:18:25 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:26 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:27 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:18:28 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:28] INFO:     127.0.0.1:55508 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:18:28 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:18:28 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-07 15:18:29 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:18:30 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:18:31 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:18:32 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:18:33 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:18:33 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:18:34 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:35 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:18:36 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:37 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:37 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:38 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:39 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:18:40 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:18:41 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:18:42 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:18:42 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:18:43 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:18:44 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:18:44] INFO:     127.0.0.1:47948 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:18:44 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:18:45 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-07 15:18:46 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.95, #queue-req: 0, 
[2025-11-07 15:18:47 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:18:47 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:18:48 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:18:49 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:18:50 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.92, #queue-req: 0, 
[2025-11-07 15:18:51 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:18:51 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:18:52 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:18:53 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:18:54 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:18:55 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:18:56 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:18:56 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:18:57 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:18:58 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:18:59 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:19:00 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:19:00 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:19:01] INFO:     127.0.0.1:53326 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:19:01 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:19:01 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.80, #queue-req: 0, 
[2025-11-07 15:19:02 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:19:03 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:19:04 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:19:05 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:05 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:06 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:07 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:19:08 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:19:09 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:19:10 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:19:10 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:19:11 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:19:12 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:13 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:19:14 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:19:15 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:19:15 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:19:16 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:19:17 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:19:17] INFO:     127.0.0.1:53492 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:19:17 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:19:18 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.77, #queue-req: 0, 
[2025-11-07 15:19:19 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:19:20 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:19:20 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:19:21 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:19:22 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:19:23 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:19:24 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:19:24 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:19:25 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:19:26 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:19:27 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:19:28 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:29 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:19:29 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:19:30 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:31 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:32 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:19:33 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:33 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:19:34] INFO:     127.0.0.1:41238 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:19:34 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:19:34 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.28, #queue-req: 0, 
[2025-11-07 15:19:35 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:36 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:19:37 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:19:38 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:19:38 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:19:39 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:19:40 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.78, #queue-req: 0, 
[2025-11-07 15:19:41 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:19:42 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-07 15:19:43 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-07 15:19:43 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-07 15:19:44 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-07 15:19:45 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:19:46 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-07 15:19:47 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-07 15:19:47 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-07 15:19:48 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.77, #queue-req: 0, 
[2025-11-07 15:19:49 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.75, #queue-req: 0, 
[2025-11-07 15:19:50 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.76, #queue-req: 0, 
[2025-11-07 15:19:50] INFO:     127.0.0.1:52290 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:19:50 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:19:51 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.29, #queue-req: 0, 
[2025-11-07 15:19:52 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:19:52 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:19:53 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:54 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:55 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:19:56 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:57 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:57 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:58 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:19:59 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:20:00 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:20:01 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:20:01 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:20:02 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:20:03 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:20:04 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.79, #queue-req: 0, 
[2025-11-07 15:20:05 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:20:06 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:20:06 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:20:07] INFO:     127.0.0.1:39654 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:20:07 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:20:07 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.32, #queue-req: 0, 
[2025-11-07 15:20:08 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.94, #queue-req: 0, 
[2025-11-07 15:20:09 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:20:10 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:20:11 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.93, #queue-req: 0, 
[2025-11-07 15:20:11 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.90, #queue-req: 0, 
[2025-11-07 15:20:12 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:20:13 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:20:14 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:20:15 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:20:15 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:20:16 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:20:17 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:20:18 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:20:19 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:20:20 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:20:20 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:20:21 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:20:22 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:20:23 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:20:23] INFO:     127.0.0.1:51818 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:20:23 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:20:24 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.33, #queue-req: 0, 
[2025-11-07 15:20:25 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.91, #queue-req: 0, 
[2025-11-07 15:20:25 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:20:26 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:20:27 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:20:28 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:20:29 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:20:29 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:20:30 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:20:31 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:20:32 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:20:33 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:20:34 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:20:34 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:20:35 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:20:36 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:20:37 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:20:38 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:20:38 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:20:39 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:20:40] INFO:     127.0.0.1:47210 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:20:40 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:20:40 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 43.78, #queue-req: 0, 
[2025-11-07 15:20:41 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:20:42 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:20:43 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:20:44 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:20:44 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:20:45 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:20:46 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:20:47 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:20:48 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:20:48 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:20:49 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:20:50 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:20:51 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:20:52 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:20:53 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:20:53 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:20:54 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:20:55 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.81, #queue-req: 0, 
[2025-11-07 15:20:56 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.80, #queue-req: 0, 
[2025-11-07 15:20:56] INFO:     127.0.0.1:41482 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:20:56 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:20:57 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.30, #queue-req: 0, 
[2025-11-07 15:20:58 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:20:58 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:20:59 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:21:00 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:21:01 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:21:02 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:21:02 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.88, #queue-req: 0, 
[2025-11-07 15:21:03 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:21:04 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:21:05 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.87, #queue-req: 0, 
[2025-11-07 15:21:06 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:21:07 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:21:07 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:21:08 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:21:09 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:21:10 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:21:11 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:21:11 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.86, #queue-req: 0, 
[2025-11-07 15:21:12 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:21:13] INFO:     127.0.0.1:60466 - "POST /generate HTTP/1.1" 200 OK
[2025-11-07 15:21:13 TP0] Prefill batch, #new-seq: 1, #new-token: 1, #cached-token: 3200, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-11-07 15:21:13 TP0] Decode batch, #running-req: 1, #token: 3227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 44.34, #queue-req: 0, 
[2025-11-07 15:21:14 TP0] Decode batch, #running-req: 1, #token: 3267, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.89, #queue-req: 0, 
[2025-11-07 15:21:15 TP0] Decode batch, #running-req: 1, #token: 3307, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:21:16 TP0] Decode batch, #running-req: 1, #token: 3347, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:21:16 TP0] Decode batch, #running-req: 1, #token: 3387, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:21:17 TP0] Decode batch, #running-req: 1, #token: 3427, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:21:18 TP0] Decode batch, #running-req: 1, #token: 3467, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:21:19 TP0] Decode batch, #running-req: 1, #token: 3507, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:21:20 TP0] Decode batch, #running-req: 1, #token: 3547, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:21:21 TP0] Decode batch, #running-req: 1, #token: 3587, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:21:21 TP0] Decode batch, #running-req: 1, #token: 3627, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.85, #queue-req: 0, 
[2025-11-07 15:21:22 TP0] Decode batch, #running-req: 1, #token: 3667, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:21:23 TP0] Decode batch, #running-req: 1, #token: 3707, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:21:24 TP0] Decode batch, #running-req: 1, #token: 3747, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:21:25 TP0] Decode batch, #running-req: 1, #token: 3787, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:21:25 TP0] Decode batch, #running-req: 1, #token: 3827, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.84, #queue-req: 0, 
[2025-11-07 15:21:26 TP0] Decode batch, #running-req: 1, #token: 3867, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.82, #queue-req: 0, 
[2025-11-07 15:21:27 TP0] Decode batch, #running-req: 1, #token: 3907, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:21:28 TP0] Decode batch, #running-req: 1, #token: 3947, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:21:29 TP0] Decode batch, #running-req: 1, #token: 3987, token usage: 0.00, cuda graph: True, gen throughput (token/s): 48.83, #queue-req: 0, 
[2025-11-07 15:21:29] INFO:     127.0.0.1:39922 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-07 15:21:29] INFO:     127.0.0.1:39930 - "GET /get_server_info HTTP/1.1" 200 OK
[2025-11-07 15:21:36] SIGTERM received. signum=None frame=None. Draining requests and shutting down...
[2025-11-07 15:21:37] Gracefully exiting... Remaining number of requests 0. Remaining requests remaining_rids=[].
